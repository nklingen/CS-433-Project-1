{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%run implementations.ipynb\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. First model: Use raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first to inspect the data, I notice many invalid values of -999\n",
    "# for this first model I disregard them\n",
    "\n",
    "# first to split the data into training and testing\n",
    "\n",
    "ratio = 0.8\n",
    "seed = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX, y, ratio, seed)\n",
    "\n",
    "# then to standardize x_tr and x_te\n",
    "\n",
    "x_tr = standardize(x_tr)[0]\n",
    "x_te = standardize(x_te)[0]\n",
    "\n",
    "# next to add a column of ones\n",
    "\n",
    "y_tr, x_tr = build_model_data(x_tr, y_tr)\n",
    "y_te, x_te = build_model_data(x_te, y_te)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.1 First model: Use Least squares : train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.339273076205053 11.1346836777524\n"
     ]
    }
   ],
   "source": [
    "# First Model using least squares\n",
    "\n",
    "weights = least_squares(y_tr, x_tr)\n",
    "MSE_tr = compute_loss(y_tr, x_tr, weights)\n",
    "MSE_te = compute_loss(y_te, x_te, weights)\n",
    "\n",
    "print(MSE_tr, MSE_te) #0.33927307620272856 11.175145651957722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.2 First model: Use Least squares : predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34396"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on the test data slice\n",
    "y_pred = predict_labels(weights, x_te)\n",
    "# Check accuracy\n",
    "np.mean(y_te==y_pred) # 0.34396"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Second Model: delete all the lines with Nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all nan but there actually are none\n",
    "tX_2 = tX[~np.isnan(tX).any(axis=1)]\n",
    "\n",
    "# removing all rows with -999, there are 181886\n",
    "y = y[np.all(tX_2 != -999, axis=1)]\n",
    "tX_2 = tX_2[np.all(tX_2 != -999, axis=1)]\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX_2, y, ratio, seed)\n",
    "\n",
    "x_tr = standardize(x_tr)[0]\n",
    "x_te = standardize(x_te)[0]\n",
    "\n",
    "y_tr, x_tr = build_model_data(x_tr, y_tr)\n",
    "y_te, x_te = build_model_data(x_te, y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Least squares for Second Model : train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36864162046560583 0.5411160179301698\n"
     ]
    }
   ],
   "source": [
    "weights = least_squares(y_tr, x_tr)\n",
    "MSE_tr = compute_loss(y_tr, x_tr, weights)\n",
    "MSE_te = compute_loss(y_te, x_te, weights)\n",
    "\n",
    "print(MSE_tr, MSE_te) #0.36864162046559024 0.5411866496785176"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Least squares for Second Model : prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6120531454158409"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on the test data slice\n",
    "y_pred = predict_labels(weights, x_te)\n",
    "# Check accuracy\n",
    "np.mean(y_te==y_pred) # 0.6120531454158409"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1  GD for Second Model : train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/150 loss = 16.742723002367942\n",
      "Step 2/150 loss = 13.640113171583597\n",
      "Step 3/150 loss = 11.197208614873615\n",
      "Step 4/150 loss = 9.24138694198122\n",
      "Step 5/150 loss = 7.663721730951572\n",
      "Step 6/150 loss = 6.386469086966046\n",
      "Step 7/150 loss = 5.350289705071325\n",
      "Step 8/150 loss = 4.508432432453081\n",
      "Step 9/150 loss = 3.8235432932229165\n",
      "Step 10/150 loss = 3.2655939588419374\n",
      "Step 11/150 loss = 2.8103795766576734\n",
      "Step 12/150 loss = 2.438364170002802\n",
      "Step 13/150 loss = 2.1337688629695815\n",
      "Step 14/150 loss = 1.883843255860384\n",
      "Step 15/150 loss = 1.6782801536029626\n",
      "Step 16/150 loss = 1.5087443685174675\n",
      "Step 17/150 loss = 1.3684929531113685\n",
      "Step 18/150 loss = 1.2520689442005632\n",
      "Step 19/150 loss = 1.1550542964765458\n",
      "Step 20/150 loss = 1.0738705085735651\n",
      "Step 21/150 loss = 1.0056176944472426\n",
      "Step 22/150 loss = 0.9479446554141302\n",
      "Step 23/150 loss = 0.8989439562644808\n",
      "Step 24/150 loss = 0.8570671735948375\n",
      "Step 25/150 loss = 0.8210564219510522\n",
      "Step 26/150 loss = 0.7898890181788067\n",
      "Step 27/150 loss = 0.7627327523082613\n",
      "Step 28/150 loss = 0.7389097230556663\n",
      "Step 29/150 loss = 0.7178670906709673\n",
      "Step 30/150 loss = 0.6991534179387846\n",
      "Step 31/150 loss = 0.6823995265631576\n",
      "Step 32/150 loss = 0.6673030029310048\n",
      "Step 33/150 loss = 0.6536156540088474\n",
      "Step 34/150 loss = 0.6411333486518159\n",
      "Step 35/150 loss = 0.6296877881482001\n",
      "Step 36/150 loss = 0.6191398374234841\n",
      "Step 37/150 loss = 0.6093741190414084\n",
      "Step 38/150 loss = 0.6002946292338985\n",
      "Step 39/150 loss = 0.5918211812997781\n",
      "Step 40/150 loss = 0.5838865189560524\n",
      "Step 41/150 loss = 0.5764339723158555\n",
      "Step 42/150 loss = 0.5694155534830962\n",
      "Step 43/150 loss = 0.5627904084077477\n",
      "Step 44/150 loss = 0.5565235575348213\n",
      "Step 45/150 loss = 0.5505848706282401\n",
      "Step 46/150 loss = 0.5449482315422846\n",
      "Step 47/150 loss = 0.539590857119521\n",
      "Step 48/150 loss = 0.5344927411958728\n",
      "Step 49/150 loss = 0.5296362001981278\n",
      "Step 50/150 loss = 0.5250055012750212\n",
      "Step 51/150 loss = 0.5205865575106466\n",
      "Step 52/150 loss = 0.5163666776904517\n",
      "Step 53/150 loss = 0.5123343604564157\n",
      "Step 54/150 loss = 0.5084791246051258\n",
      "Step 55/150 loss = 0.5047913688359524\n",
      "Step 56/150 loss = 0.5012622555156311\n",
      "Step 57/150 loss = 0.49788361404629056\n",
      "Step 58/150 loss = 0.4946478602516447\n",
      "Step 59/150 loss = 0.49154792886734333\n",
      "Step 60/150 loss = 0.48857721676605154\n",
      "Step 61/150 loss = 0.4857295349897055\n",
      "Step 62/150 loss = 0.48299906802004455\n",
      "Step 63/150 loss = 0.48038033900968163\n",
      "Step 64/150 loss = 0.47786817993243175\n",
      "Step 65/150 loss = 0.4754577058036787\n",
      "Step 66/150 loss = 0.47314429227764065\n",
      "Step 67/150 loss = 0.47092355605524155\n",
      "Step 68/150 loss = 0.4687913376394557\n",
      "Step 69/150 loss = 0.46674368605889416\n",
      "Step 70/150 loss = 0.4647768452486901\n",
      "Step 71/150 loss = 0.4628872418333281\n",
      "Step 72/150 loss = 0.4610714741013584\n",
      "Step 73/150 loss = 0.4593263019988413\n",
      "Step 74/150 loss = 0.45764863799848027\n",
      "Step 75/150 loss = 0.45603553872597224\n",
      "Step 76/150 loss = 0.4544841972451775\n",
      "Step 77/150 loss = 0.45299193592012627\n",
      "Step 78/150 loss = 0.45155619978531264\n",
      "Step 79/150 loss = 0.45017455036673854\n",
      "Step 80/150 loss = 0.4488446599052014\n",
      "Step 81/150 loss = 0.44756430594075225\n",
      "Step 82/150 loss = 0.4463313662233585\n",
      "Step 83/150 loss = 0.44514381391985436\n",
      "Step 84/150 loss = 0.44399971309143343\n",
      "Step 85/150 loss = 0.4428972144193931\n",
      "Step 86/150 loss = 0.441834551159717\n",
      "Step 87/150 loss = 0.4408100353094807\n",
      "Step 88/150 loss = 0.43982205397006785\n",
      "Step 89/150 loss = 0.43886906589387764\n",
      "Step 90/150 loss = 0.43794959820261736\n",
      "Step 91/150 loss = 0.43706224326649024\n",
      "Step 92/150 loss = 0.4362056557346086\n",
      "Step 93/150 loss = 0.435378549707848\n",
      "Step 94/150 loss = 0.43457969604611235\n",
      "Step 95/150 loss = 0.43380791980264066\n",
      "Step 96/150 loss = 0.433062097778558\n",
      "Step 97/150 loss = 0.43234115619137475\n",
      "Step 98/150 loss = 0.4316440684515835\n",
      "Step 99/150 loss = 0.4309698530418978\n",
      "Step 100/150 loss = 0.43031757149402805\n",
      "Step 101/150 loss = 0.4296863264582085\n",
      "Step 102/150 loss = 0.4290752598609743\n",
      "Step 103/150 loss = 0.4284835511469504\n",
      "Step 104/150 loss = 0.4279104156006517\n",
      "Step 105/150 loss = 0.42735510274451216\n",
      "Step 106/150 loss = 0.42681689480956453\n",
      "Step 107/150 loss = 0.4262951052753771\n",
      "Step 108/150 loss = 0.4257890774760318\n",
      "Step 109/150 loss = 0.42529818326908575\n",
      "Step 110/150 loss = 0.42482182176461386\n",
      "Step 111/150 loss = 0.42435941811157146\n",
      "Step 112/150 loss = 0.4239104223388498\n",
      "Step 113/150 loss = 0.423474308248523\n",
      "Step 114/150 loss = 0.42305057235890703\n",
      "Step 115/150 loss = 0.42263873289515896\n",
      "Step 116/150 loss = 0.4222383288252571\n",
      "Step 117/150 loss = 0.42184891893929843\n",
      "Step 118/150 loss = 0.42147008097015043\n",
      "Step 119/150 loss = 0.4211014107535819\n",
      "Step 120/150 loss = 0.420742521426085\n",
      "Step 121/150 loss = 0.4203930426586834\n",
      "Step 122/150 loss = 0.4200526199251005\n",
      "Step 123/150 loss = 0.4197209138027333\n",
      "Step 124/150 loss = 0.4193975993049524\n",
      "Step 125/150 loss = 0.4190823652433125\n",
      "Step 126/150 loss = 0.41877491361832453\n",
      "Step 127/150 loss = 0.41847495903750165\n",
      "Step 128/150 loss = 0.4181822281594492\n",
      "Step 129/150 loss = 0.4178964591628246\n",
      "Step 130/150 loss = 0.4176174012390471\n",
      "Step 131/150 loss = 0.4173448141076879\n",
      "Step 132/150 loss = 0.41707846755351863\n",
      "Step 133/150 loss = 0.41681814098424436\n",
      "Step 134/150 loss = 0.4165636230079889\n",
      "Step 135/150 loss = 0.4163147110296451\n",
      "Step 136/150 loss = 0.4160712108652411\n",
      "Step 137/150 loss = 0.41583293637351065\n",
      "Step 138/150 loss = 0.41559970910389793\n",
      "Step 139/150 loss = 0.4153713579602538\n",
      "Step 140/150 loss = 0.41514771887952084\n",
      "Step 141/150 loss = 0.4149286345247334\n",
      "Step 142/150 loss = 0.4147139539916889\n",
      "Step 143/150 loss = 0.41450353252867767\n",
      "Step 144/150 loss = 0.41429723126868334\n",
      "Step 145/150 loss = 0.41409491697349454\n",
      "Step 146/150 loss = 0.4138964617891952\n",
      "Step 147/150 loss = 0.4137017430125198\n",
      "Step 148/150 loss = 0.41351064286758865\n",
      "Step 149/150 loss = 0.41332304829255556\n",
      "Step 150/150 loss = 0.4131388507357264\n",
      "Gradient final loss for w* = 0.4131388507357264\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 150\n",
    "gamma = 0.08\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones(x_tr.shape[1])\n",
    "\n",
    "gradient_w, gradient_loss = least_squares_GD(y_tr, x_tr, w_initial, max_iters, gamma) \n",
    "\n",
    "print(\"Gradient final loss for w* = \" + str(gradient_loss)) # 0.4131388507357264"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2  GD for Second Model : predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6810115431906186\n",
      "0.687954195111209\n"
     ]
    }
   ],
   "source": [
    "# predict on the train data slice using the gradient_w\n",
    "y_pred = predict_labels(gradient_w, x_tr)\n",
    "# Check accuracy\n",
    "print(np.mean(y_tr == y_pred)) # 0.6810115431906186\n",
    "\n",
    "# predict on the test data slice\n",
    "y_pred = predict_labels(gradient_w, x_te)\n",
    "# Check accuracy\n",
    "print(np.mean(y_te == y_pred)) # 0.687954195111209"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3  Ridge regression for Second Model : train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda=1e-10; Loss=0.3686442007271991\n",
      "Predict: \n",
      " \t0.7238076012552531\n",
      "\t0.732511194303751\n",
      "\n",
      "Lambda=1e-05; Loss=0.3812050031774571\n",
      "Predict: \n",
      " \t0.7140078178047751\n",
      "\t0.716141818982603\n",
      "\n",
      "Lambda=0.0001; Loss=0.3912165017182041\n",
      "Predict: \n",
      " \t0.7069240792057404\n",
      "\t0.7082874550392718\n",
      "\n",
      "Lambda=0.001; Loss=0.3942938205380461\n",
      "Predict: \n",
      " \t0.7047585839863464\n",
      "\t0.7071863759817955\n",
      "\n",
      "Lambda=0.01; Loss=0.39827148514628047\n",
      "Predict: \n",
      " \t0.6991429777394432\n",
      "\t0.7037363282683696\n",
      "\n",
      "Lambda=0.1; Loss=0.41825458605913995\n",
      "Predict: \n",
      " \t0.679855388963315\n",
      "\t0.6844307421272847\n",
      "\n",
      "Lambda=1; Loss=0.44446970952443027\n",
      "Predict: \n",
      " \t0.6696885724248041\n",
      "\t0.6718784408720546\n",
      "\n",
      "Lambda=10.0; Loss=0.47831430825184806\n",
      "Predict: \n",
      " \t0.6635591198546549\n",
      "\t0.6697496880276004\n",
      "\n",
      "Lambda=100.0; Loss=0.4968364258616815\n",
      "Predict: \n",
      " \t0.6343983410104421\n",
      "\t0.6381120164427806\n",
      "\n",
      "Lambda=1000.0; Loss=0.49966605666548564\n",
      "Predict: \n",
      " \t0.6239562496559065\n",
      "\t0.6285693312779858\n",
      "\n",
      "Lambda=10000.0; Loss=0.4999664133399347\n",
      "Predict: \n",
      " \t0.6227450404653979\n",
      "\t0.6270278205975189\n",
      "\n",
      "Lambda=100000.0; Loss=0.4999966393921833\n",
      "Predict: \n",
      " \t0.622689985502193\n",
      "\t0.6269544153270205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lambda_ in [1e-10, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4, 1e5]:\n",
    "    w, loss = ridge_regression(y_tr, x_tr, lambda_)\n",
    "    y_pred = predict_labels(w, x_te)\n",
    "    print(\"Lambda=\" + str(lambda_)+\"; Loss=\"+str(loss))\n",
    "    \n",
    "    # predict on the train data slice using the w\n",
    "    y_pred = predict_labels(w, x_tr)\n",
    "    # Check accuracy\n",
    "    print(\"Predict: \\n \\t\" + str(np.mean(y_tr == y_pred)))\n",
    "\n",
    "    # predict on the test data slice\n",
    "    y_pred = predict_labels(w, x_te)\n",
    "    # Check accuracy\n",
    "    print(\"\\t\" + str(np.mean(y_te == y_pred)) + \"\\n\") # Best for lambda = 1e-10, 0.732511194303751"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Third Model: replace all the nanValues with the column mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_3 = replace_nan_with_mean(tX, -999)\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX_3, y, ratio, seed)\n",
    "\n",
    "x_tr = standardize(x_tr)[0]\n",
    "x_te = standardize(x_te)[0]\n",
    "\n",
    "y_tr, x_tr = build_model_data(x_tr, y_tr)\n",
    "y_te, x_te = build_model_data(x_te, y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Third Model: Use least_squares : train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4978447945914003 0.62752070923795\n"
     ]
    }
   ],
   "source": [
    "weights = least_squares(y_tr, x_tr)\n",
    "MSE_tr = compute_loss(y_tr, x_tr, weights)\n",
    "MSE_te = compute_loss(y_te, x_te, weights)\n",
    "\n",
    "print(MSE_tr, MSE_te) # 0.4978447945914003 0.62752070923795"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Third Model: Use least_squares: prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5331424796300375"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on the test data slice\n",
    "y_pred = predict_labels(weights, x_te)\n",
    "# Check accuracy\n",
    "np.mean(y_te==y_pred) # 0.5331424796300375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Third Model: Use GD : Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/150 loss = 17.3154463737639\n",
      "Step 2/150 loss = 12.85154559694331\n",
      "Step 3/150 loss = 10.15793488186782\n",
      "Step 4/150 loss = 8.393558092837589\n",
      "Step 5/150 loss = 7.146208061898201\n",
      "Step 6/150 loss = 6.207766930275732\n",
      "Step 7/150 loss = 5.468531107274744\n",
      "Step 8/150 loss = 4.867250480498819\n",
      "Step 9/150 loss = 4.367293147568957\n",
      "Step 10/150 loss = 3.9451073486036248\n",
      "Step 11/150 loss = 3.5845078798959227\n",
      "Step 12/150 loss = 3.2737494035343864\n",
      "Step 13/150 loss = 3.003955009009131\n",
      "Step 14/150 loss = 2.768220499254645\n",
      "Step 15/150 loss = 2.561068155727898\n",
      "Step 16/150 loss = 2.3780904919748393\n",
      "Step 17/150 loss = 2.215703848009918\n",
      "Step 18/150 loss = 2.0709699189386273\n",
      "Step 19/150 loss = 1.941462118389316\n",
      "Step 20/150 loss = 1.8251632119498764\n",
      "Step 21/150 loss = 1.720385701622581\n",
      "Step 22/150 loss = 1.6257092655990142\n",
      "Step 23/150 loss = 1.5399312431392322\n",
      "Step 24/150 loss = 1.4620272297442622\n",
      "Step 25/150 loss = 1.3911195764000153\n",
      "Step 26/150 loss = 1.3264521046700746\n",
      "Step 27/150 loss = 1.2673697309008858\n",
      "Step 28/150 loss = 1.2133019805895244\n",
      "Step 29/150 loss = 1.1637495944975642\n",
      "Step 30/150 loss = 1.1182735987878027\n",
      "Step 31/150 loss = 1.0764863444154509\n",
      "Step 32/150 loss = 1.0380441249870365\n",
      "Step 33/150 loss = 1.0026410638425003\n",
      "Step 34/150 loss = 0.970004025184978\n",
      "Step 35/150 loss = 0.939888354498013\n",
      "Step 36/150 loss = 0.9120742932129505\n",
      "Step 37/150 loss = 0.8863639439261465\n",
      "Step 38/150 loss = 0.8625786872179575\n",
      "Step 39/150 loss = 0.8405569707031486\n",
      "Step 40/150 loss = 0.8201524064499346\n",
      "Step 41/150 loss = 0.8012321252086354\n",
      "Step 42/150 loss = 0.7836753456707564\n",
      "Step 43/150 loss = 0.767372124768748\n",
      "Step 44/150 loss = 0.7522222612451404\n",
      "Step 45/150 loss = 0.7381343296966276\n",
      "Step 46/150 loss = 0.7250248262931975\n",
      "Step 47/150 loss = 0.712817410588377\n",
      "Step 48/150 loss = 0.7014422304348127\n",
      "Step 49/150 loss = 0.6908353191260862\n",
      "Step 50/150 loss = 0.6809380556008516\n",
      "Step 51/150 loss = 0.6716966799477738\n",
      "Step 52/150 loss = 0.6630618576017095\n",
      "Step 53/150 loss = 0.65498828657237\n",
      "Step 54/150 loss = 0.6474343428354742\n",
      "Step 55/150 loss = 0.6403617596741392\n",
      "Step 56/150 loss = 0.6337353373097039\n",
      "Step 57/150 loss = 0.6275226796259905\n",
      "Step 58/150 loss = 0.6216939551849394\n",
      "Step 59/150 loss = 0.6162216800671956\n",
      "Step 60/150 loss = 0.6110805203587449\n",
      "Step 61/150 loss = 0.6062471123522138\n",
      "Step 62/150 loss = 0.6016998987456249\n",
      "Step 63/150 loss = 0.5974189793075652\n",
      "Step 64/150 loss = 0.5933859746402773\n",
      "Step 65/150 loss = 0.5895839018147043\n",
      "Step 66/150 loss = 0.5859970607769619\n",
      "Step 67/150 loss = 0.5826109305365063\n",
      "Step 68/150 loss = 0.5794120742444596\n",
      "Step 69/150 loss = 0.5763880523578198\n",
      "Step 70/150 loss = 0.5735273431630585\n",
      "Step 71/150 loss = 0.5708192700021014\n",
      "Step 72/150 loss = 0.5682539346058828\n",
      "Step 73/150 loss = 0.5658221559964925\n",
      "Step 74/150 loss = 0.5635154144690777\n",
      "Step 75/150 loss = 0.5613258002098145\n",
      "Step 76/150 loss = 0.5592459661469541\n",
      "Step 77/150 loss = 0.5572690846686743\n",
      "Step 78/150 loss = 0.5553888078746554\n",
      "Step 79/150 loss = 0.5535992310582993\n",
      "Step 80/150 loss = 0.5518948591436936\n",
      "Step 81/150 loss = 0.5502705758260209\n",
      "Step 82/150 loss = 0.5487216151864425\n",
      "Step 83/150 loss = 0.5472435355727231\n",
      "Step 84/150 loss = 0.5458321955552448\n",
      "Step 85/150 loss = 0.544483731784755\n",
      "Step 86/150 loss = 0.5431945385933596\n",
      "Step 87/150 loss = 0.5419612491940675\n",
      "Step 88/150 loss = 0.5407807183467386\n",
      "Step 89/150 loss = 0.5396500063697021\n",
      "Step 90/150 loss = 0.5385663643867049\n",
      "Step 91/150 loss = 0.5375272207083142\n",
      "Step 92/150 loss = 0.5365301682555131\n",
      "Step 93/150 loss = 0.535572952941096\n",
      "Step 94/150 loss = 0.5346534629316161\n",
      "Step 95/150 loss = 0.5337697187191817\n",
      "Step 96/150 loss = 0.5329198639383429\n",
      "Step 97/150 loss = 0.532102156868753\n",
      "Step 98/150 loss = 0.5313149625692425\n",
      "Step 99/150 loss = 0.5305567455934777\n",
      "Step 100/150 loss = 0.5298260632415077\n",
      "Step 101/150 loss = 0.5291215593052824\n",
      "Step 102/150 loss = 0.5284419582696784\n",
      "Step 103/150 loss = 0.5277860599337224\n",
      "Step 104/150 loss = 0.5271527344195903\n",
      "Step 105/150 loss = 0.526540917539597\n",
      "Step 106/150 loss = 0.525949606493806\n",
      "Step 107/150 loss = 0.5253778558731026\n",
      "Step 108/150 loss = 0.5248247739445909\n",
      "Step 109/150 loss = 0.5242895191980314\n",
      "Step 110/150 loss = 0.5237712971337333\n",
      "Step 111/150 loss = 0.5232693572738711\n",
      "Step 112/150 loss = 0.5227829903806157\n",
      "Step 113/150 loss = 0.5223115258657869\n",
      "Step 114/150 loss = 0.5218543293779249\n",
      "Step 115/150 loss = 0.5214108005537809\n",
      "Step 116/150 loss = 0.520980370922242\n",
      "Step 117/150 loss = 0.5205625019496256\n",
      "Step 118/150 loss = 0.5201566832161325\n",
      "Step 119/150 loss = 0.519762430714036\n",
      "Step 120/150 loss = 0.5193792852588957\n",
      "Step 121/150 loss = 0.5190068110057512\n",
      "Step 122/150 loss = 0.5186445940628578\n",
      "Step 123/150 loss = 0.5182922411960876\n",
      "Step 124/150 loss = 0.5179493786176316\n",
      "Step 125/150 loss = 0.5176156508531166\n",
      "Step 126/150 loss = 0.5172907196816855\n",
      "Step 127/150 loss = 0.5169742631439945\n",
      "Step 128/150 loss = 0.5166659746134483\n",
      "Step 129/150 loss = 0.5163655619263404\n",
      "Step 130/150 loss = 0.5160727465668825\n",
      "Step 131/150 loss = 0.5157872629033923\n",
      "Step 132/150 loss = 0.515508857472185\n",
      "Step 133/150 loss = 0.5152372883059593\n",
      "Step 134/150 loss = 0.5149723243036971\n",
      "Step 135/150 loss = 0.5147137446393111\n",
      "Step 136/150 loss = 0.5144613382064693\n",
      "Step 137/150 loss = 0.5142149030972065\n",
      "Step 138/150 loss = 0.5139742461121015\n",
      "Step 139/150 loss = 0.5137391822999532\n",
      "Step 140/150 loss = 0.5135095345250347\n",
      "Step 141/150 loss = 0.5132851330601353\n",
      "Step 142/150 loss = 0.5130658152037264\n",
      "Step 143/150 loss = 0.5128514249196959\n",
      "Step 144/150 loss = 0.5126418124982142\n",
      "Step 145/150 loss = 0.5124368342363763\n",
      "Step 146/150 loss = 0.512236352137372\n",
      "Step 147/150 loss = 0.512040233627009\n",
      "Step 148/150 loss = 0.5118483512865011\n",
      "Step 149/150 loss = 0.511660582600499\n",
      "Step 150/150 loss = 0.5114768097194176\n",
      "Gradient final loss for w* = 0.5114768097194176\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 150\n",
    "gamma = 0.08\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones(x_tr.shape[1])\n",
    "\n",
    "gradient_w, gradient_loss = least_squares_GD(y_tr, x_tr, w_initial, max_iters, gamma) \n",
    "\n",
    "print(\"Gradient final loss for w* = \" + str(gradient_loss)) # 0.5114768097194176\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Third Model: Use GD : Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5006239447992366"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on the test data slice\n",
    "y_pred = predict_labels(gradient_w, x_te)\n",
    "# Check accuracy\n",
    "np.mean(y_te==y_pred) # 0.5006239447992366"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fourth Model with Feature Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-597711cd602c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpolynome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolynome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolynome\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-135-861c55fb9329>\u001b[0m in \u001b[0;36mleast_squares\u001b[0;34m(y, tx)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mleast_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "degrees = [1, 2, 3, 4]\n",
    "\n",
    "# define the structure of the figure\n",
    "num_row = 2\n",
    "num_col = 2\n",
    "f, axs = plt.subplots(num_row, num_col)\n",
    "    \n",
    "for ind, degree in enumerate(degrees):\n",
    "\n",
    "        polynome = build_poly(x_tr,degree)\n",
    "        w = least_squares(y_tr, polynome.T)\n",
    "        MSE = compute_loss(y_tr, polynome, w)\n",
    "\n",
    "        print(\"Processing {i}th experiment, degree={d}, mse={loss}\".format( \n",
    "            i=ind + 1, d=degree, loss=MSE))\n",
    "#         plot_fitted_curve(y_tr, x_tr, weights, degree, axs[ind // num_col][ind % num_col])\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We observed that model 2 with ridge_regression method and lambda_ = 1e-10 (TODO, we have to check them all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, loss = ridge_regression(y, tX, 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: unzip the file\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'out.csv'\n",
    "y_pred = predict_labels(w, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nklingen/CS-433-Project-1/blob/master/scripts/project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJwBaFjTmN7O"
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import datetime\n",
    "# %run implementations.ipynb\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCDzdiFmmN7U"
   },
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k4oJSK5smN7V"
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jg-e8g_YmN7Z"
   },
   "source": [
    "# 1. First model: Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yU8bkkXjmN7a"
   },
   "outputs": [],
   "source": [
    "# first to split the data into training and testing\n",
    "\n",
    "ratio = 0.8\n",
    "seed = 3\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX, y, ratio, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UqlLtJHJmN8m"
   },
   "source": [
    "#  Logistic regression for Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yXwARnNRmN8n",
    "outputId": "21bddea8-c088-46f2-b286-a4c0f0b8825e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (1/5000): loss=0.6406145440041099\n",
      "Logistic Regression (2/5000): loss=0.6265591505191764\n",
      "Logistic Regression (3/5000): loss=0.6359256703694344\n",
      "Logistic Regression (4/5000): loss=0.6241624464539798\n",
      "Logistic Regression (5/5000): loss=0.6228315428310076\n",
      "Logistic Regression (6/5000): loss=0.6219220942013178\n",
      "Logistic Regression (7/5000): loss=0.6242508686228869\n",
      "Logistic Regression (8/5000): loss=0.621111122846317\n",
      "Logistic Regression (9/5000): loss=0.6199830209113237\n",
      "Logistic Regression (10/5000): loss=0.6223708904397687\n",
      "Logistic Regression (11/5000): loss=0.6307837589779017\n",
      "Logistic Regression (12/5000): loss=0.6179018551002593\n",
      "Logistic Regression (13/5000): loss=0.6191979106219369\n",
      "Logistic Regression (14/5000): loss=0.6217662602735045\n",
      "Logistic Regression (15/5000): loss=0.6176919554230607\n",
      "Logistic Regression (16/5000): loss=0.6186899271523812\n",
      "Logistic Regression (17/5000): loss=0.6150692543025016\n",
      "Logistic Regression (18/5000): loss=0.6214439210508684\n",
      "Logistic Regression (19/5000): loss=0.61464948948813\n",
      "Logistic Regression (20/5000): loss=0.6143219497220697\n",
      "Logistic Regression (21/5000): loss=0.6143973237608781\n",
      "Logistic Regression (22/5000): loss=0.6128654456694012\n",
      "Logistic Regression (23/5000): loss=0.6145932097056803\n",
      "Logistic Regression (24/5000): loss=0.6113020886381118\n",
      "Logistic Regression (25/5000): loss=0.6138905450468131\n",
      "Logistic Regression (26/5000): loss=0.6116211798986062\n",
      "Logistic Regression (27/5000): loss=0.6101937708539334\n",
      "Logistic Regression (28/5000): loss=0.6182044387755331\n",
      "Logistic Regression (29/5000): loss=0.6188109133924974\n",
      "Logistic Regression (30/5000): loss=0.6123381800540363\n",
      "Logistic Regression (31/5000): loss=0.6087523906671439\n",
      "Logistic Regression (32/5000): loss=0.6082988000086293\n",
      "Logistic Regression (33/5000): loss=0.612907639818659\n",
      "Logistic Regression (34/5000): loss=0.6074648952406551\n",
      "Logistic Regression (35/5000): loss=0.6073238624523754\n",
      "Logistic Regression (36/5000): loss=0.6070511185266888\n",
      "Logistic Regression (37/5000): loss=0.6069705995801431\n",
      "Logistic Regression (38/5000): loss=0.6105026481450624\n",
      "Logistic Regression (39/5000): loss=0.6075067892386761\n",
      "Logistic Regression (40/5000): loss=0.6086994029901409\n",
      "Logistic Regression (41/5000): loss=0.609841804203105\n",
      "Logistic Regression (42/5000): loss=0.6071674846919777\n",
      "Logistic Regression (43/5000): loss=0.6066733987378892\n",
      "Logistic Regression (44/5000): loss=0.6096586296118985\n",
      "Logistic Regression (45/5000): loss=0.610345125213016\n",
      "Logistic Regression (46/5000): loss=0.6080921582475769\n",
      "Logistic Regression (47/5000): loss=0.6061120858346601\n",
      "Logistic Regression (48/5000): loss=0.6144890900061097\n",
      "Logistic Regression (49/5000): loss=0.6042463092327665\n",
      "Logistic Regression (50/5000): loss=0.6065540628530299\n",
      "Logistic Regression (51/5000): loss=0.603209375888689\n",
      "Logistic Regression (52/5000): loss=0.604252483360681\n",
      "Logistic Regression (53/5000): loss=0.6032926101404812\n",
      "Logistic Regression (54/5000): loss=0.6164387769058624\n",
      "Logistic Regression (55/5000): loss=0.6080418296357433\n",
      "Logistic Regression (56/5000): loss=0.6092081144123204\n",
      "Logistic Regression (57/5000): loss=0.6030866625694036\n",
      "Logistic Regression (58/5000): loss=0.6152616497845982\n",
      "Logistic Regression (59/5000): loss=0.6018856895648872\n",
      "Logistic Regression (60/5000): loss=0.610854297837499\n",
      "Logistic Regression (61/5000): loss=0.6021907608853003\n",
      "Logistic Regression (62/5000): loss=0.6160467140681071\n",
      "Logistic Regression (63/5000): loss=0.6094506816818708\n",
      "Logistic Regression (64/5000): loss=0.6016814720744922\n",
      "Logistic Regression (65/5000): loss=0.60089265462611\n",
      "Logistic Regression (66/5000): loss=0.6015767057571959\n",
      "Logistic Regression (67/5000): loss=0.6006607829642742\n",
      "Logistic Regression (68/5000): loss=0.6011064842045831\n",
      "Logistic Regression (69/5000): loss=0.605525667669464\n",
      "Logistic Regression (70/5000): loss=0.6115661617391698\n",
      "Logistic Regression (71/5000): loss=0.6018876526840133\n",
      "Logistic Regression (72/5000): loss=0.6067956357759436\n",
      "Logistic Regression (73/5000): loss=0.5998701011618875\n",
      "Logistic Regression (74/5000): loss=0.5997832193843083\n",
      "Logistic Regression (75/5000): loss=0.6002054776300534\n",
      "Logistic Regression (76/5000): loss=0.6018212979964408\n",
      "Logistic Regression (77/5000): loss=0.599411628250718\n",
      "Logistic Regression (78/5000): loss=0.5994622781873179\n",
      "Logistic Regression (79/5000): loss=0.5987829205137435\n",
      "Logistic Regression (80/5000): loss=0.6000896896156613\n",
      "Logistic Regression (81/5000): loss=0.5993351607752622\n",
      "Logistic Regression (82/5000): loss=0.5999455842862843\n",
      "Logistic Regression (83/5000): loss=0.6009397830658063\n",
      "Logistic Regression (84/5000): loss=0.6015300954173847\n",
      "Logistic Regression (85/5000): loss=0.6013506142329464\n",
      "Logistic Regression (86/5000): loss=0.6013665910096007\n",
      "Logistic Regression (87/5000): loss=0.6005300203003316\n",
      "Logistic Regression (88/5000): loss=0.600703638857262\n",
      "Logistic Regression (89/5000): loss=0.5986039942371459\n",
      "Logistic Regression (90/5000): loss=0.5982329859655592\n",
      "Logistic Regression (91/5000): loss=0.6026816394447854\n",
      "Logistic Regression (92/5000): loss=0.5987638206936442\n",
      "Logistic Regression (93/5000): loss=0.5979175468983049\n",
      "Logistic Regression (94/5000): loss=0.6031671120070525\n",
      "Logistic Regression (95/5000): loss=0.5987636071890408\n",
      "Logistic Regression (96/5000): loss=0.5983859515670604\n",
      "Logistic Regression (97/5000): loss=0.5978470680961557\n",
      "Logistic Regression (98/5000): loss=0.5988787537841023\n",
      "Logistic Regression (99/5000): loss=0.6073672566592885\n",
      "Logistic Regression (100/5000): loss=0.6010144893455488\n",
      "Logistic Regression (101/5000): loss=0.5988336396835932\n",
      "Logistic Regression (102/5000): loss=0.6075804613243928\n",
      "Logistic Regression (103/5000): loss=0.5978437599654318\n",
      "Logistic Regression (104/5000): loss=0.5993230459985002\n",
      "Logistic Regression (105/5000): loss=0.5969064837663052\n",
      "Logistic Regression (106/5000): loss=0.596715043147361\n",
      "Logistic Regression (107/5000): loss=0.5981492754240808\n",
      "Logistic Regression (108/5000): loss=0.5978437313845628\n",
      "Logistic Regression (109/5000): loss=0.5964571573703115\n",
      "Logistic Regression (110/5000): loss=0.6007472444551657\n",
      "Logistic Regression (111/5000): loss=0.606462061767743\n",
      "Logistic Regression (112/5000): loss=0.6103996718697634\n",
      "Logistic Regression (113/5000): loss=0.6036522507434127\n",
      "Logistic Regression (114/5000): loss=0.597280666466598\n",
      "Logistic Regression (115/5000): loss=0.5961728888681088\n",
      "Logistic Regression (116/5000): loss=0.5955254104415032\n",
      "Logistic Regression (117/5000): loss=0.5954147416739706\n",
      "Logistic Regression (118/5000): loss=0.6033603745590786\n",
      "Logistic Regression (119/5000): loss=0.5959282874415901\n",
      "Logistic Regression (120/5000): loss=0.5988663851195224\n",
      "Logistic Regression (121/5000): loss=0.6041966925992199\n",
      "Logistic Regression (122/5000): loss=0.5961588168634898\n",
      "Logistic Regression (123/5000): loss=0.5957101023158377\n",
      "Logistic Regression (124/5000): loss=0.5980588793373944\n",
      "Logistic Regression (125/5000): loss=0.596099862793046\n",
      "Logistic Regression (126/5000): loss=0.5985506749166627\n",
      "Logistic Regression (127/5000): loss=0.5957286543068788\n",
      "Logistic Regression (128/5000): loss=0.5965658190381851\n",
      "Logistic Regression (129/5000): loss=0.5990082200341692\n",
      "Logistic Regression (130/5000): loss=0.5999938617461997\n",
      "Logistic Regression (131/5000): loss=0.5961227528839778\n",
      "Logistic Regression (132/5000): loss=0.5956266717243432\n",
      "Logistic Regression (133/5000): loss=0.5980135871867148\n",
      "Logistic Regression (134/5000): loss=0.5945169930874089\n",
      "Logistic Regression (135/5000): loss=0.5940972291330344\n",
      "Logistic Regression (136/5000): loss=0.5972641498954702\n",
      "Logistic Regression (137/5000): loss=0.6006324161797054\n",
      "Logistic Regression (138/5000): loss=0.6011324342181218\n",
      "Logistic Regression (139/5000): loss=0.5943088895359907\n",
      "Logistic Regression (140/5000): loss=0.6060928539556614\n",
      "Logistic Regression (141/5000): loss=0.5943503521512297\n",
      "Logistic Regression (142/5000): loss=0.6050704405723311\n",
      "Logistic Regression (143/5000): loss=0.5960926475196849\n",
      "Logistic Regression (144/5000): loss=0.5939688592040261\n",
      "Logistic Regression (145/5000): loss=0.5966298959104221\n",
      "Logistic Regression (146/5000): loss=0.5996776116472922\n",
      "Logistic Regression (147/5000): loss=0.6069190166201459\n",
      "Logistic Regression (148/5000): loss=0.6028872850895524\n",
      "Logistic Regression (149/5000): loss=0.594802685074253\n",
      "Logistic Regression (150/5000): loss=0.5944987221217535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (151/5000): loss=0.5938553391884482\n",
      "Logistic Regression (152/5000): loss=0.5940086395463633\n",
      "Logistic Regression (153/5000): loss=0.5930390214940345\n",
      "Logistic Regression (154/5000): loss=0.5982186974293753\n",
      "Logistic Regression (155/5000): loss=0.5928192037847065\n",
      "Logistic Regression (156/5000): loss=0.5986629690070696\n",
      "Logistic Regression (157/5000): loss=0.5929988325868693\n",
      "Logistic Regression (158/5000): loss=0.5933028930047292\n",
      "Logistic Regression (159/5000): loss=0.594595085871125\n",
      "Logistic Regression (160/5000): loss=0.5939136307647199\n",
      "Logistic Regression (161/5000): loss=0.5929523905244499\n",
      "Logistic Regression (162/5000): loss=0.5943786277564879\n",
      "Logistic Regression (163/5000): loss=0.5927295547980943\n",
      "Logistic Regression (164/5000): loss=0.5929209858430959\n",
      "Logistic Regression (165/5000): loss=0.5932823998404391\n",
      "Logistic Regression (166/5000): loss=0.5923464508567464\n",
      "Logistic Regression (167/5000): loss=0.5922579712440922\n",
      "Logistic Regression (168/5000): loss=0.5926478420908201\n",
      "Logistic Regression (169/5000): loss=0.5987124004369667\n",
      "Logistic Regression (170/5000): loss=0.5949716039282117\n",
      "Logistic Regression (171/5000): loss=0.6143223674298877\n",
      "Logistic Regression (172/5000): loss=0.5946941450080329\n",
      "Logistic Regression (173/5000): loss=0.6092863592240153\n",
      "Logistic Regression (174/5000): loss=0.5927122880743773\n",
      "Logistic Regression (175/5000): loss=0.592703627822255\n",
      "Logistic Regression (176/5000): loss=0.591718567407993\n",
      "Logistic Regression (177/5000): loss=0.5923204766849821\n",
      "Logistic Regression (178/5000): loss=0.5946939540520199\n",
      "Logistic Regression (179/5000): loss=0.6055598972099656\n",
      "Logistic Regression (180/5000): loss=0.5952005370527708\n",
      "Logistic Regression (181/5000): loss=0.5942667948791778\n",
      "Logistic Regression (182/5000): loss=0.5923830774734848\n",
      "Logistic Regression (183/5000): loss=0.6144845317833786\n",
      "Logistic Regression (184/5000): loss=0.6029940081310591\n",
      "Logistic Regression (185/5000): loss=0.5921145324470796\n",
      "Logistic Regression (186/5000): loss=0.5909354714677835\n",
      "Logistic Regression (187/5000): loss=0.5949054802282391\n",
      "Logistic Regression (188/5000): loss=0.5911031556332136\n",
      "Logistic Regression (189/5000): loss=0.5964319861493208\n",
      "Logistic Regression (190/5000): loss=0.5911716288280275\n",
      "Logistic Regression (191/5000): loss=0.594809681761556\n",
      "Logistic Regression (192/5000): loss=0.591190159731289\n",
      "Logistic Regression (193/5000): loss=0.597280863637191\n",
      "Logistic Regression (194/5000): loss=0.5908267220276611\n",
      "Logistic Regression (195/5000): loss=0.591017508172576\n",
      "Logistic Regression (196/5000): loss=0.5907398008773648\n",
      "Logistic Regression (197/5000): loss=0.6010005283644729\n",
      "Logistic Regression (198/5000): loss=0.5904555895671697\n",
      "Logistic Regression (199/5000): loss=0.5907449150476856\n",
      "Logistic Regression (200/5000): loss=0.5906381000936093\n",
      "Logistic Regression (201/5000): loss=0.5953609488794606\n",
      "Logistic Regression (202/5000): loss=0.6070868552347186\n",
      "Logistic Regression (203/5000): loss=0.5956622874536894\n",
      "Logistic Regression (204/5000): loss=0.5924956310916178\n",
      "Logistic Regression (205/5000): loss=0.594943324246404\n",
      "Logistic Regression (206/5000): loss=0.5922829820295084\n",
      "Logistic Regression (207/5000): loss=0.6151375020673541\n",
      "Logistic Regression (208/5000): loss=0.5903436571616715\n",
      "Logistic Regression (209/5000): loss=0.597598434295388\n",
      "Logistic Regression (210/5000): loss=0.5928120983986335\n",
      "Logistic Regression (211/5000): loss=0.5929755897532359\n",
      "Logistic Regression (212/5000): loss=0.590028309313834\n",
      "Logistic Regression (213/5000): loss=0.5902823597472386\n",
      "Logistic Regression (214/5000): loss=0.5901112104318184\n",
      "Logistic Regression (215/5000): loss=0.6034590929857784\n",
      "Logistic Regression (216/5000): loss=0.5920033971384396\n",
      "Logistic Regression (217/5000): loss=0.6060079575833948\n",
      "Logistic Regression (218/5000): loss=0.5893642269841927\n",
      "Logistic Regression (219/5000): loss=0.5998123828208909\n",
      "Logistic Regression (220/5000): loss=0.5895901926115201\n",
      "Logistic Regression (221/5000): loss=0.5914283848061864\n",
      "Logistic Regression (222/5000): loss=0.6069712374382276\n",
      "Logistic Regression (223/5000): loss=0.5918506551748807\n",
      "Logistic Regression (224/5000): loss=0.6011217407381592\n",
      "Logistic Regression (225/5000): loss=0.5895356509372296\n",
      "Logistic Regression (226/5000): loss=0.5954687274838563\n",
      "Logistic Regression (227/5000): loss=0.5940462129240422\n",
      "Logistic Regression (228/5000): loss=0.5973621865639471\n",
      "Logistic Regression (229/5000): loss=0.598366412817443\n",
      "Logistic Regression (230/5000): loss=0.5890874255588898\n",
      "Logistic Regression (231/5000): loss=0.5968779046202973\n",
      "Logistic Regression (232/5000): loss=0.5920947341485048\n",
      "Logistic Regression (233/5000): loss=0.5933218425584897\n",
      "Logistic Regression (234/5000): loss=0.5900056532580511\n",
      "Logistic Regression (235/5000): loss=0.5902809747721149\n",
      "Logistic Regression (236/5000): loss=0.5899562405479429\n",
      "Logistic Regression (237/5000): loss=0.5935804023154828\n",
      "Logistic Regression (238/5000): loss=0.5888626653678604\n",
      "Logistic Regression (239/5000): loss=0.5887542034663293\n",
      "Logistic Regression (240/5000): loss=0.591128990986768\n",
      "Logistic Regression (241/5000): loss=0.5924822640236357\n",
      "Logistic Regression (242/5000): loss=0.5913042148751712\n",
      "Logistic Regression (243/5000): loss=0.5895575464904087\n",
      "Logistic Regression (244/5000): loss=0.5904019103950114\n",
      "Logistic Regression (245/5000): loss=0.5928768451686812\n",
      "Logistic Regression (246/5000): loss=0.6252657381569898\n",
      "Logistic Regression (247/5000): loss=0.6246830428233547\n",
      "Logistic Regression (248/5000): loss=0.5956925205415692\n",
      "Logistic Regression (249/5000): loss=0.5889998155169935\n",
      "Logistic Regression (250/5000): loss=0.5968814066535516\n",
      "Logistic Regression (251/5000): loss=0.5945059398840948\n",
      "Logistic Regression (252/5000): loss=0.5884862628320777\n",
      "Logistic Regression (253/5000): loss=0.5895254276009732\n",
      "Logistic Regression (254/5000): loss=0.5892516318508126\n",
      "Logistic Regression (255/5000): loss=0.5923122116757283\n",
      "Logistic Regression (256/5000): loss=0.5881719794207189\n",
      "Logistic Regression (257/5000): loss=0.5880735465179713\n",
      "Logistic Regression (258/5000): loss=0.596062962451462\n",
      "Logistic Regression (259/5000): loss=0.5888797274713448\n",
      "Logistic Regression (260/5000): loss=0.5880288639993763\n",
      "Logistic Regression (261/5000): loss=0.5892379789470049\n",
      "Logistic Regression (262/5000): loss=0.5895612844898642\n",
      "Logistic Regression (263/5000): loss=0.5880667374610923\n",
      "Logistic Regression (264/5000): loss=0.5932129358069461\n",
      "Logistic Regression (265/5000): loss=0.592161943806936\n",
      "Logistic Regression (266/5000): loss=0.5880353154433262\n",
      "Logistic Regression (267/5000): loss=0.588297193045006\n",
      "Logistic Regression (268/5000): loss=0.5960624149554997\n",
      "Logistic Regression (269/5000): loss=0.5917624847702964\n",
      "Logistic Regression (270/5000): loss=0.5877178300108135\n",
      "Logistic Regression (271/5000): loss=0.5905724328316919\n",
      "Logistic Regression (272/5000): loss=0.5891987226362931\n",
      "Logistic Regression (273/5000): loss=0.6259488326771206\n",
      "Logistic Regression (274/5000): loss=0.5876142412538686\n",
      "Logistic Regression (275/5000): loss=0.587746433602793\n",
      "Logistic Regression (276/5000): loss=0.5877085222938243\n",
      "Logistic Regression (277/5000): loss=0.5878067841544353\n",
      "Logistic Regression (278/5000): loss=0.5878172781868598\n",
      "Logistic Regression (279/5000): loss=0.6040242493431288\n",
      "Logistic Regression (280/5000): loss=0.590769090690879\n",
      "Logistic Regression (281/5000): loss=0.5917753839432404\n",
      "Logistic Regression (282/5000): loss=0.5964936981691575\n",
      "Logistic Regression (283/5000): loss=0.5913213427509779\n",
      "Logistic Regression (284/5000): loss=0.5907079097401127\n",
      "Logistic Regression (285/5000): loss=0.6031168260129357\n",
      "Logistic Regression (286/5000): loss=0.5874304651899733\n",
      "Logistic Regression (287/5000): loss=0.6020310372791944\n",
      "Logistic Regression (288/5000): loss=0.5907350017512824\n",
      "Logistic Regression (289/5000): loss=0.5899444699896434\n",
      "Logistic Regression (290/5000): loss=0.5879491138880958\n",
      "Logistic Regression (291/5000): loss=0.588037432946952\n",
      "Logistic Regression (292/5000): loss=0.5876844486715571\n",
      "Logistic Regression (293/5000): loss=0.5957020470819303\n",
      "Logistic Regression (294/5000): loss=0.5995854071324555\n",
      "Logistic Regression (295/5000): loss=0.587742891412182\n",
      "Logistic Regression (296/5000): loss=0.5940084126601543\n",
      "Logistic Regression (297/5000): loss=0.5882704237955918\n",
      "Logistic Regression (298/5000): loss=0.5878875554649834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (299/5000): loss=0.5878943776048648\n",
      "Logistic Regression (300/5000): loss=0.613775852368613\n",
      "Logistic Regression (301/5000): loss=0.5872927487244392\n",
      "Logistic Regression (302/5000): loss=0.5876262188937605\n",
      "Logistic Regression (303/5000): loss=0.5874379859170487\n",
      "Logistic Regression (304/5000): loss=0.5892034335268357\n",
      "Logistic Regression (305/5000): loss=0.5954695900691734\n",
      "Logistic Regression (306/5000): loss=0.5869279018940862\n",
      "Logistic Regression (307/5000): loss=0.5906673734656793\n",
      "Logistic Regression (308/5000): loss=0.604275300781323\n",
      "Logistic Regression (309/5000): loss=0.5989771656777513\n",
      "Logistic Regression (310/5000): loss=0.5882266589451831\n",
      "Logistic Regression (311/5000): loss=0.5898349184981067\n",
      "Logistic Regression (312/5000): loss=0.5871581473678659\n",
      "Logistic Regression (313/5000): loss=0.5866817652419787\n",
      "Logistic Regression (314/5000): loss=0.5867147316569303\n",
      "Logistic Regression (315/5000): loss=0.5873111312692559\n",
      "Logistic Regression (316/5000): loss=0.5872215317772688\n",
      "Logistic Regression (317/5000): loss=0.5864542200618428\n",
      "Logistic Regression (318/5000): loss=0.5908300975546272\n",
      "Logistic Regression (319/5000): loss=0.5908561720473203\n",
      "Logistic Regression (320/5000): loss=0.5865722597752551\n",
      "Logistic Regression (321/5000): loss=0.5887534287432992\n",
      "Logistic Regression (322/5000): loss=0.6058541438497306\n",
      "Logistic Regression (323/5000): loss=0.6010590779288157\n",
      "Logistic Regression (324/5000): loss=0.5879343868045422\n",
      "Logistic Regression (325/5000): loss=0.5892900493718349\n",
      "Logistic Regression (326/5000): loss=0.5901286605234632\n",
      "Logistic Regression (327/5000): loss=0.5951216231433488\n",
      "Logistic Regression (328/5000): loss=0.5886591036768118\n",
      "Logistic Regression (329/5000): loss=0.6017040370086308\n",
      "Logistic Regression (330/5000): loss=0.5881283870840557\n",
      "Logistic Regression (331/5000): loss=0.587263025051856\n",
      "Logistic Regression (332/5000): loss=0.5866068468847705\n",
      "Logistic Regression (333/5000): loss=0.5894743451128748\n",
      "Logistic Regression (334/5000): loss=0.6102569999569315\n",
      "Logistic Regression (335/5000): loss=0.6023275709396047\n",
      "Logistic Regression (336/5000): loss=0.5943923473094879\n",
      "Logistic Regression (337/5000): loss=0.589596852521804\n",
      "Logistic Regression (338/5000): loss=0.5876879085533893\n",
      "Logistic Regression (339/5000): loss=0.5860327718863632\n",
      "Logistic Regression (340/5000): loss=0.5864769368764146\n",
      "Logistic Regression (341/5000): loss=0.5890674840089309\n",
      "Logistic Regression (342/5000): loss=0.5871580759533297\n",
      "Logistic Regression (343/5000): loss=0.5927031946039821\n",
      "Logistic Regression (344/5000): loss=0.5910726591937363\n",
      "Logistic Regression (345/5000): loss=0.5860478644455499\n",
      "Logistic Regression (346/5000): loss=0.5980940538254071\n",
      "Logistic Regression (347/5000): loss=0.5898236793298758\n",
      "Logistic Regression (348/5000): loss=0.5868178123820397\n",
      "Logistic Regression (349/5000): loss=0.5885363163571777\n",
      "Logistic Regression (350/5000): loss=0.5860904309694523\n",
      "Logistic Regression (351/5000): loss=0.5887295957746096\n",
      "Logistic Regression (352/5000): loss=0.5873573576894486\n",
      "Logistic Regression (353/5000): loss=0.588716636204026\n",
      "Logistic Regression (354/5000): loss=0.5865589584254197\n",
      "Logistic Regression (355/5000): loss=0.5862138058658999\n",
      "Logistic Regression (356/5000): loss=0.5872564076474174\n",
      "Logistic Regression (357/5000): loss=0.5918106679776229\n",
      "Logistic Regression (358/5000): loss=0.585832340293636\n",
      "Logistic Regression (359/5000): loss=0.5879945545115243\n",
      "Logistic Regression (360/5000): loss=0.5890584078300842\n",
      "Logistic Regression (361/5000): loss=0.5881448456224858\n",
      "Logistic Regression (362/5000): loss=0.5964204661415496\n",
      "Logistic Regression (363/5000): loss=0.5883107039074289\n",
      "Logistic Regression (364/5000): loss=0.5855587284411391\n",
      "Logistic Regression (365/5000): loss=0.5855446894712318\n",
      "Logistic Regression (366/5000): loss=0.5910256185601006\n",
      "Logistic Regression (367/5000): loss=0.5899801276383805\n",
      "Logistic Regression (368/5000): loss=0.5895747276165983\n",
      "Logistic Regression (369/5000): loss=0.5888675321615517\n",
      "Logistic Regression (370/5000): loss=0.5866312437465854\n",
      "Logistic Regression (371/5000): loss=0.5854560414076885\n",
      "Logistic Regression (372/5000): loss=0.5854161193021186\n",
      "Logistic Regression (373/5000): loss=0.5937454420613055\n",
      "Logistic Regression (374/5000): loss=0.5866946903912653\n",
      "Logistic Regression (375/5000): loss=0.5874560877046922\n",
      "Logistic Regression (376/5000): loss=0.587880425412817\n",
      "Logistic Regression (377/5000): loss=0.5861811812146588\n",
      "Logistic Regression (378/5000): loss=0.5854396421889909\n",
      "Logistic Regression (379/5000): loss=0.5851657941414709\n",
      "Logistic Regression (380/5000): loss=0.587014860614748\n",
      "Logistic Regression (381/5000): loss=0.5942647717063622\n",
      "Logistic Regression (382/5000): loss=0.5882457349104739\n",
      "Logistic Regression (383/5000): loss=0.5856336800788987\n",
      "Logistic Regression (384/5000): loss=0.5850248629645264\n",
      "Logistic Regression (385/5000): loss=0.5848593034172105\n",
      "Logistic Regression (386/5000): loss=0.5922577634313115\n",
      "Logistic Regression (387/5000): loss=0.5918298282370246\n",
      "Logistic Regression (388/5000): loss=0.5863045198416929\n",
      "Logistic Regression (389/5000): loss=0.585646819190426\n",
      "Logistic Regression (390/5000): loss=0.5860229127216136\n",
      "Logistic Regression (391/5000): loss=0.5853129875277915\n",
      "Logistic Regression (392/5000): loss=0.5850324697433341\n",
      "Logistic Regression (393/5000): loss=0.5884648641537479\n",
      "Logistic Regression (394/5000): loss=0.584910565863347\n",
      "Logistic Regression (395/5000): loss=0.6221072894277191\n",
      "Logistic Regression (396/5000): loss=0.5870573812929006\n",
      "Logistic Regression (397/5000): loss=0.5860121795389763\n",
      "Logistic Regression (398/5000): loss=0.5857752518181804\n",
      "Logistic Regression (399/5000): loss=0.6069810408529632\n",
      "Logistic Regression (400/5000): loss=0.5913075880165796\n",
      "Logistic Regression (401/5000): loss=0.5849721505036518\n",
      "Logistic Regression (402/5000): loss=0.5858869637356496\n",
      "Logistic Regression (403/5000): loss=0.5903495303663061\n",
      "Logistic Regression (404/5000): loss=0.5862066537722663\n",
      "Logistic Regression (405/5000): loss=0.5988458269401473\n",
      "Logistic Regression (406/5000): loss=0.5893347591366509\n",
      "Logistic Regression (407/5000): loss=0.5849106143566458\n",
      "Logistic Regression (408/5000): loss=0.5869088283563958\n",
      "Logistic Regression (409/5000): loss=0.5894497822056667\n",
      "Logistic Regression (410/5000): loss=0.5891283508960229\n",
      "Logistic Regression (411/5000): loss=0.5850295785862085\n",
      "Logistic Regression (412/5000): loss=0.5886733572605567\n",
      "Logistic Regression (413/5000): loss=0.5879188568305656\n",
      "Logistic Regression (414/5000): loss=0.587315150570432\n",
      "Logistic Regression (415/5000): loss=0.5929385920787537\n",
      "Logistic Regression (416/5000): loss=0.5851693390119246\n",
      "Logistic Regression (417/5000): loss=0.595144761798939\n",
      "Logistic Regression (418/5000): loss=0.5845069149415691\n",
      "Logistic Regression (419/5000): loss=0.5855568846501126\n",
      "Logistic Regression (420/5000): loss=0.5959370165696625\n",
      "Logistic Regression (421/5000): loss=0.5866880715446102\n",
      "Logistic Regression (422/5000): loss=0.5944164047145233\n",
      "Logistic Regression (423/5000): loss=0.5848357915719169\n",
      "Logistic Regression (424/5000): loss=0.5865317447464833\n",
      "Logistic Regression (425/5000): loss=0.585371133323749\n",
      "Logistic Regression (426/5000): loss=0.5847757546022325\n",
      "Logistic Regression (427/5000): loss=0.5946893558222991\n",
      "Logistic Regression (428/5000): loss=0.586113056869816\n",
      "Logistic Regression (429/5000): loss=0.5948178472581077\n",
      "Logistic Regression (430/5000): loss=0.5842563488842101\n",
      "Logistic Regression (431/5000): loss=0.6141906340995826\n",
      "Logistic Regression (432/5000): loss=0.5891606067543302\n",
      "Logistic Regression (433/5000): loss=0.5849704468519811\n",
      "Logistic Regression (434/5000): loss=0.5924169588521803\n",
      "Logistic Regression (435/5000): loss=0.5939143865894162\n",
      "Logistic Regression (436/5000): loss=0.584853541226916\n",
      "Logistic Regression (437/5000): loss=0.5929965720382021\n",
      "Logistic Regression (438/5000): loss=0.5845089897434944\n",
      "Logistic Regression (439/5000): loss=0.589472581355194\n",
      "Logistic Regression (440/5000): loss=0.5862323382687933\n",
      "Logistic Regression (441/5000): loss=0.5940992392217587\n",
      "Logistic Regression (442/5000): loss=0.5926288869350134\n",
      "Logistic Regression (443/5000): loss=0.5883420646374569\n",
      "Logistic Regression (444/5000): loss=0.6135322087602492\n",
      "Logistic Regression (445/5000): loss=0.5871084805640207\n",
      "Logistic Regression (446/5000): loss=0.6020166448211804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (447/5000): loss=0.5847075957131748\n",
      "Logistic Regression (448/5000): loss=0.5842949841192122\n",
      "Logistic Regression (449/5000): loss=0.5856893184839129\n",
      "Logistic Regression (450/5000): loss=0.5878407726588074\n",
      "Logistic Regression (451/5000): loss=0.5836023091316677\n",
      "Logistic Regression (452/5000): loss=0.5847109351060363\n",
      "Logistic Regression (453/5000): loss=0.5954172871705211\n",
      "Logistic Regression (454/5000): loss=0.5962443962587632\n",
      "Logistic Regression (455/5000): loss=0.5876189025843928\n",
      "Logistic Regression (456/5000): loss=0.5867688819534329\n",
      "Logistic Regression (457/5000): loss=0.5849837447108668\n",
      "Logistic Regression (458/5000): loss=0.5888203944984217\n",
      "Logistic Regression (459/5000): loss=0.5840064986630918\n",
      "Logistic Regression (460/5000): loss=0.5909947474259206\n",
      "Logistic Regression (461/5000): loss=0.5840678085518478\n",
      "Logistic Regression (462/5000): loss=0.5891929634813244\n",
      "Logistic Regression (463/5000): loss=0.5879548809564906\n",
      "Logistic Regression (464/5000): loss=0.5840897253913927\n",
      "Logistic Regression (465/5000): loss=0.5874321783936872\n",
      "Logistic Regression (466/5000): loss=0.5839148772497139\n",
      "Logistic Regression (467/5000): loss=0.5956631673940361\n",
      "Logistic Regression (468/5000): loss=0.585389605321374\n",
      "Logistic Regression (469/5000): loss=0.583752383814325\n",
      "Logistic Regression (470/5000): loss=0.5836758192017957\n",
      "Logistic Regression (471/5000): loss=0.585163391971226\n",
      "Logistic Regression (472/5000): loss=0.5832311407681251\n",
      "Logistic Regression (473/5000): loss=0.585618391062277\n",
      "Logistic Regression (474/5000): loss=0.5830287183984452\n",
      "Logistic Regression (475/5000): loss=0.6076026669686226\n",
      "Logistic Regression (476/5000): loss=0.6048842538535218\n",
      "Logistic Regression (477/5000): loss=0.5832015191583191\n",
      "Logistic Regression (478/5000): loss=0.5849641231945415\n",
      "Logistic Regression (479/5000): loss=0.5835572892145373\n",
      "Logistic Regression (480/5000): loss=0.5861082139361792\n",
      "Logistic Regression (481/5000): loss=0.5888357629371512\n",
      "Logistic Regression (482/5000): loss=0.5966886905676931\n",
      "Logistic Regression (483/5000): loss=0.5856428944678657\n",
      "Logistic Regression (484/5000): loss=0.585082713439247\n",
      "Logistic Regression (485/5000): loss=0.5932089369447647\n",
      "Logistic Regression (486/5000): loss=0.5860386661115626\n",
      "Logistic Regression (487/5000): loss=0.5846305764559305\n",
      "Logistic Regression (488/5000): loss=0.5837542985770406\n",
      "Logistic Regression (489/5000): loss=0.5855781794853047\n",
      "Logistic Regression (490/5000): loss=0.5846305772099847\n",
      "Logistic Regression (491/5000): loss=0.5893304600964815\n",
      "Logistic Regression (492/5000): loss=0.5832602024548028\n",
      "Logistic Regression (493/5000): loss=0.589183492748263\n",
      "Logistic Regression (494/5000): loss=0.5868071155022967\n",
      "Logistic Regression (495/5000): loss=0.584710988735421\n",
      "Logistic Regression (496/5000): loss=0.5912247755528858\n",
      "Logistic Regression (497/5000): loss=0.5834170541935237\n",
      "Logistic Regression (498/5000): loss=0.5844031918024143\n",
      "Logistic Regression (499/5000): loss=0.5831186032729576\n",
      "Logistic Regression (500/5000): loss=0.5863207875480297\n",
      "Logistic Regression (501/5000): loss=0.590746370950236\n",
      "Logistic Regression (502/5000): loss=0.5925793659419314\n",
      "Logistic Regression (503/5000): loss=0.5903006660070648\n",
      "Logistic Regression (504/5000): loss=0.5909969790856577\n",
      "Logistic Regression (505/5000): loss=0.5865042077833693\n",
      "Logistic Regression (506/5000): loss=0.5829596998157636\n",
      "Logistic Regression (507/5000): loss=0.5890966064792498\n",
      "Logistic Regression (508/5000): loss=0.5854127906686712\n",
      "Logistic Regression (509/5000): loss=0.5873999753796407\n",
      "Logistic Regression (510/5000): loss=0.5827189807951598\n",
      "Logistic Regression (511/5000): loss=0.5841762994321842\n",
      "Logistic Regression (512/5000): loss=0.5862102706742774\n",
      "Logistic Regression (513/5000): loss=0.5834538328255817\n",
      "Logistic Regression (514/5000): loss=0.582655797837218\n",
      "Logistic Regression (515/5000): loss=0.5911306339493801\n",
      "Logistic Regression (516/5000): loss=0.5827330051707762\n",
      "Logistic Regression (517/5000): loss=0.5853184441404005\n",
      "Logistic Regression (518/5000): loss=0.5865880310501383\n",
      "Logistic Regression (519/5000): loss=0.5828582718316297\n",
      "Logistic Regression (520/5000): loss=0.5845504595363807\n",
      "Logistic Regression (521/5000): loss=0.5824871118148136\n",
      "Logistic Regression (522/5000): loss=0.5843909705777799\n",
      "Logistic Regression (523/5000): loss=0.5826516201732098\n",
      "Logistic Regression (524/5000): loss=0.5828624987914318\n",
      "Logistic Regression (525/5000): loss=0.584263208843005\n",
      "Logistic Regression (526/5000): loss=0.5832191659667899\n",
      "Logistic Regression (527/5000): loss=0.5836050762402433\n",
      "Logistic Regression (528/5000): loss=0.5830106882373594\n",
      "Logistic Regression (529/5000): loss=0.5839389145439867\n",
      "Logistic Regression (530/5000): loss=0.5823778278973833\n",
      "Logistic Regression (531/5000): loss=0.5836421003409196\n",
      "Logistic Regression (532/5000): loss=0.5968846020669654\n",
      "Logistic Regression (533/5000): loss=0.5851303261682744\n",
      "Logistic Regression (534/5000): loss=0.5824019771174989\n",
      "Logistic Regression (535/5000): loss=0.5846348930017687\n",
      "Logistic Regression (536/5000): loss=0.5829309819877119\n",
      "Logistic Regression (537/5000): loss=0.5903876846711776\n",
      "Logistic Regression (538/5000): loss=0.5903191532189214\n",
      "Logistic Regression (539/5000): loss=0.5840718825828702\n",
      "Logistic Regression (540/5000): loss=0.5853386063223294\n",
      "Logistic Regression (541/5000): loss=0.5860001219807411\n",
      "Logistic Regression (542/5000): loss=0.5820118335947584\n",
      "Logistic Regression (543/5000): loss=0.5836275256201793\n",
      "Logistic Regression (544/5000): loss=0.5865905339619684\n",
      "Logistic Regression (545/5000): loss=0.5847799944282438\n",
      "Logistic Regression (546/5000): loss=0.5831972164802008\n",
      "Logistic Regression (547/5000): loss=0.5824798458124334\n",
      "Logistic Regression (548/5000): loss=0.5819175481292156\n",
      "Logistic Regression (549/5000): loss=0.5893881122565301\n",
      "Logistic Regression (550/5000): loss=0.5867456527531778\n",
      "Logistic Regression (551/5000): loss=0.5876691498798688\n",
      "Logistic Regression (552/5000): loss=0.5823604346295151\n",
      "Logistic Regression (553/5000): loss=0.5817064854442455\n",
      "Logistic Regression (554/5000): loss=0.5816347820828042\n",
      "Logistic Regression (555/5000): loss=0.5949663626964964\n",
      "Logistic Regression (556/5000): loss=0.5821717803861106\n",
      "Logistic Regression (557/5000): loss=0.58460549178019\n",
      "Logistic Regression (558/5000): loss=0.5815756684288903\n",
      "Logistic Regression (559/5000): loss=0.5816376406763792\n",
      "Logistic Regression (560/5000): loss=0.5817141146072343\n",
      "Logistic Regression (561/5000): loss=0.597104661957355\n",
      "Logistic Regression (562/5000): loss=0.5933199887568833\n",
      "Logistic Regression (563/5000): loss=0.5821101300499152\n",
      "Logistic Regression (564/5000): loss=0.589799438300115\n",
      "Logistic Regression (565/5000): loss=0.5888912205957489\n",
      "Logistic Regression (566/5000): loss=0.5835133789110303\n",
      "Logistic Regression (567/5000): loss=0.5846010317934586\n",
      "Logistic Regression (568/5000): loss=0.5853957703080033\n",
      "Logistic Regression (569/5000): loss=0.5844169961112802\n",
      "Logistic Regression (570/5000): loss=0.5891957358531448\n",
      "Logistic Regression (571/5000): loss=0.5819256881744802\n",
      "Logistic Regression (572/5000): loss=0.5860637621925224\n",
      "Logistic Regression (573/5000): loss=0.5900197718345505\n",
      "Logistic Regression (574/5000): loss=0.5815274573294281\n",
      "Logistic Regression (575/5000): loss=0.5875276431516044\n",
      "Logistic Regression (576/5000): loss=0.5816651542112875\n",
      "Logistic Regression (577/5000): loss=0.582782258718741\n",
      "Logistic Regression (578/5000): loss=0.5816880297913817\n",
      "Logistic Regression (579/5000): loss=0.5955372007167673\n",
      "Logistic Regression (580/5000): loss=0.5838568466168125\n",
      "Logistic Regression (581/5000): loss=0.5827712730551795\n",
      "Logistic Regression (582/5000): loss=0.5812236072461751\n",
      "Logistic Regression (583/5000): loss=0.5816333693317617\n",
      "Logistic Regression (584/5000): loss=0.5814141647455543\n",
      "Logistic Regression (585/5000): loss=0.5829078970302134\n",
      "Logistic Regression (586/5000): loss=0.5938783722424872\n",
      "Logistic Regression (587/5000): loss=0.6265893812191392\n",
      "Logistic Regression (588/5000): loss=0.5963926907343386\n",
      "Logistic Regression (589/5000): loss=0.581398043271813\n",
      "Logistic Regression (590/5000): loss=0.5826487576591864\n",
      "Logistic Regression (591/5000): loss=0.5874898514136382\n",
      "Logistic Regression (592/5000): loss=0.5815995297592172\n",
      "Logistic Regression (593/5000): loss=0.5819058612736447\n",
      "Logistic Regression (594/5000): loss=0.5815599442276326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (595/5000): loss=0.5829088614478577\n",
      "Logistic Regression (596/5000): loss=0.5812049566799014\n",
      "Logistic Regression (597/5000): loss=0.5813368186323408\n",
      "Logistic Regression (598/5000): loss=0.5809464456775502\n",
      "Logistic Regression (599/5000): loss=0.586074615014013\n",
      "Logistic Regression (600/5000): loss=0.5833353858485878\n",
      "Logistic Regression (601/5000): loss=0.5813639342610554\n",
      "Logistic Regression (602/5000): loss=0.5815385706960613\n",
      "Logistic Regression (603/5000): loss=0.5816263337061752\n",
      "Logistic Regression (604/5000): loss=0.5878014775753686\n",
      "Logistic Regression (605/5000): loss=0.5815230516772688\n",
      "Logistic Regression (606/5000): loss=0.5811661886619336\n",
      "Logistic Regression (607/5000): loss=0.5814523372787472\n",
      "Logistic Regression (608/5000): loss=0.5831480165822049\n",
      "Logistic Regression (609/5000): loss=0.5837981662202485\n",
      "Logistic Regression (610/5000): loss=0.581622427606279\n",
      "Logistic Regression (611/5000): loss=0.581046427346558\n",
      "Logistic Regression (612/5000): loss=0.5849242790228096\n",
      "Logistic Regression (613/5000): loss=0.58554693221559\n",
      "Logistic Regression (614/5000): loss=0.5859400309400218\n",
      "Logistic Regression (615/5000): loss=0.5822882488504848\n",
      "Logistic Regression (616/5000): loss=0.5807893285065859\n",
      "Logistic Regression (617/5000): loss=0.5809288874585861\n",
      "Logistic Regression (618/5000): loss=0.5817323937182716\n",
      "Logistic Regression (619/5000): loss=0.5864055106459828\n",
      "Logistic Regression (620/5000): loss=0.5924131454939933\n",
      "Logistic Regression (621/5000): loss=0.583840458536202\n",
      "Logistic Regression (622/5000): loss=0.597742536376413\n",
      "Logistic Regression (623/5000): loss=0.5844617455655691\n",
      "Logistic Regression (624/5000): loss=0.5822564284671459\n",
      "Logistic Regression (625/5000): loss=0.5807910808414352\n",
      "Logistic Regression (626/5000): loss=0.5818561298323803\n",
      "Logistic Regression (627/5000): loss=0.5955163548270676\n",
      "Logistic Regression (628/5000): loss=0.5891957263148586\n",
      "Logistic Regression (629/5000): loss=0.5859454769653482\n",
      "Logistic Regression (630/5000): loss=0.5820495580048176\n",
      "Logistic Regression (631/5000): loss=0.5871208572733068\n",
      "Logistic Regression (632/5000): loss=0.5815210686854567\n",
      "Logistic Regression (633/5000): loss=0.5860867108099204\n",
      "Logistic Regression (634/5000): loss=0.5821897081813551\n",
      "Logistic Regression (635/5000): loss=0.5805300455533309\n",
      "Logistic Regression (636/5000): loss=0.5806698048794676\n",
      "Logistic Regression (637/5000): loss=0.5808849199899849\n",
      "Logistic Regression (638/5000): loss=0.5857540422557714\n",
      "Logistic Regression (639/5000): loss=0.5807323606531278\n",
      "Logistic Regression (640/5000): loss=0.5867731225469547\n",
      "Logistic Regression (641/5000): loss=0.581323116593804\n",
      "Logistic Regression (642/5000): loss=0.5812087692774018\n",
      "Logistic Regression (643/5000): loss=0.580888951885512\n",
      "Logistic Regression (644/5000): loss=0.5816557103782667\n",
      "Logistic Regression (645/5000): loss=0.5911703266651444\n",
      "Logistic Regression (646/5000): loss=0.597716143329768\n",
      "Logistic Regression (647/5000): loss=0.5873727468611147\n",
      "Logistic Regression (648/5000): loss=0.5872567453550459\n",
      "Logistic Regression (649/5000): loss=0.5803231229062641\n",
      "Logistic Regression (650/5000): loss=0.5849965181455089\n",
      "Logistic Regression (651/5000): loss=0.6512706506462544\n",
      "Logistic Regression (652/5000): loss=0.5873068488940134\n",
      "Logistic Regression (653/5000): loss=0.5841372028318489\n",
      "Logistic Regression (654/5000): loss=0.5825561112551483\n",
      "Logistic Regression (655/5000): loss=0.5805015241789062\n",
      "Logistic Regression (656/5000): loss=0.5807773973154416\n",
      "Logistic Regression (657/5000): loss=0.5819108290773917\n",
      "Logistic Regression (658/5000): loss=0.5860572519254648\n",
      "Logistic Regression (659/5000): loss=0.586919580488126\n",
      "Logistic Regression (660/5000): loss=0.5837774804283291\n",
      "Logistic Regression (661/5000): loss=0.5864791233859141\n",
      "Logistic Regression (662/5000): loss=0.582801088389775\n",
      "Logistic Regression (663/5000): loss=0.5991676757232373\n",
      "Logistic Regression (664/5000): loss=0.5838496786204133\n",
      "Logistic Regression (665/5000): loss=0.5802649940741365\n",
      "Logistic Regression (666/5000): loss=0.5883113344704024\n",
      "Logistic Regression (667/5000): loss=0.5812855136005753\n",
      "Logistic Regression (668/5000): loss=0.5813739023491787\n",
      "Logistic Regression (669/5000): loss=0.5807249958545342\n",
      "Logistic Regression (670/5000): loss=0.5802709524130001\n",
      "Logistic Regression (671/5000): loss=0.5845554368355956\n",
      "Logistic Regression (672/5000): loss=0.5885247082158702\n",
      "Logistic Regression (673/5000): loss=0.5825851535732057\n",
      "Logistic Regression (674/5000): loss=0.5799705892429919\n",
      "Logistic Regression (675/5000): loss=0.5831306520592247\n",
      "Logistic Regression (676/5000): loss=0.5839286503145892\n",
      "Logistic Regression (677/5000): loss=0.5871782123946012\n",
      "Logistic Regression (678/5000): loss=0.5864053597792678\n",
      "Logistic Regression (679/5000): loss=0.5817724585151323\n",
      "Logistic Regression (680/5000): loss=0.5886287182279971\n",
      "Logistic Regression (681/5000): loss=0.5842840113422623\n",
      "Logistic Regression (682/5000): loss=0.5908525646910057\n",
      "Logistic Regression (683/5000): loss=0.5814413866187892\n",
      "Logistic Regression (684/5000): loss=0.5804021019583427\n",
      "Logistic Regression (685/5000): loss=0.581592774653472\n",
      "Logistic Regression (686/5000): loss=0.5876720566502058\n",
      "Logistic Regression (687/5000): loss=0.5827644331108878\n",
      "Logistic Regression (688/5000): loss=0.589149054160304\n",
      "Logistic Regression (689/5000): loss=0.6002416879457713\n",
      "Logistic Regression (690/5000): loss=0.5803309463088341\n",
      "Logistic Regression (691/5000): loss=0.5847131331874004\n",
      "Logistic Regression (692/5000): loss=0.5807995908985797\n",
      "Logistic Regression (693/5000): loss=0.5801353704719063\n",
      "Logistic Regression (694/5000): loss=0.6027347530395657\n",
      "Logistic Regression (695/5000): loss=0.5821565439654801\n",
      "Logistic Regression (696/5000): loss=0.5795970196578362\n",
      "Logistic Regression (697/5000): loss=0.5810802292419844\n",
      "Logistic Regression (698/5000): loss=0.5852161800469661\n",
      "Logistic Regression (699/5000): loss=0.5852028058878209\n",
      "Logistic Regression (700/5000): loss=0.5903902718815118\n",
      "Logistic Regression (701/5000): loss=0.5948810487071056\n",
      "Logistic Regression (702/5000): loss=0.5796987397078981\n",
      "Logistic Regression (703/5000): loss=0.5815992059923207\n",
      "Logistic Regression (704/5000): loss=0.5797338928161151\n",
      "Logistic Regression (705/5000): loss=0.5795230098134936\n",
      "Logistic Regression (706/5000): loss=0.5796988568391375\n",
      "Logistic Regression (707/5000): loss=0.5823264497056142\n",
      "Logistic Regression (708/5000): loss=0.5837235127187764\n",
      "Logistic Regression (709/5000): loss=0.5829374787948641\n",
      "Logistic Regression (710/5000): loss=0.5815861748320873\n",
      "Logistic Regression (711/5000): loss=0.5846048739103499\n",
      "Logistic Regression (712/5000): loss=0.5799199125690284\n",
      "Logistic Regression (713/5000): loss=0.5796405356420136\n",
      "Logistic Regression (714/5000): loss=0.5795301388672494\n",
      "Logistic Regression (715/5000): loss=0.5921945372027908\n",
      "Logistic Regression (716/5000): loss=0.5797836059738034\n",
      "Logistic Regression (717/5000): loss=0.5792071740988411\n",
      "Logistic Regression (718/5000): loss=0.5806250907096094\n",
      "Logistic Regression (719/5000): loss=0.5853097379278032\n",
      "Logistic Regression (720/5000): loss=0.5945580621985674\n",
      "Logistic Regression (721/5000): loss=0.5819144527416239\n",
      "Logistic Regression (722/5000): loss=0.5795091525721426\n",
      "Logistic Regression (723/5000): loss=0.583547098725415\n",
      "Logistic Regression (724/5000): loss=0.5816169711481138\n",
      "Logistic Regression (725/5000): loss=0.5802437983369185\n",
      "Logistic Regression (726/5000): loss=0.5828736267494555\n",
      "Logistic Regression (727/5000): loss=0.5800525712392715\n",
      "Logistic Regression (728/5000): loss=0.5798507608668759\n",
      "Logistic Regression (729/5000): loss=0.5801242673107895\n",
      "Logistic Regression (730/5000): loss=0.5788343484384694\n",
      "Logistic Regression (731/5000): loss=0.5837584396910179\n",
      "Logistic Regression (732/5000): loss=0.581622250664909\n",
      "Logistic Regression (733/5000): loss=0.5788866179908853\n",
      "Logistic Regression (734/5000): loss=0.581874642952525\n",
      "Logistic Regression (735/5000): loss=0.582150284396405\n",
      "Logistic Regression (736/5000): loss=0.5792540370287531\n",
      "Logistic Regression (737/5000): loss=0.5915697758088917\n",
      "Logistic Regression (738/5000): loss=0.5786144616400833\n",
      "Logistic Regression (739/5000): loss=0.5934352851772026\n",
      "Logistic Regression (740/5000): loss=0.5786400494966822\n",
      "Logistic Regression (741/5000): loss=0.5788408792496145\n",
      "Logistic Regression (742/5000): loss=0.584088769667172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (743/5000): loss=0.5790375918156799\n",
      "Logistic Regression (744/5000): loss=0.5804589017365124\n",
      "Logistic Regression (745/5000): loss=0.5788489234463352\n",
      "Logistic Regression (746/5000): loss=0.580412777870717\n",
      "Logistic Regression (747/5000): loss=0.5798364873505031\n",
      "Logistic Regression (748/5000): loss=0.5821855916790584\n",
      "Logistic Regression (749/5000): loss=0.5986416195194797\n",
      "Logistic Regression (750/5000): loss=0.5802042779246359\n",
      "Logistic Regression (751/5000): loss=0.5793222070051199\n",
      "Logistic Regression (752/5000): loss=0.5809585981886342\n",
      "Logistic Regression (753/5000): loss=0.5791868925771656\n",
      "Logistic Regression (754/5000): loss=0.5784229115961186\n",
      "Logistic Regression (755/5000): loss=0.5923656839768673\n",
      "Logistic Regression (756/5000): loss=0.5849885112327072\n",
      "Logistic Regression (757/5000): loss=0.584700302539574\n",
      "Logistic Regression (758/5000): loss=0.5810867486890073\n",
      "Logistic Regression (759/5000): loss=0.5786914972019125\n",
      "Logistic Regression (760/5000): loss=0.580443623215434\n",
      "Logistic Regression (761/5000): loss=0.5954712798816979\n",
      "Logistic Regression (762/5000): loss=0.5794374782443888\n",
      "Logistic Regression (763/5000): loss=0.5810213285084163\n",
      "Logistic Regression (764/5000): loss=0.5834437198484888\n",
      "Logistic Regression (765/5000): loss=0.5788637399756215\n",
      "Logistic Regression (766/5000): loss=0.5840551490808285\n",
      "Logistic Regression (767/5000): loss=0.596150906149945\n",
      "Logistic Regression (768/5000): loss=0.5808667770765068\n",
      "Logistic Regression (769/5000): loss=0.6029374511662713\n",
      "Logistic Regression (770/5000): loss=0.6099139357544029\n",
      "Logistic Regression (771/5000): loss=0.5791639423923147\n",
      "Logistic Regression (772/5000): loss=0.5784719163274643\n",
      "Logistic Regression (773/5000): loss=0.58393166702957\n",
      "Logistic Regression (774/5000): loss=0.5781344816919087\n",
      "Logistic Regression (775/5000): loss=0.5801586994986117\n",
      "Logistic Regression (776/5000): loss=0.5802918735484961\n",
      "Logistic Regression (777/5000): loss=0.5807343750179079\n",
      "Logistic Regression (778/5000): loss=0.579443887067021\n",
      "Logistic Regression (779/5000): loss=0.5838333522878802\n",
      "Logistic Regression (780/5000): loss=0.5926293859276391\n",
      "Logistic Regression (781/5000): loss=0.5810329423727104\n",
      "Logistic Regression (782/5000): loss=0.5852421410777777\n",
      "Logistic Regression (783/5000): loss=0.5860490815094936\n",
      "Logistic Regression (784/5000): loss=0.5833091526512555\n",
      "Logistic Regression (785/5000): loss=0.5821200642526193\n",
      "Logistic Regression (786/5000): loss=0.5799822472162963\n",
      "Logistic Regression (787/5000): loss=0.5895703119313553\n",
      "Logistic Regression (788/5000): loss=0.5803451666779518\n",
      "Logistic Regression (789/5000): loss=0.5853629148698594\n",
      "Logistic Regression (790/5000): loss=0.5780671449288055\n",
      "Logistic Regression (791/5000): loss=0.5828023029346866\n",
      "Logistic Regression (792/5000): loss=0.5788683379121736\n",
      "Logistic Regression (793/5000): loss=0.583608511684627\n",
      "Logistic Regression (794/5000): loss=0.5791425619017231\n",
      "Logistic Regression (795/5000): loss=0.5777594052118105\n",
      "Logistic Regression (796/5000): loss=0.5819012589954438\n",
      "Logistic Regression (797/5000): loss=0.5781841025910359\n",
      "Logistic Regression (798/5000): loss=0.5809571089468607\n",
      "Logistic Regression (799/5000): loss=0.58793363542301\n",
      "Logistic Regression (800/5000): loss=0.5902424137862622\n",
      "Logistic Regression (801/5000): loss=0.581280524223878\n",
      "Logistic Regression (802/5000): loss=0.5797406174211117\n",
      "Logistic Regression (803/5000): loss=0.5778376448440044\n",
      "Logistic Regression (804/5000): loss=0.577995869419391\n",
      "Logistic Regression (805/5000): loss=0.5776810247914564\n",
      "Logistic Regression (806/5000): loss=0.5781363045733016\n",
      "Logistic Regression (807/5000): loss=0.5850479498157295\n",
      "Logistic Regression (808/5000): loss=0.5806844727635091\n",
      "Logistic Regression (809/5000): loss=0.5778324605169298\n",
      "Logistic Regression (810/5000): loss=0.5849898811074659\n",
      "Logistic Regression (811/5000): loss=0.5777802475960349\n",
      "Logistic Regression (812/5000): loss=0.579376884349883\n",
      "Logistic Regression (813/5000): loss=0.5793635059847412\n",
      "Logistic Regression (814/5000): loss=0.5778052070540267\n",
      "Logistic Regression (815/5000): loss=0.5783080142670888\n",
      "Logistic Regression (816/5000): loss=0.5788074138717695\n",
      "Logistic Regression (817/5000): loss=0.580581972817201\n",
      "Logistic Regression (818/5000): loss=0.5781733057009931\n",
      "Logistic Regression (819/5000): loss=0.5790869480696652\n",
      "Logistic Regression (820/5000): loss=0.5845695771734629\n",
      "Logistic Regression (821/5000): loss=0.5781474595406163\n",
      "Logistic Regression (822/5000): loss=0.5779383266658932\n",
      "Logistic Regression (823/5000): loss=0.5780677444970698\n",
      "Logistic Regression (824/5000): loss=0.5884301259161744\n",
      "Logistic Regression (825/5000): loss=0.5800379802752657\n",
      "Logistic Regression (826/5000): loss=0.5784456425981688\n",
      "Logistic Regression (827/5000): loss=0.595054022872076\n",
      "Logistic Regression (828/5000): loss=0.586535915114081\n",
      "Logistic Regression (829/5000): loss=0.5814759106824827\n",
      "Logistic Regression (830/5000): loss=0.5804409841050621\n",
      "Logistic Regression (831/5000): loss=0.577507939409701\n",
      "Logistic Regression (832/5000): loss=0.5815684899118392\n",
      "Logistic Regression (833/5000): loss=0.5775901679840459\n",
      "Logistic Regression (834/5000): loss=0.5791653640081575\n",
      "Logistic Regression (835/5000): loss=0.5836843556256393\n",
      "Logistic Regression (836/5000): loss=0.5785311874776924\n",
      "Logistic Regression (837/5000): loss=0.5790296580035201\n",
      "Logistic Regression (838/5000): loss=0.589792565074612\n",
      "Logistic Regression (839/5000): loss=0.5774910337759566\n",
      "Logistic Regression (840/5000): loss=0.5775093239137615\n",
      "Logistic Regression (841/5000): loss=0.5782599275841915\n",
      "Logistic Regression (842/5000): loss=0.579813281691633\n",
      "Logistic Regression (843/5000): loss=0.5818170591473268\n",
      "Logistic Regression (844/5000): loss=0.577683991491884\n",
      "Logistic Regression (845/5000): loss=0.5812581202803613\n",
      "Logistic Regression (846/5000): loss=0.604322682036251\n",
      "Logistic Regression (847/5000): loss=0.5776901395695786\n",
      "Logistic Regression (848/5000): loss=0.5782802963159355\n",
      "Logistic Regression (849/5000): loss=0.5829305240214266\n",
      "Logistic Regression (850/5000): loss=0.578746417785353\n",
      "Logistic Regression (851/5000): loss=0.5815097347957763\n",
      "Logistic Regression (852/5000): loss=0.5794033377519576\n",
      "Logistic Regression (853/5000): loss=0.5938759762671607\n",
      "Logistic Regression (854/5000): loss=0.5801739439050878\n",
      "Logistic Regression (855/5000): loss=0.5784201625341642\n",
      "Logistic Regression (856/5000): loss=0.5834795587100057\n",
      "Logistic Regression (857/5000): loss=0.5773871048449511\n",
      "Logistic Regression (858/5000): loss=0.579240911954127\n",
      "Logistic Regression (859/5000): loss=0.5784173064481244\n",
      "Logistic Regression (860/5000): loss=0.5768783156652173\n",
      "Logistic Regression (861/5000): loss=0.5950397807221178\n",
      "Logistic Regression (862/5000): loss=0.5912539026410226\n",
      "Logistic Regression (863/5000): loss=0.5776938983315459\n",
      "Logistic Regression (864/5000): loss=0.5774853385000139\n",
      "Logistic Regression (865/5000): loss=0.5774508685334475\n",
      "Logistic Regression (866/5000): loss=0.5777135541879648\n",
      "Logistic Regression (867/5000): loss=0.5794341781273813\n",
      "Logistic Regression (868/5000): loss=0.5774035966152892\n",
      "Logistic Regression (869/5000): loss=0.5777124235263738\n",
      "Logistic Regression (870/5000): loss=0.5786358308128459\n",
      "Logistic Regression (871/5000): loss=0.5810219732081394\n",
      "Logistic Regression (872/5000): loss=0.5937245968712888\n",
      "Logistic Regression (873/5000): loss=0.5781593373265819\n",
      "Logistic Regression (874/5000): loss=0.577155239695299\n",
      "Logistic Regression (875/5000): loss=0.5844645723106747\n",
      "Logistic Regression (876/5000): loss=0.5858083418645674\n",
      "Logistic Regression (877/5000): loss=0.5770941563511842\n",
      "Logistic Regression (878/5000): loss=0.5837748545668422\n",
      "Logistic Regression (879/5000): loss=0.588726611920904\n",
      "Logistic Regression (880/5000): loss=0.5776244657629309\n",
      "Logistic Regression (881/5000): loss=0.5778732812070734\n",
      "Logistic Regression (882/5000): loss=0.5787415048130267\n",
      "Logistic Regression (883/5000): loss=0.5784880142930169\n",
      "Logistic Regression (884/5000): loss=0.5817188878434593\n",
      "Logistic Regression (885/5000): loss=0.5790527946971754\n",
      "Logistic Regression (886/5000): loss=0.5783061404043115\n",
      "Logistic Regression (887/5000): loss=0.5831098234616591\n",
      "Logistic Regression (888/5000): loss=0.5790133918165554\n",
      "Logistic Regression (889/5000): loss=0.5923749945857444\n",
      "Logistic Regression (890/5000): loss=0.5814156061923759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (891/5000): loss=0.5790610135111034\n",
      "Logistic Regression (892/5000): loss=0.5833567752204389\n",
      "Logistic Regression (893/5000): loss=0.5784419018024611\n",
      "Logistic Regression (894/5000): loss=0.58079376328329\n",
      "Logistic Regression (895/5000): loss=0.5769809577888501\n",
      "Logistic Regression (896/5000): loss=0.5767150788646328\n",
      "Logistic Regression (897/5000): loss=0.5765576111380191\n",
      "Logistic Regression (898/5000): loss=0.5772977016331478\n",
      "Logistic Regression (899/5000): loss=0.5794725948455344\n",
      "Logistic Regression (900/5000): loss=0.5803349539860175\n",
      "Logistic Regression (901/5000): loss=0.5773656474966561\n",
      "Logistic Regression (902/5000): loss=0.5807504260568443\n",
      "Logistic Regression (903/5000): loss=0.5938632972251292\n",
      "Logistic Regression (904/5000): loss=0.577162010585151\n",
      "Logistic Regression (905/5000): loss=0.5766752458155089\n",
      "Logistic Regression (906/5000): loss=0.5762397881489334\n",
      "Logistic Regression (907/5000): loss=0.5762567566546606\n",
      "Logistic Regression (908/5000): loss=0.579184603589592\n",
      "Logistic Regression (909/5000): loss=0.5771860268983323\n",
      "Logistic Regression (910/5000): loss=0.5774325774188452\n",
      "Logistic Regression (911/5000): loss=0.5764625826740803\n",
      "Logistic Regression (912/5000): loss=0.5763871320398327\n",
      "Logistic Regression (913/5000): loss=0.5851901014679125\n",
      "Logistic Regression (914/5000): loss=0.5891925040252646\n",
      "Logistic Regression (915/5000): loss=0.5813990513340653\n",
      "Logistic Regression (916/5000): loss=0.5801965213542843\n",
      "Logistic Regression (917/5000): loss=0.5829232643021379\n",
      "Logistic Regression (918/5000): loss=0.578764539258522\n",
      "Logistic Regression (919/5000): loss=0.579643575752967\n",
      "Logistic Regression (920/5000): loss=0.5772617915438666\n",
      "Logistic Regression (921/5000): loss=0.5859658789822164\n",
      "Logistic Regression (922/5000): loss=0.576512099972944\n",
      "Logistic Regression (923/5000): loss=0.5773126182335673\n",
      "Logistic Regression (924/5000): loss=0.5879264267920402\n",
      "Logistic Regression (925/5000): loss=0.5825092582342922\n",
      "Logistic Regression (926/5000): loss=0.5762150655616677\n",
      "Logistic Regression (927/5000): loss=0.5880192183790939\n",
      "Logistic Regression (928/5000): loss=0.5766389617904905\n",
      "Logistic Regression (929/5000): loss=0.5778421814982283\n",
      "Logistic Regression (930/5000): loss=0.5767662596639316\n",
      "Logistic Regression (931/5000): loss=0.5765984173872455\n",
      "Logistic Regression (932/5000): loss=0.5781011086006868\n",
      "Logistic Regression (933/5000): loss=0.5769780832494736\n",
      "Logistic Regression (934/5000): loss=0.5765470424230531\n",
      "Logistic Regression (935/5000): loss=0.5774001744265563\n",
      "Logistic Regression (936/5000): loss=0.5894945925731784\n",
      "Logistic Regression (937/5000): loss=0.5863625124172469\n",
      "Logistic Regression (938/5000): loss=0.5808619379414559\n",
      "Logistic Regression (939/5000): loss=0.5964920556348184\n",
      "Logistic Regression (940/5000): loss=0.5774035953682545\n",
      "Logistic Regression (941/5000): loss=0.5773883048964688\n",
      "Logistic Regression (942/5000): loss=0.5898474724132523\n",
      "Logistic Regression (943/5000): loss=0.5899150723219633\n",
      "Logistic Regression (944/5000): loss=0.5758355190946319\n",
      "Logistic Regression (945/5000): loss=0.5758194446828186\n",
      "Logistic Regression (946/5000): loss=0.5768740970189226\n",
      "Logistic Regression (947/5000): loss=0.5783043156830591\n",
      "Logistic Regression (948/5000): loss=0.5783647703868496\n",
      "Logistic Regression (949/5000): loss=0.5771749069529296\n",
      "Logistic Regression (950/5000): loss=0.5764080409909302\n",
      "Logistic Regression (951/5000): loss=0.5758535025919622\n",
      "Logistic Regression (952/5000): loss=0.5800240979069345\n",
      "Logistic Regression (953/5000): loss=0.5770855629272477\n",
      "Logistic Regression (954/5000): loss=0.5936466685961161\n",
      "Logistic Regression (955/5000): loss=0.5764560013478717\n",
      "Logistic Regression (956/5000): loss=0.5958829973738756\n",
      "Logistic Regression (957/5000): loss=0.5794965541297104\n",
      "Logistic Regression (958/5000): loss=0.5804919692099512\n",
      "Logistic Regression (959/5000): loss=0.5909649197552214\n",
      "Logistic Regression (960/5000): loss=0.5758441290852719\n",
      "Logistic Regression (961/5000): loss=0.5773991295058397\n",
      "Logistic Regression (962/5000): loss=0.579890760895901\n",
      "Logistic Regression (963/5000): loss=0.5806153978481818\n",
      "Logistic Regression (964/5000): loss=0.5756297930413768\n",
      "Logistic Regression (965/5000): loss=0.5757000065616604\n",
      "Logistic Regression (966/5000): loss=0.5765402590829615\n",
      "Logistic Regression (967/5000): loss=0.5824080037142451\n",
      "Logistic Regression (968/5000): loss=0.5846313443826696\n",
      "Logistic Regression (969/5000): loss=0.5755724132547285\n",
      "Logistic Regression (970/5000): loss=0.5816169608197975\n",
      "Logistic Regression (971/5000): loss=0.5822058221567529\n",
      "Logistic Regression (972/5000): loss=0.5755473488159257\n",
      "Logistic Regression (973/5000): loss=0.5822056677135709\n",
      "Logistic Regression (974/5000): loss=0.5769268336598641\n",
      "Logistic Regression (975/5000): loss=0.5839708622971719\n",
      "Logistic Regression (976/5000): loss=0.5780253176845119\n",
      "Logistic Regression (977/5000): loss=0.5754320862210046\n",
      "Logistic Regression (978/5000): loss=0.5757540070024579\n",
      "Logistic Regression (979/5000): loss=0.5876708841609808\n",
      "Logistic Regression (980/5000): loss=0.5893554177234351\n",
      "Logistic Regression (981/5000): loss=0.5914275713439334\n",
      "Logistic Regression (982/5000): loss=0.5753385760155825\n",
      "Logistic Regression (983/5000): loss=0.5788068149987105\n",
      "Logistic Regression (984/5000): loss=0.5795451705498761\n",
      "Logistic Regression (985/5000): loss=0.5783418722788043\n",
      "Logistic Regression (986/5000): loss=0.5752075179060799\n",
      "Logistic Regression (987/5000): loss=0.5770533182921362\n",
      "Logistic Regression (988/5000): loss=0.5850142580924091\n",
      "Logistic Regression (989/5000): loss=0.5751447390674908\n",
      "Logistic Regression (990/5000): loss=0.5961104515230417\n",
      "Logistic Regression (991/5000): loss=0.5790992489680205\n",
      "Logistic Regression (992/5000): loss=0.5881791801710848\n",
      "Logistic Regression (993/5000): loss=0.5762189158327192\n",
      "Logistic Regression (994/5000): loss=0.5762775959236907\n",
      "Logistic Regression (995/5000): loss=0.5838828329454882\n",
      "Logistic Regression (996/5000): loss=0.5767687481241983\n",
      "Logistic Regression (997/5000): loss=0.5751651712131333\n",
      "Logistic Regression (998/5000): loss=0.5880177207531976\n",
      "Logistic Regression (999/5000): loss=0.5753140244011816\n",
      "Logistic Regression (1000/5000): loss=0.588765985668148\n",
      "Logistic Regression (1001/5000): loss=0.5830673184423103\n",
      "Logistic Regression (1002/5000): loss=0.584429569021213\n",
      "Logistic Regression (1003/5000): loss=0.58453575232999\n",
      "Logistic Regression (1004/5000): loss=0.5758704662493932\n",
      "Logistic Regression (1005/5000): loss=0.5854828573431229\n",
      "Logistic Regression (1006/5000): loss=0.5774944746534267\n",
      "Logistic Regression (1007/5000): loss=0.5751872765839657\n",
      "Logistic Regression (1008/5000): loss=0.5771363824209111\n",
      "Logistic Regression (1009/5000): loss=0.575219636653149\n",
      "Logistic Regression (1010/5000): loss=0.5754985107355498\n",
      "Logistic Regression (1011/5000): loss=0.5778554979616182\n",
      "Logistic Regression (1012/5000): loss=0.5765300457772933\n",
      "Logistic Regression (1013/5000): loss=0.5905048721896518\n",
      "Logistic Regression (1014/5000): loss=0.5833103709907912\n",
      "Logistic Regression (1015/5000): loss=0.5823748918176508\n",
      "Logistic Regression (1016/5000): loss=0.5816722106914408\n",
      "Logistic Regression (1017/5000): loss=0.5751811790585244\n",
      "Logistic Regression (1018/5000): loss=0.5764705658720781\n",
      "Logistic Regression (1019/5000): loss=0.5763654423628225\n",
      "Logistic Regression (1020/5000): loss=0.5760887380339792\n",
      "Logistic Regression (1021/5000): loss=0.5750100573833968\n",
      "Logistic Regression (1022/5000): loss=0.576401480647066\n",
      "Logistic Regression (1023/5000): loss=0.5761889629792752\n",
      "Logistic Regression (1024/5000): loss=0.5827724791961238\n",
      "Logistic Regression (1025/5000): loss=0.576176961828149\n",
      "Logistic Regression (1026/5000): loss=0.5746564261562571\n",
      "Logistic Regression (1027/5000): loss=0.5748040844844452\n",
      "Logistic Regression (1028/5000): loss=0.5812349985117272\n",
      "Logistic Regression (1029/5000): loss=0.5822797158437203\n",
      "Logistic Regression (1030/5000): loss=0.6105177758187473\n",
      "Logistic Regression (1031/5000): loss=0.6015049991120744\n",
      "Logistic Regression (1032/5000): loss=0.5762118230692734\n",
      "Logistic Regression (1033/5000): loss=0.5754124011610479\n",
      "Logistic Regression (1034/5000): loss=0.5745753877347397\n",
      "Logistic Regression (1035/5000): loss=0.5758480312257185\n",
      "Logistic Regression (1036/5000): loss=0.578052831682663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (1037/5000): loss=0.5752005975173077\n",
      "Logistic Regression (1038/5000): loss=0.5752192696886416\n",
      "Logistic Regression (1039/5000): loss=0.5767522690992504\n",
      "Logistic Regression (1040/5000): loss=0.5803911630616158\n",
      "Logistic Regression (1041/5000): loss=0.575172212709266\n",
      "Logistic Regression (1042/5000): loss=0.5771675903888176\n",
      "Logistic Regression (1043/5000): loss=0.5745127385400731\n",
      "Logistic Regression (1044/5000): loss=0.5745460883869518\n",
      "Logistic Regression (1045/5000): loss=0.5774841273864121\n",
      "Logistic Regression (1046/5000): loss=0.5755463394392758\n",
      "Logistic Regression (1047/5000): loss=0.5785355759818475\n",
      "Logistic Regression (1048/5000): loss=0.5755691829359847\n",
      "Logistic Regression (1049/5000): loss=0.5756878545010008\n",
      "Logistic Regression (1050/5000): loss=0.576085270661746\n",
      "Logistic Regression (1051/5000): loss=0.5914233646417446\n",
      "Logistic Regression (1052/5000): loss=0.5802079588969167\n",
      "Logistic Regression (1053/5000): loss=0.5752471496753707\n",
      "Logistic Regression (1054/5000): loss=0.594184623940687\n",
      "Logistic Regression (1055/5000): loss=0.5820351955946045\n",
      "Logistic Regression (1056/5000): loss=0.6000552826471962\n",
      "Logistic Regression (1057/5000): loss=0.5788657859467312\n",
      "Logistic Regression (1058/5000): loss=0.5830937447594667\n",
      "Logistic Regression (1059/5000): loss=0.5822420839721072\n",
      "Logistic Regression (1060/5000): loss=0.5839359871262955\n",
      "Logistic Regression (1061/5000): loss=0.5770490671522338\n",
      "Logistic Regression (1062/5000): loss=0.5762466520435806\n",
      "Logistic Regression (1063/5000): loss=0.5791811340341537\n",
      "Logistic Regression (1064/5000): loss=0.5795977741459761\n",
      "Logistic Regression (1065/5000): loss=0.5794162623580645\n",
      "Logistic Regression (1066/5000): loss=0.577138806970004\n",
      "Logistic Regression (1067/5000): loss=0.5771104216390557\n",
      "Logistic Regression (1068/5000): loss=0.5759329666607694\n",
      "Logistic Regression (1069/5000): loss=0.5755694758926333\n",
      "Logistic Regression (1070/5000): loss=0.5767361349375594\n",
      "Logistic Regression (1071/5000): loss=0.579375838437866\n",
      "Logistic Regression (1072/5000): loss=0.5788595138906152\n",
      "Logistic Regression (1073/5000): loss=0.5784704627058985\n",
      "Logistic Regression (1074/5000): loss=0.5752429768835156\n",
      "Logistic Regression (1075/5000): loss=0.5753922929505285\n",
      "Logistic Regression (1076/5000): loss=0.5757076106141349\n",
      "Logistic Regression (1077/5000): loss=0.576986558201475\n",
      "Logistic Regression (1078/5000): loss=0.5786490945254881\n",
      "Logistic Regression (1079/5000): loss=0.5744720699165047\n",
      "Logistic Regression (1080/5000): loss=0.5743109709919721\n",
      "Logistic Regression (1081/5000): loss=0.5778277358582731\n",
      "Logistic Regression (1082/5000): loss=0.5775420069936165\n",
      "Logistic Regression (1083/5000): loss=0.5800064072374754\n",
      "Logistic Regression (1084/5000): loss=0.5811017519343685\n",
      "Logistic Regression (1085/5000): loss=0.5750420061060636\n",
      "Logistic Regression (1086/5000): loss=0.5766485044119998\n",
      "Logistic Regression (1087/5000): loss=0.5756649877199898\n",
      "Logistic Regression (1088/5000): loss=0.5740490693035513\n",
      "Logistic Regression (1089/5000): loss=0.5793407222654886\n",
      "Logistic Regression (1090/5000): loss=0.5741370132211157\n",
      "Logistic Regression (1091/5000): loss=0.5747132468087727\n",
      "Logistic Regression (1092/5000): loss=0.5747449719550131\n",
      "Logistic Regression (1093/5000): loss=0.5788769390274914\n",
      "Logistic Regression (1094/5000): loss=0.5759627399322655\n",
      "Logistic Regression (1095/5000): loss=0.5800923574906389\n",
      "Logistic Regression (1096/5000): loss=0.5757771359604565\n",
      "Logistic Regression (1097/5000): loss=0.5886187004586286\n",
      "Logistic Regression (1098/5000): loss=0.5743348776896668\n",
      "Logistic Regression (1099/5000): loss=0.5776773359526588\n",
      "Logistic Regression (1100/5000): loss=0.5841732136481855\n",
      "Logistic Regression (1101/5000): loss=0.5763675382002309\n",
      "Logistic Regression (1102/5000): loss=0.5741600665998515\n",
      "Logistic Regression (1103/5000): loss=0.5752908383695028\n",
      "Logistic Regression (1104/5000): loss=0.5745074711436088\n",
      "Logistic Regression (1105/5000): loss=0.5739078689336858\n",
      "Logistic Regression (1106/5000): loss=0.5781006662141014\n",
      "Logistic Regression (1107/5000): loss=0.5777081269189878\n",
      "Logistic Regression (1108/5000): loss=0.5764638171448618\n",
      "Logistic Regression (1109/5000): loss=0.573767591785412\n",
      "Logistic Regression (1110/5000): loss=0.5744039540600088\n",
      "Logistic Regression (1111/5000): loss=0.5771393005937613\n",
      "Logistic Regression (1112/5000): loss=0.5742817086106092\n",
      "Logistic Regression (1113/5000): loss=0.5817398889701173\n",
      "Logistic Regression (1114/5000): loss=0.5752505322602359\n",
      "Logistic Regression (1115/5000): loss=0.5952552185286174\n",
      "Logistic Regression (1116/5000): loss=0.5964316254882729\n",
      "Logistic Regression (1117/5000): loss=0.575670493322977\n",
      "Logistic Regression (1118/5000): loss=0.5757168712336753\n",
      "Logistic Regression (1119/5000): loss=0.5924865199929542\n",
      "Logistic Regression (1120/5000): loss=0.5790612010632171\n",
      "Logistic Regression (1121/5000): loss=0.5743379352171756\n",
      "Logistic Regression (1122/5000): loss=0.5747605129954733\n",
      "Logistic Regression (1123/5000): loss=0.5741311832214753\n",
      "Logistic Regression (1124/5000): loss=0.5759512742180021\n",
      "Logistic Regression (1125/5000): loss=0.5783053884735037\n",
      "Logistic Regression (1126/5000): loss=0.5740367509973338\n",
      "Logistic Regression (1127/5000): loss=0.5737608704212306\n",
      "Logistic Regression (1128/5000): loss=0.5809110609628075\n",
      "Logistic Regression (1129/5000): loss=0.573470085295286\n",
      "Logistic Regression (1130/5000): loss=0.5767777553738199\n",
      "Logistic Regression (1131/5000): loss=0.5737746082214031\n",
      "Logistic Regression (1132/5000): loss=0.5777412926470726\n",
      "Logistic Regression (1133/5000): loss=0.5865724904945663\n",
      "Logistic Regression (1134/5000): loss=0.5850549322964546\n",
      "Logistic Regression (1135/5000): loss=0.5749275343756689\n",
      "Logistic Regression (1136/5000): loss=0.5742319768638576\n",
      "Logistic Regression (1137/5000): loss=0.5753618959822557\n",
      "Logistic Regression (1138/5000): loss=0.5766190830133362\n",
      "Logistic Regression (1139/5000): loss=0.5809051344825809\n",
      "Logistic Regression (1140/5000): loss=0.5784809505408535\n",
      "Logistic Regression (1141/5000): loss=0.579005219755625\n",
      "Logistic Regression (1142/5000): loss=0.5744689128839234\n",
      "Logistic Regression (1143/5000): loss=0.5739288969633976\n",
      "Logistic Regression (1144/5000): loss=0.5773073845957515\n",
      "Logistic Regression (1145/5000): loss=0.5745467416443554\n",
      "Logistic Regression (1146/5000): loss=0.5825892712932762\n",
      "Logistic Regression (1147/5000): loss=0.5773819725305648\n",
      "Logistic Regression (1148/5000): loss=0.5734129842476107\n",
      "Logistic Regression (1149/5000): loss=0.57849189323025\n",
      "Logistic Regression (1150/5000): loss=0.5744014704721223\n",
      "Logistic Regression (1151/5000): loss=0.5739358267228128\n",
      "Logistic Regression (1152/5000): loss=0.5774527798682494\n",
      "Logistic Regression (1153/5000): loss=0.574254315411146\n",
      "Logistic Regression (1154/5000): loss=0.5734764774103087\n",
      "Logistic Regression (1155/5000): loss=0.5775644191553135\n",
      "Logistic Regression (1156/5000): loss=0.5735574977493266\n",
      "Logistic Regression (1157/5000): loss=0.5742023216765156\n",
      "Logistic Regression (1158/5000): loss=0.5734704642727436\n",
      "Logistic Regression (1159/5000): loss=0.5735165904708227\n",
      "Logistic Regression (1160/5000): loss=0.5814074883727923\n",
      "Logistic Regression (1161/5000): loss=0.5733480986337525\n",
      "Logistic Regression (1162/5000): loss=0.5797526166336696\n",
      "Logistic Regression (1163/5000): loss=0.6025774837032496\n",
      "Logistic Regression (1164/5000): loss=0.5736687364220598\n",
      "Logistic Regression (1165/5000): loss=0.5805195270554868\n",
      "Logistic Regression (1166/5000): loss=0.5869687838451387\n",
      "Logistic Regression (1167/5000): loss=0.5739619431273276\n",
      "Logistic Regression (1168/5000): loss=0.5788781755002484\n",
      "Logistic Regression (1169/5000): loss=0.5744579285788175\n",
      "Logistic Regression (1170/5000): loss=0.5737397267009323\n",
      "Logistic Regression (1171/5000): loss=0.5741761425337099\n",
      "Logistic Regression (1172/5000): loss=0.5802088728481617\n",
      "Logistic Regression (1173/5000): loss=0.5739373845748819\n",
      "Logistic Regression (1174/5000): loss=0.5734348419606053\n",
      "Logistic Regression (1175/5000): loss=0.5732072014109114\n",
      "Logistic Regression (1176/5000): loss=0.575174792825412\n",
      "Logistic Regression (1177/5000): loss=0.5795222431812792\n",
      "Logistic Regression (1178/5000): loss=0.5762746827618181\n",
      "Logistic Regression (1179/5000): loss=0.5735099267532561\n",
      "Logistic Regression (1180/5000): loss=0.5750214804591294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (1181/5000): loss=0.5754469787658942\n",
      "Logistic Regression (1182/5000): loss=0.5732634685122577\n",
      "Logistic Regression (1183/5000): loss=0.580221604764118\n",
      "Logistic Regression (1184/5000): loss=0.5817374204023127\n",
      "Logistic Regression (1185/5000): loss=0.573569815994284\n",
      "Logistic Regression (1186/5000): loss=0.5741047210783472\n",
      "Logistic Regression (1187/5000): loss=0.5744826350408353\n",
      "Logistic Regression (1188/5000): loss=0.5733689678062991\n",
      "Logistic Regression (1189/5000): loss=0.5747961647021973\n",
      "Logistic Regression (1190/5000): loss=0.5727779961906297\n",
      "Logistic Regression (1191/5000): loss=0.5774828297029564\n",
      "Logistic Regression (1192/5000): loss=0.5773031940280982\n",
      "Logistic Regression (1193/5000): loss=0.5732566402843061\n",
      "Logistic Regression (1194/5000): loss=0.5751414424351713\n",
      "Logistic Regression (1195/5000): loss=0.5733456786831382\n",
      "Logistic Regression (1196/5000): loss=0.5736567863573305\n",
      "Logistic Regression (1197/5000): loss=0.5763623248252785\n",
      "Logistic Regression (1198/5000): loss=0.5775367128190061\n",
      "Logistic Regression (1199/5000): loss=0.5729782315944735\n",
      "Logistic Regression (1200/5000): loss=0.5788480566606838\n",
      "Logistic Regression (1201/5000): loss=0.5743619683990263\n",
      "Logistic Regression (1202/5000): loss=0.5871313847312175\n",
      "Logistic Regression (1203/5000): loss=0.5727485159546761\n",
      "Logistic Regression (1204/5000): loss=0.5805336484028768\n",
      "Logistic Regression (1205/5000): loss=0.5732151592845524\n",
      "Logistic Regression (1206/5000): loss=0.5743379058652618\n",
      "Logistic Regression (1207/5000): loss=0.5731454312024279\n",
      "Logistic Regression (1208/5000): loss=0.5779227573400929\n",
      "Logistic Regression (1209/5000): loss=0.5728772910306501\n",
      "Logistic Regression (1210/5000): loss=0.572647009958372\n",
      "Logistic Regression (1211/5000): loss=0.5758210128989195\n",
      "Logistic Regression (1212/5000): loss=0.5730879632978974\n",
      "Logistic Regression (1213/5000): loss=0.577314712793421\n",
      "Logistic Regression (1214/5000): loss=0.5729751564758312\n",
      "Logistic Regression (1215/5000): loss=0.6050005091875802\n",
      "Logistic Regression (1216/5000): loss=0.5739173181294275\n",
      "Logistic Regression (1217/5000): loss=0.5724601700913637\n",
      "Logistic Regression (1218/5000): loss=0.5755790245723357\n",
      "Logistic Regression (1219/5000): loss=0.5737291984890385\n",
      "Logistic Regression (1220/5000): loss=0.5817998317756736\n",
      "Logistic Regression (1221/5000): loss=0.5945126692128323\n",
      "Logistic Regression (1222/5000): loss=0.574880682264801\n",
      "Logistic Regression (1223/5000): loss=0.5731279790165226\n",
      "Logistic Regression (1224/5000): loss=0.5755969087823498\n",
      "Logistic Regression (1225/5000): loss=0.578067996011238\n",
      "Logistic Regression (1226/5000): loss=0.5862441017654193\n",
      "Logistic Regression (1227/5000): loss=0.5785157284213691\n",
      "Logistic Regression (1228/5000): loss=0.5728792696660349\n",
      "Logistic Regression (1229/5000): loss=0.5725635689183292\n",
      "Logistic Regression (1230/5000): loss=0.58020877328225\n",
      "Logistic Regression (1231/5000): loss=0.5871328588673245\n",
      "Logistic Regression (1232/5000): loss=0.5737604479327552\n",
      "Logistic Regression (1233/5000): loss=0.5882767160782095\n",
      "Logistic Regression (1234/5000): loss=0.5737914668152697\n",
      "Logistic Regression (1235/5000): loss=0.5730193272333034\n",
      "Logistic Regression (1236/5000): loss=0.5732276395284149\n",
      "Logistic Regression (1237/5000): loss=0.5737672340764335\n",
      "Logistic Regression (1238/5000): loss=0.5737757000269401\n",
      "Logistic Regression (1239/5000): loss=0.5765127696028367\n",
      "Logistic Regression (1240/5000): loss=0.5731284452455586\n",
      "Logistic Regression (1241/5000): loss=0.5722775858198431\n",
      "Logistic Regression (1242/5000): loss=0.5753936787099089\n",
      "Logistic Regression (1243/5000): loss=0.5875389048412745\n",
      "Logistic Regression (1244/5000): loss=0.5763068802449305\n",
      "Logistic Regression (1245/5000): loss=0.5722315107748621\n",
      "Logistic Regression (1246/5000): loss=0.5756373287220541\n",
      "Logistic Regression (1247/5000): loss=0.6047777126581619\n",
      "Logistic Regression (1248/5000): loss=0.5789272378449842\n",
      "Logistic Regression (1249/5000): loss=0.5736596460617472\n",
      "Logistic Regression (1250/5000): loss=0.5722906723652732\n",
      "Logistic Regression (1251/5000): loss=0.5727152399710624\n",
      "Logistic Regression (1252/5000): loss=0.5730784715151519\n",
      "Logistic Regression (1253/5000): loss=0.5724973659200759\n",
      "Logistic Regression (1254/5000): loss=0.573153957515887\n",
      "Logistic Regression (1255/5000): loss=0.5725085904307011\n",
      "Logistic Regression (1256/5000): loss=0.5738789329576152\n",
      "Logistic Regression (1257/5000): loss=0.5727792290694353\n",
      "Logistic Regression (1258/5000): loss=0.5783664747393104\n",
      "Logistic Regression (1259/5000): loss=0.5726526403380351\n",
      "Logistic Regression (1260/5000): loss=0.5728872439625382\n",
      "Logistic Regression (1261/5000): loss=0.5726139280307773\n",
      "Logistic Regression (1262/5000): loss=0.584455895480529\n",
      "Logistic Regression (1263/5000): loss=0.5745273044413227\n",
      "Logistic Regression (1264/5000): loss=0.57378992320925\n",
      "Logistic Regression (1265/5000): loss=0.5730245388206093\n",
      "Logistic Regression (1266/5000): loss=0.5721038725229035\n",
      "Logistic Regression (1267/5000): loss=0.5722980585580497\n",
      "Logistic Regression (1268/5000): loss=0.57334349280121\n",
      "Logistic Regression (1269/5000): loss=0.573329716258787\n",
      "Logistic Regression (1270/5000): loss=0.5727188276386784\n",
      "Logistic Regression (1271/5000): loss=0.5849539064566788\n",
      "Logistic Regression (1272/5000): loss=0.5722846210819439\n",
      "Logistic Regression (1273/5000): loss=0.5737164825233719\n",
      "Logistic Regression (1274/5000): loss=0.5726340077367842\n",
      "Logistic Regression (1275/5000): loss=0.5721355185060941\n",
      "Logistic Regression (1276/5000): loss=0.5720672796709481\n",
      "Logistic Regression (1277/5000): loss=0.5721495298635841\n",
      "Logistic Regression (1278/5000): loss=0.5754854881411638\n",
      "Logistic Regression (1279/5000): loss=0.5719806920676483\n",
      "Logistic Regression (1280/5000): loss=0.5718179073627174\n",
      "Logistic Regression (1281/5000): loss=0.5742369376664866\n",
      "Logistic Regression (1282/5000): loss=0.5728093382619873\n",
      "Logistic Regression (1283/5000): loss=0.576202240043636\n",
      "Logistic Regression (1284/5000): loss=0.5722211979806945\n",
      "Logistic Regression (1285/5000): loss=0.5717491218570384\n",
      "Logistic Regression (1286/5000): loss=0.5764109204892852\n",
      "Logistic Regression (1287/5000): loss=0.5972785469983115\n",
      "Logistic Regression (1288/5000): loss=0.5799179456165239\n",
      "Logistic Regression (1289/5000): loss=0.5717568485292621\n",
      "Logistic Regression (1290/5000): loss=0.5733461953283647\n",
      "Logistic Regression (1291/5000): loss=0.572458524715963\n",
      "Logistic Regression (1292/5000): loss=0.5793178159339777\n",
      "Logistic Regression (1293/5000): loss=0.5824984003962734\n",
      "Logistic Regression (1294/5000): loss=0.5725881007181499\n",
      "Logistic Regression (1295/5000): loss=0.5747178028552188\n",
      "Logistic Regression (1296/5000): loss=0.5743497675470903\n",
      "Logistic Regression (1297/5000): loss=0.5736110009758308\n",
      "Logistic Regression (1298/5000): loss=0.5717093643934199\n",
      "Logistic Regression (1299/5000): loss=0.5772126894943451\n",
      "Logistic Regression (1300/5000): loss=0.5717236771919527\n",
      "Logistic Regression (1301/5000): loss=0.5726536031737923\n",
      "Logistic Regression (1302/5000): loss=0.5718024576715792\n",
      "Logistic Regression (1303/5000): loss=0.5753218631344689\n",
      "Logistic Regression (1304/5000): loss=0.571912418307039\n",
      "Logistic Regression (1305/5000): loss=0.5722748648842164\n",
      "Logistic Regression (1306/5000): loss=0.5720990625820582\n",
      "Logistic Regression (1307/5000): loss=0.5813767430830055\n",
      "Logistic Regression (1308/5000): loss=0.5722051969938006\n",
      "Logistic Regression (1309/5000): loss=0.5797389900898666\n",
      "Logistic Regression (1310/5000): loss=0.5912747496583286\n",
      "Logistic Regression (1311/5000): loss=0.5784859744874193\n",
      "Logistic Regression (1312/5000): loss=0.5891696853075775\n",
      "Logistic Regression (1313/5000): loss=0.5725969555249038\n",
      "Logistic Regression (1314/5000): loss=0.5713304795652203\n",
      "Logistic Regression (1315/5000): loss=0.5739116805420675\n",
      "Logistic Regression (1316/5000): loss=0.5765589068644056\n",
      "Logistic Regression (1317/5000): loss=0.5753056926825493\n",
      "Logistic Regression (1318/5000): loss=0.5749593183114481\n",
      "Logistic Regression (1319/5000): loss=0.5713327236914619\n",
      "Logistic Regression (1320/5000): loss=0.5713554006548485\n",
      "Logistic Regression (1321/5000): loss=0.5712811468001344\n",
      "Logistic Regression (1322/5000): loss=0.5753120352912182\n",
      "Logistic Regression (1323/5000): loss=0.579129936969935\n",
      "Logistic Regression (1324/5000): loss=0.5724541580958253\n",
      "Logistic Regression (1325/5000): loss=0.5889561154170722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (1326/5000): loss=0.5711834445390579\n",
      "Logistic Regression (1327/5000): loss=0.5739677949112418\n",
      "Logistic Regression (1328/5000): loss=0.5712503527633891\n",
      "Logistic Regression (1329/5000): loss=0.5724912472257759\n",
      "Logistic Regression (1330/5000): loss=0.5774301188781937\n",
      "Logistic Regression (1331/5000): loss=0.5739233372986347\n",
      "Logistic Regression (1332/5000): loss=0.5717470246785726\n",
      "Logistic Regression (1333/5000): loss=0.5718441806947334\n",
      "Logistic Regression (1334/5000): loss=0.5751054976665559\n",
      "Logistic Regression (1335/5000): loss=0.5754440711027782\n",
      "Logistic Regression (1336/5000): loss=0.5740199218052013\n",
      "Logistic Regression (1337/5000): loss=0.571881534983521\n",
      "Logistic Regression (1338/5000): loss=0.5720115237611669\n",
      "Logistic Regression (1339/5000): loss=0.5716141688430445\n",
      "Logistic Regression (1340/5000): loss=0.5731280410713978\n",
      "Logistic Regression (1341/5000): loss=0.5711574450161052\n",
      "Logistic Regression (1342/5000): loss=0.5835503322812623\n",
      "Logistic Regression (1343/5000): loss=0.5729186016921677\n",
      "Logistic Regression (1344/5000): loss=0.5720522426443897\n",
      "Logistic Regression (1345/5000): loss=0.5733330170501287\n",
      "Logistic Regression (1346/5000): loss=0.572621659694769\n",
      "Logistic Regression (1347/5000): loss=0.5724948683848333\n",
      "Logistic Regression (1348/5000): loss=0.573220890125456\n",
      "Logistic Regression (1349/5000): loss=0.5749721305291317\n",
      "Logistic Regression (1350/5000): loss=0.5712889936963705\n",
      "Logistic Regression (1351/5000): loss=0.571674933968761\n",
      "Logistic Regression (1352/5000): loss=0.5744399995846495\n",
      "Logistic Regression (1353/5000): loss=0.5709730923801912\n",
      "Logistic Regression (1354/5000): loss=0.5709941245520785\n",
      "Logistic Regression (1355/5000): loss=0.5727988003081625\n",
      "Logistic Regression (1356/5000): loss=0.5710591970343069\n",
      "Logistic Regression (1357/5000): loss=0.572747109462259\n",
      "Logistic Regression (1358/5000): loss=0.5822409475418208\n",
      "Logistic Regression (1359/5000): loss=0.5729734153587113\n",
      "Logistic Regression (1360/5000): loss=0.5735950447235197\n",
      "Logistic Regression (1361/5000): loss=0.5730676017637157\n",
      "Logistic Regression (1362/5000): loss=0.5740374388709394\n",
      "Logistic Regression (1363/5000): loss=0.5753585254560631\n",
      "Logistic Regression (1364/5000): loss=0.5743075683453494\n",
      "Logistic Regression (1365/5000): loss=0.5737011506668547\n",
      "Logistic Regression (1366/5000): loss=0.5709828941153922\n",
      "Logistic Regression (1367/5000): loss=0.5717583675640512\n",
      "Logistic Regression (1368/5000): loss=0.571196982158452\n",
      "Logistic Regression (1369/5000): loss=0.5726584796828867\n",
      "Logistic Regression (1370/5000): loss=0.5860345548374685\n",
      "Logistic Regression (1371/5000): loss=0.589551897863622\n",
      "Logistic Regression (1372/5000): loss=0.5751953406658458\n",
      "Logistic Regression (1373/5000): loss=0.5838885000080581\n",
      "Logistic Regression (1374/5000): loss=0.5729373206332306\n",
      "Logistic Regression (1375/5000): loss=0.5721776579182303\n",
      "Logistic Regression (1376/5000): loss=0.5721165297033881\n",
      "Logistic Regression (1377/5000): loss=0.578877398908579\n",
      "Logistic Regression (1378/5000): loss=0.5870853014271844\n",
      "Logistic Regression (1379/5000): loss=0.5716715337162255\n",
      "Logistic Regression (1380/5000): loss=0.573688268701275\n",
      "Logistic Regression (1381/5000): loss=0.5711775562691301\n",
      "Logistic Regression (1382/5000): loss=0.5736711320114243\n",
      "Logistic Regression (1383/5000): loss=0.5749028308044668\n",
      "Logistic Regression (1384/5000): loss=0.5747459459709549\n",
      "Logistic Regression (1385/5000): loss=0.5707730713534195\n",
      "Logistic Regression (1386/5000): loss=0.5729170005099623\n",
      "Logistic Regression (1387/5000): loss=0.5731558789692336\n",
      "Logistic Regression (1388/5000): loss=0.590777778415918\n",
      "Logistic Regression (1389/5000): loss=0.5739576111292357\n",
      "Logistic Regression (1390/5000): loss=0.5727494011417846\n",
      "Logistic Regression (1391/5000): loss=0.5721988105242288\n",
      "Logistic Regression (1392/5000): loss=0.5712848295801636\n",
      "Logistic Regression (1393/5000): loss=0.5714615907177407\n",
      "Logistic Regression (1394/5000): loss=0.5758959172127303\n",
      "Logistic Regression (1395/5000): loss=0.573636740867202\n",
      "Logistic Regression (1396/5000): loss=0.5708973423636642\n",
      "Logistic Regression (1397/5000): loss=0.5784169307698606\n",
      "Logistic Regression (1398/5000): loss=0.5704866825909553\n",
      "Logistic Regression (1399/5000): loss=0.573252004982212\n",
      "Logistic Regression (1400/5000): loss=0.5739074593778329\n",
      "Logistic Regression (1401/5000): loss=0.571615603866625\n",
      "Logistic Regression (1402/5000): loss=0.5713467835627093\n",
      "Logistic Regression (1403/5000): loss=0.5851785078746357\n",
      "Logistic Regression (1404/5000): loss=0.5807235507246817\n",
      "Logistic Regression (1405/5000): loss=0.5752246975622871\n",
      "Logistic Regression (1406/5000): loss=0.5707466065028471\n",
      "Logistic Regression (1407/5000): loss=0.5708165489562412\n",
      "Logistic Regression (1408/5000): loss=0.5758279666245745\n",
      "Logistic Regression (1409/5000): loss=0.5886869031442409\n",
      "Logistic Regression (1410/5000): loss=0.587780609442236\n",
      "Logistic Regression (1411/5000): loss=0.5749914968695402\n",
      "Logistic Regression (1412/5000): loss=0.5723976324502491\n",
      "Logistic Regression (1413/5000): loss=0.5740749482215781\n",
      "Logistic Regression (1414/5000): loss=0.5714263102284332\n",
      "Logistic Regression (1415/5000): loss=0.5907087039516566\n",
      "Logistic Regression (1416/5000): loss=0.5726436470216385\n",
      "Logistic Regression (1417/5000): loss=0.5743065679179951\n",
      "Logistic Regression (1418/5000): loss=0.5702614049380539\n",
      "Logistic Regression (1419/5000): loss=0.5734713320684162\n",
      "Logistic Regression (1420/5000): loss=0.5706368955085596\n",
      "Logistic Regression (1421/5000): loss=0.5947136635849837\n",
      "Logistic Regression (1422/5000): loss=0.5728406872643175\n",
      "Logistic Regression (1423/5000): loss=0.5744094855010995\n",
      "Logistic Regression (1424/5000): loss=0.5702521777356223\n",
      "Logistic Regression (1425/5000): loss=0.5754542253019099\n",
      "Logistic Regression (1426/5000): loss=0.5701528887381407\n",
      "Logistic Regression (1427/5000): loss=0.5723811734693509\n",
      "Logistic Regression (1428/5000): loss=0.5843052654122913\n",
      "Logistic Regression (1429/5000): loss=0.5704100477005811\n",
      "Logistic Regression (1430/5000): loss=0.5706310595502243\n",
      "Logistic Regression (1431/5000): loss=0.5706834737651392\n",
      "Logistic Regression (1432/5000): loss=0.5702107387689728\n",
      "Logistic Regression (1433/5000): loss=0.5735588996396426\n",
      "Logistic Regression (1434/5000): loss=0.5725555855608397\n",
      "Logistic Regression (1435/5000): loss=0.5736963610948896\n",
      "Logistic Regression (1436/5000): loss=0.5704836973797728\n",
      "Logistic Regression (1437/5000): loss=0.5724918762551995\n",
      "Logistic Regression (1438/5000): loss=0.572387164218503\n",
      "Logistic Regression (1439/5000): loss=0.5720382086625264\n",
      "Logistic Regression (1440/5000): loss=0.5710491169520834\n",
      "Logistic Regression (1441/5000): loss=0.5707731372252942\n",
      "Logistic Regression (1442/5000): loss=0.5712055614783846\n",
      "Logistic Regression (1443/5000): loss=0.5741466918955387\n",
      "Logistic Regression (1444/5000): loss=0.57041209173146\n",
      "Logistic Regression (1445/5000): loss=0.5713398742478858\n",
      "Logistic Regression (1446/5000): loss=0.5698804037917491\n",
      "Logistic Regression (1447/5000): loss=0.5767617778015788\n",
      "Logistic Regression (1448/5000): loss=0.5700303046922949\n",
      "Logistic Regression (1449/5000): loss=0.5703700451366016\n",
      "Logistic Regression (1450/5000): loss=0.5711150403958428\n",
      "Logistic Regression (1451/5000): loss=0.5712098160921633\n",
      "Logistic Regression (1452/5000): loss=0.5702150828669554\n",
      "Logistic Regression (1453/5000): loss=0.5749330120625238\n",
      "Logistic Regression (1454/5000): loss=0.5875400667006332\n",
      "Logistic Regression (1455/5000): loss=0.5764316782916206\n",
      "Logistic Regression (1456/5000): loss=0.570579912406682\n",
      "Logistic Regression (1457/5000): loss=0.5708721339309061\n",
      "Logistic Regression (1458/5000): loss=0.5700520748315571\n",
      "Logistic Regression (1459/5000): loss=0.5870375356946885\n",
      "Logistic Regression (1460/5000): loss=0.571840683539195\n",
      "Logistic Regression (1461/5000): loss=0.57085870682109\n",
      "Logistic Regression (1462/5000): loss=0.5697222995098143\n",
      "Logistic Regression (1463/5000): loss=0.5700952978209368\n",
      "Logistic Regression (1464/5000): loss=0.570201312329683\n",
      "Logistic Regression (1465/5000): loss=0.5697688839859381\n",
      "Logistic Regression (1466/5000): loss=0.5702513787409487\n",
      "Logistic Regression (1467/5000): loss=0.5834239452823966\n",
      "Logistic Regression (1468/5000): loss=0.5723043439128094\n",
      "Logistic Regression (1469/5000): loss=0.5699704468653819\n",
      "Logistic Regression (1470/5000): loss=0.5730586807266659\n",
      "Logistic Regression (1471/5000): loss=0.5741301858922273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (1472/5000): loss=0.5700605432915576\n",
      "Logistic Regression (1473/5000): loss=0.570864474888724\n",
      "Logistic Regression (1474/5000): loss=0.5746042040554884\n",
      "Logistic Regression (1475/5000): loss=0.5697645533973914\n",
      "Logistic Regression (1476/5000): loss=0.5830249704624948\n",
      "Logistic Regression (1477/5000): loss=0.5746053939545925\n",
      "Logistic Regression (1478/5000): loss=0.5696133971863957\n",
      "Logistic Regression (1479/5000): loss=0.5861562367646728\n",
      "Logistic Regression (1480/5000): loss=0.5734127030366688\n",
      "Logistic Regression (1481/5000): loss=0.5721175668210109\n",
      "Logistic Regression (1482/5000): loss=0.5700728575058341\n",
      "Logistic Regression (1483/5000): loss=0.5709156211387565\n",
      "Logistic Regression (1484/5000): loss=0.5703383542784083\n",
      "Logistic Regression (1485/5000): loss=0.5710386146276102\n",
      "Logistic Regression (1486/5000): loss=0.5726453179185995\n",
      "Logistic Regression (1487/5000): loss=0.5716196234210698\n",
      "Logistic Regression (1488/5000): loss=0.5721267312306019\n",
      "Logistic Regression (1489/5000): loss=0.5729823575161326\n",
      "Logistic Regression (1490/5000): loss=0.582172327100828\n",
      "Logistic Regression (1491/5000): loss=0.5724855746624773\n",
      "Logistic Regression (1492/5000): loss=0.5694548283473517\n",
      "Logistic Regression (1493/5000): loss=0.5694663741414304\n",
      "Logistic Regression (1494/5000): loss=0.5813242135829841\n",
      "Logistic Regression (1495/5000): loss=0.5700138102017174\n",
      "Logistic Regression (1496/5000): loss=0.5699589356790646\n",
      "Logistic Regression (1497/5000): loss=0.5724296725405461\n",
      "Logistic Regression (1498/5000): loss=0.5696374247760151\n",
      "Logistic Regression (1499/5000): loss=0.572479044896393\n",
      "Logistic Regression (1500/5000): loss=0.5712320868206466\n",
      "Logistic Regression (1501/5000): loss=0.5693504795938185\n",
      "Logistic Regression (1502/5000): loss=0.5702110284780599\n",
      "Logistic Regression (1503/5000): loss=0.5744434740519269\n",
      "Logistic Regression (1504/5000): loss=0.5712679403404295\n",
      "Logistic Regression (1505/5000): loss=0.5723213323874018\n",
      "Logistic Regression (1506/5000): loss=0.5697245115280757\n",
      "Logistic Regression (1507/5000): loss=0.5729458950994502\n",
      "Logistic Regression (1508/5000): loss=0.5772609645445334\n",
      "Logistic Regression (1509/5000): loss=0.5706352065016447\n",
      "Logistic Regression (1510/5000): loss=0.5699963167210228\n",
      "Logistic Regression (1511/5000): loss=0.5752923073490114\n",
      "Logistic Regression (1512/5000): loss=0.5700134407721343\n",
      "Logistic Regression (1513/5000): loss=0.5788676729620916\n",
      "Logistic Regression (1514/5000): loss=0.5701567315909041\n",
      "Logistic Regression (1515/5000): loss=0.5697152385061914\n",
      "Logistic Regression (1516/5000): loss=0.5708144593035543\n",
      "Logistic Regression (1517/5000): loss=0.5715497003176809\n",
      "Logistic Regression (1518/5000): loss=0.5770075427516813\n",
      "Logistic Regression (1519/5000): loss=0.5699391876965985\n",
      "Logistic Regression (1520/5000): loss=0.569931177316202\n",
      "Logistic Regression (1521/5000): loss=0.5695772583321911\n",
      "Logistic Regression (1522/5000): loss=0.5697391452218717\n",
      "Logistic Regression (1523/5000): loss=0.5742059468163967\n",
      "Logistic Regression (1524/5000): loss=0.5739061698705885\n",
      "Logistic Regression (1525/5000): loss=0.56960566635388\n",
      "Logistic Regression (1526/5000): loss=0.5697446250300188\n",
      "Logistic Regression (1527/5000): loss=0.5849455153291473\n",
      "Logistic Regression (1528/5000): loss=0.5724636080234465\n",
      "Logistic Regression (1529/5000): loss=0.5709754376038699\n",
      "Logistic Regression (1530/5000): loss=0.5722428312839912\n",
      "Logistic Regression (1531/5000): loss=0.585444805901745\n",
      "Logistic Regression (1532/5000): loss=0.5754136173334127\n",
      "Logistic Regression (1533/5000): loss=0.5712111541862277\n",
      "Logistic Regression (1534/5000): loss=0.5707746635720158\n",
      "Logistic Regression (1535/5000): loss=0.5698355363193953\n",
      "Logistic Regression (1536/5000): loss=0.5769944979770185\n",
      "Logistic Regression (1537/5000): loss=0.5699946096197648\n",
      "Logistic Regression (1538/5000): loss=0.5711013876466816\n",
      "Logistic Regression (1539/5000): loss=0.5767807413675091\n",
      "Logistic Regression (1540/5000): loss=0.5695410100795792\n",
      "Logistic Regression (1541/5000): loss=0.569557582875659\n",
      "Logistic Regression (1542/5000): loss=0.5716478240836039\n",
      "Logistic Regression (1543/5000): loss=0.571290214833794\n",
      "Logistic Regression (1544/5000): loss=0.5802454269857575\n",
      "Logistic Regression (1545/5000): loss=0.5695222341844558\n",
      "Logistic Regression (1546/5000): loss=0.5693780096046486\n",
      "Logistic Regression (1547/5000): loss=0.5720276963223316\n",
      "Logistic Regression (1548/5000): loss=0.5694061439901162\n",
      "Logistic Regression (1549/5000): loss=0.569118024465316\n",
      "Logistic Regression (1550/5000): loss=0.5709207231882418\n",
      "Logistic Regression (1551/5000): loss=0.5690715667002655\n",
      "Logistic Regression (1552/5000): loss=0.5692440140476616\n",
      "Logistic Regression (1553/5000): loss=0.5691425120360885\n",
      "Logistic Regression (1554/5000): loss=0.5758392650019424\n",
      "Logistic Regression (1555/5000): loss=0.5714486806104369\n",
      "Logistic Regression (1556/5000): loss=0.575491868801927\n",
      "Logistic Regression (1557/5000): loss=0.5759651779416836\n",
      "Logistic Regression (1558/5000): loss=0.5698257089337071\n",
      "Logistic Regression (1559/5000): loss=0.5709231285182674\n",
      "Logistic Regression (1560/5000): loss=0.5692322148181767\n",
      "Logistic Regression (1561/5000): loss=0.5753957278123523\n",
      "Logistic Regression (1562/5000): loss=0.5699659041692572\n",
      "Logistic Regression (1563/5000): loss=0.5700587451948836\n",
      "Logistic Regression (1564/5000): loss=0.5704992964861136\n",
      "Logistic Regression (1565/5000): loss=0.5726696959893625\n",
      "Logistic Regression (1566/5000): loss=0.5752273977187767\n",
      "Logistic Regression (1567/5000): loss=0.5702139549322\n",
      "Logistic Regression (1568/5000): loss=0.575161593319967\n",
      "Logistic Regression (1569/5000): loss=0.5827795226194104\n",
      "Logistic Regression (1570/5000): loss=0.57144345404124\n",
      "Logistic Regression (1571/5000): loss=0.5743686233684318\n",
      "Logistic Regression (1572/5000): loss=0.5735430883390855\n",
      "Logistic Regression (1573/5000): loss=0.5711742689875443\n",
      "Logistic Regression (1574/5000): loss=0.5701913556111093\n",
      "Logistic Regression (1575/5000): loss=0.5701582307856042\n",
      "Logistic Regression (1576/5000): loss=0.5725529506906075\n",
      "Logistic Regression (1577/5000): loss=0.5707187205605195\n",
      "Logistic Regression (1578/5000): loss=0.5705542802153807\n",
      "Logistic Regression (1579/5000): loss=0.5701313827731728\n",
      "Logistic Regression (1580/5000): loss=0.5709551999024248\n",
      "Logistic Regression (1581/5000): loss=0.5697266379808245\n",
      "Logistic Regression (1582/5000): loss=0.5701335129925029\n",
      "Logistic Regression (1583/5000): loss=0.5711556816205868\n",
      "Logistic Regression (1584/5000): loss=0.569333245570356\n",
      "Logistic Regression (1585/5000): loss=0.5739526272060295\n",
      "Logistic Regression (1586/5000): loss=0.5695799548842038\n",
      "Logistic Regression (1587/5000): loss=0.5744955791909907\n",
      "Logistic Regression (1588/5000): loss=0.5705684912347284\n",
      "Logistic Regression (1589/5000): loss=0.5838606163010086\n",
      "Logistic Regression (1590/5000): loss=0.569668981765625\n",
      "Logistic Regression (1591/5000): loss=0.5688963124096528\n",
      "Logistic Regression (1592/5000): loss=0.5701175894812355\n",
      "Logistic Regression (1593/5000): loss=0.5730341423375389\n",
      "Logistic Regression (1594/5000): loss=0.568600155744893\n",
      "Logistic Regression (1595/5000): loss=0.570870383435608\n",
      "Logistic Regression (1596/5000): loss=0.5687659471193939\n",
      "Logistic Regression (1597/5000): loss=0.571365002614969\n",
      "Logistic Regression (1598/5000): loss=0.5742867398143996\n",
      "Logistic Regression (1599/5000): loss=0.5690368779511865\n",
      "Logistic Regression (1600/5000): loss=0.5684846837489016\n",
      "Logistic Regression (1601/5000): loss=0.5741816485480697\n",
      "Logistic Regression (1602/5000): loss=0.5739291772435322\n",
      "Logistic Regression (1603/5000): loss=0.5789764133708546\n",
      "Logistic Regression (1604/5000): loss=0.5696572360292032\n",
      "Logistic Regression (1605/5000): loss=0.5697878160672721\n",
      "Logistic Regression (1606/5000): loss=0.5708317572270547\n",
      "Logistic Regression (1607/5000): loss=0.5682936960743801\n",
      "Logistic Regression (1608/5000): loss=0.571683741717984\n",
      "Logistic Regression (1609/5000): loss=0.5708887905297825\n",
      "Logistic Regression (1610/5000): loss=0.5684078252157929\n",
      "Logistic Regression (1611/5000): loss=0.5709608652367789\n",
      "Logistic Regression (1612/5000): loss=0.5705173977765262\n",
      "Logistic Regression (1613/5000): loss=0.5824620777164348\n",
      "Logistic Regression (1614/5000): loss=0.5688121757283054\n",
      "Logistic Regression (1615/5000): loss=0.5759436296740572\n",
      "Logistic Regression (1616/5000): loss=0.5832890357139977\n",
      "Logistic Regression (1617/5000): loss=0.5746874156716665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (1618/5000): loss=0.5711270340179485\n",
      "Logistic Regression (1619/5000): loss=0.5686702708052509\n",
      "Logistic Regression (1620/5000): loss=0.5690797944308704\n",
      "Logistic Regression (1621/5000): loss=0.5736359427133201\n",
      "Logistic Regression (1622/5000): loss=0.5687805567612113\n",
      "Logistic Regression (1623/5000): loss=0.5854282786140194\n",
      "Logistic Regression (1624/5000): loss=0.5700214451299291\n",
      "Logistic Regression (1625/5000): loss=0.5703193752337367\n",
      "Logistic Regression (1626/5000): loss=0.5770401691467144\n",
      "Logistic Regression (1627/5000): loss=0.5682798341674553\n",
      "Logistic Regression (1628/5000): loss=0.5868690906881734\n",
      "Logistic Regression (1629/5000): loss=0.5711560399296866\n",
      "Logistic Regression (1630/5000): loss=0.5746506650563843\n",
      "Logistic Regression (1631/5000): loss=0.5683564144866923\n",
      "Logistic Regression (1632/5000): loss=0.5699877379446087\n",
      "Logistic Regression (1633/5000): loss=0.5701760473621793\n",
      "Logistic Regression (1634/5000): loss=0.5723524053270745\n",
      "Logistic Regression (1635/5000): loss=0.568075131974476\n",
      "Logistic Regression (1636/5000): loss=0.5749506913542519\n",
      "Logistic Regression (1637/5000): loss=0.5680396835912815\n",
      "Logistic Regression (1638/5000): loss=0.5722397753843164\n",
      "Logistic Regression (1639/5000): loss=0.574455388714877\n",
      "Logistic Regression (1640/5000): loss=0.5708046549247412\n",
      "Logistic Regression (1641/5000): loss=0.568124847692045\n",
      "Logistic Regression (1642/5000): loss=0.5854565048220485\n",
      "Logistic Regression (1643/5000): loss=0.5753256822519931\n",
      "Logistic Regression (1644/5000): loss=0.5695945908597186\n",
      "Logistic Regression (1645/5000): loss=0.5710161412060059\n",
      "Logistic Regression (1646/5000): loss=0.5716256787270121\n",
      "Logistic Regression (1647/5000): loss=0.5700774352551573\n",
      "Logistic Regression (1648/5000): loss=0.5738720639912767\n",
      "Logistic Regression (1649/5000): loss=0.5713947387618434\n",
      "Logistic Regression (1650/5000): loss=0.5769875901054212\n",
      "Logistic Regression (1651/5000): loss=0.5772052268739029\n",
      "Logistic Regression (1652/5000): loss=0.5723744514789126\n",
      "Logistic Regression (1653/5000): loss=0.5766074122803008\n",
      "Logistic Regression (1654/5000): loss=0.5715675754916385\n",
      "Logistic Regression (1655/5000): loss=0.5701249811875644\n",
      "Logistic Regression (1656/5000): loss=0.5740570619442843\n",
      "Logistic Regression (1657/5000): loss=0.5870130181659481\n",
      "Logistic Regression (1658/5000): loss=0.5691341319306996\n",
      "Logistic Regression (1659/5000): loss=0.5786104197413232\n",
      "Logistic Regression (1660/5000): loss=0.5751266580044792\n",
      "Logistic Regression (1661/5000): loss=0.5682758833383945\n",
      "Logistic Regression (1662/5000): loss=0.5681686879291141\n",
      "Logistic Regression (1663/5000): loss=0.5811398975500887\n",
      "Logistic Regression (1664/5000): loss=0.5753251360620173\n",
      "Logistic Regression (1665/5000): loss=0.5710607833247032\n",
      "Logistic Regression (1666/5000): loss=0.5704416602623662\n",
      "Logistic Regression (1667/5000): loss=0.5691360799558726\n",
      "Logistic Regression (1668/5000): loss=0.5713355854972944\n",
      "Logistic Regression (1669/5000): loss=0.5732330863643003\n",
      "Logistic Regression (1670/5000): loss=0.5719073154800126\n",
      "Logistic Regression (1671/5000): loss=0.5741196263058337\n",
      "Logistic Regression (1672/5000): loss=0.5706918860459133\n",
      "Logistic Regression (1673/5000): loss=0.578270599975041\n",
      "Logistic Regression (1674/5000): loss=0.569965102812355\n",
      "Logistic Regression (1675/5000): loss=0.5693292038122658\n",
      "Logistic Regression (1676/5000): loss=0.5682624479215153\n",
      "Logistic Regression (1677/5000): loss=0.5681186350670065\n",
      "Logistic Regression (1678/5000): loss=0.571801879458043\n",
      "Logistic Regression (1679/5000): loss=0.5770745904875747\n",
      "Logistic Regression (1680/5000): loss=0.5710587713913419\n",
      "Logistic Regression (1681/5000): loss=0.5687713703524697\n",
      "Logistic Regression (1682/5000): loss=0.5722515154250323\n",
      "Logistic Regression (1683/5000): loss=0.568226107967669\n",
      "Logistic Regression (1684/5000): loss=0.5838031027325535\n",
      "Logistic Regression (1685/5000): loss=0.5716276334076946\n",
      "Logistic Regression (1686/5000): loss=0.5741030564295183\n",
      "Logistic Regression (1687/5000): loss=0.5698834715125858\n",
      "Logistic Regression (1688/5000): loss=0.5678122066336114\n",
      "Logistic Regression (1689/5000): loss=0.5744610493328015\n",
      "Logistic Regression (1690/5000): loss=0.5685705714011412\n",
      "Logistic Regression (1691/5000): loss=0.5679477833387093\n",
      "Logistic Regression (1692/5000): loss=0.5703120546398427\n",
      "Logistic Regression (1693/5000): loss=0.569578836755784\n",
      "Logistic Regression (1694/5000): loss=0.5787626060258081\n",
      "Logistic Regression (1695/5000): loss=0.5684832847224951\n",
      "Logistic Regression (1696/5000): loss=0.5683615072908901\n",
      "Logistic Regression (1697/5000): loss=0.5688324583620918\n",
      "Logistic Regression (1698/5000): loss=0.5701448327028109\n",
      "Logistic Regression (1699/5000): loss=0.5679359180784102\n",
      "Logistic Regression (1700/5000): loss=0.577277205875094\n",
      "Logistic Regression (1701/5000): loss=0.5682596822852667\n",
      "Logistic Regression (1702/5000): loss=0.5680511914378882\n",
      "Logistic Regression (1703/5000): loss=0.5797168491206652\n",
      "Logistic Regression (1704/5000): loss=0.5702036640038439\n",
      "Logistic Regression (1705/5000): loss=0.5762791396160045\n",
      "Logistic Regression (1706/5000): loss=0.5702465913074707\n",
      "Logistic Regression (1707/5000): loss=0.5696835441812458\n",
      "Logistic Regression (1708/5000): loss=0.5713974729438737\n",
      "Logistic Regression (1709/5000): loss=0.5676352527748861\n",
      "Logistic Regression (1710/5000): loss=0.5693971739466371\n",
      "Logistic Regression (1711/5000): loss=0.567781766314567\n",
      "Logistic Regression (1712/5000): loss=0.5831321027146383\n",
      "Logistic Regression (1713/5000): loss=0.5675608282484034\n",
      "Logistic Regression (1714/5000): loss=0.5692636765396218\n",
      "Logistic Regression (1715/5000): loss=0.5681488178064801\n",
      "Logistic Regression (1716/5000): loss=0.575094601702907\n",
      "Logistic Regression (1717/5000): loss=0.5682044591109012\n",
      "Logistic Regression (1718/5000): loss=0.5694335588807702\n",
      "Logistic Regression (1719/5000): loss=0.5701632335185575\n",
      "Logistic Regression (1720/5000): loss=0.5690123927440179\n",
      "Logistic Regression (1721/5000): loss=0.5698816674935939\n",
      "Logistic Regression (1722/5000): loss=0.5695133878161399\n",
      "Logistic Regression (1723/5000): loss=0.572316715577634\n",
      "Logistic Regression (1724/5000): loss=0.5779652392684385\n",
      "Logistic Regression (1725/5000): loss=0.5874516155985023\n",
      "Logistic Regression (1726/5000): loss=0.59429972656314\n",
      "Logistic Regression (1727/5000): loss=0.578486023380866\n",
      "Logistic Regression (1728/5000): loss=0.5798278608143765\n",
      "Logistic Regression (1729/5000): loss=0.5718784984026652\n",
      "Logistic Regression (1730/5000): loss=0.5679229250380154\n",
      "Logistic Regression (1731/5000): loss=0.5875582018750289\n",
      "Logistic Regression (1732/5000): loss=0.5747978036299186\n",
      "Logistic Regression (1733/5000): loss=0.5695684366041576\n",
      "Logistic Regression (1734/5000): loss=0.5672505121335111\n",
      "Logistic Regression (1735/5000): loss=0.5675075526081967\n",
      "Logistic Regression (1736/5000): loss=0.5673196485107089\n",
      "Logistic Regression (1737/5000): loss=0.5673768652392052\n",
      "Logistic Regression (1738/5000): loss=0.5678689043581878\n",
      "Logistic Regression (1739/5000): loss=0.5700776824289229\n",
      "Logistic Regression (1740/5000): loss=0.6116536824870419\n",
      "Logistic Regression (1741/5000): loss=0.5672427704709981\n",
      "Logistic Regression (1742/5000): loss=0.5692257778082659\n",
      "Logistic Regression (1743/5000): loss=0.5725478154447283\n",
      "Logistic Regression (1744/5000): loss=0.5673293708276214\n",
      "Logistic Regression (1745/5000): loss=0.5680949102449153\n",
      "Logistic Regression (1746/5000): loss=0.572181416332017\n",
      "Logistic Regression (1747/5000): loss=0.5675818495072187\n",
      "Logistic Regression (1748/5000): loss=0.5686888158318839\n",
      "Logistic Regression (1749/5000): loss=0.5733208567164703\n",
      "Logistic Regression (1750/5000): loss=0.5712859860342188\n",
      "Logistic Regression (1751/5000): loss=0.568408580281337\n",
      "Logistic Regression (1752/5000): loss=0.5709023834605524\n",
      "Logistic Regression (1753/5000): loss=0.5690238368282445\n",
      "Logistic Regression (1754/5000): loss=0.5695260555842483\n",
      "Logistic Regression (1755/5000): loss=0.5778911616344453\n",
      "Logistic Regression (1756/5000): loss=0.5963401300366882\n",
      "Logistic Regression (1757/5000): loss=0.5827546891431837\n",
      "Logistic Regression (1758/5000): loss=0.5794642161095187\n",
      "Logistic Regression (1759/5000): loss=0.5781984157088993\n",
      "Logistic Regression (1760/5000): loss=0.5723092860073513\n",
      "Logistic Regression (1761/5000): loss=0.5839758161251886\n",
      "Logistic Regression (1762/5000): loss=0.5677222597124125\n",
      "Logistic Regression (1763/5000): loss=0.5724618479213031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (1764/5000): loss=0.5686581238552145\n",
      "Logistic Regression (1765/5000): loss=0.5682693228473189\n",
      "Logistic Regression (1766/5000): loss=0.5680826424546389\n",
      "Logistic Regression (1767/5000): loss=0.5685062946081891\n",
      "Logistic Regression (1768/5000): loss=0.5734940039606545\n",
      "Logistic Regression (1769/5000): loss=0.5694678549157516\n",
      "Logistic Regression (1770/5000): loss=0.5806176274462048\n",
      "Logistic Regression (1771/5000): loss=0.5769271977093268\n",
      "Logistic Regression (1772/5000): loss=0.5725339256755515\n",
      "Logistic Regression (1773/5000): loss=0.5671564440169109\n",
      "Logistic Regression (1774/5000): loss=0.5669873004703935\n",
      "Logistic Regression (1775/5000): loss=0.5730176755962396\n",
      "Logistic Regression (1776/5000): loss=0.5677665915838453\n",
      "Logistic Regression (1777/5000): loss=0.5673618662143699\n",
      "Logistic Regression (1778/5000): loss=0.5700748176730049\n",
      "Logistic Regression (1779/5000): loss=0.5686660836877367\n",
      "Logistic Regression (1780/5000): loss=0.5714829206171265\n",
      "Logistic Regression (1781/5000): loss=0.5744631804406829\n",
      "Logistic Regression (1782/5000): loss=0.5673475251928018\n",
      "Logistic Regression (1783/5000): loss=0.5690569440424174\n",
      "Logistic Regression (1784/5000): loss=0.5692867419339269\n",
      "Logistic Regression (1785/5000): loss=0.5677842778862133\n",
      "Logistic Regression (1786/5000): loss=0.6066067224954176\n",
      "Logistic Regression (1787/5000): loss=0.5670739530956841\n",
      "Logistic Regression (1788/5000): loss=0.5714830159707536\n",
      "Logistic Regression (1789/5000): loss=0.5715871628390001\n",
      "Logistic Regression (1790/5000): loss=0.5777709092445537\n",
      "Logistic Regression (1791/5000): loss=0.5717200826587399\n",
      "Logistic Regression (1792/5000): loss=0.5771572784268163\n",
      "Logistic Regression (1793/5000): loss=0.5849930447230278\n",
      "Logistic Regression (1794/5000): loss=0.5676101457231143\n",
      "Logistic Regression (1795/5000): loss=0.5725127451711812\n",
      "Logistic Regression (1796/5000): loss=0.566939606233104\n",
      "Logistic Regression (1797/5000): loss=0.5727782070779495\n",
      "Logistic Regression (1798/5000): loss=0.5670561562928358\n",
      "Logistic Regression (1799/5000): loss=0.5715835214264285\n",
      "Logistic Regression (1800/5000): loss=0.574310111117876\n",
      "Logistic Regression (1801/5000): loss=0.5672608639327723\n",
      "Logistic Regression (1802/5000): loss=0.5762215164419037\n",
      "Logistic Regression (1803/5000): loss=0.5692065924364137\n",
      "Logistic Regression (1804/5000): loss=0.5738888321244686\n",
      "Logistic Regression (1805/5000): loss=0.5671383128057874\n",
      "Logistic Regression (1806/5000): loss=0.574957166260858\n",
      "Logistic Regression (1807/5000): loss=0.5668921898682261\n",
      "Logistic Regression (1808/5000): loss=0.5772725242350597\n",
      "Logistic Regression (1809/5000): loss=0.5671456533813916\n",
      "Logistic Regression (1810/5000): loss=0.566821644648058\n",
      "Logistic Regression (1811/5000): loss=0.5680501945326012\n",
      "Logistic Regression (1812/5000): loss=0.5670238461089057\n",
      "Logistic Regression (1813/5000): loss=0.5805326140784938\n",
      "Logistic Regression (1814/5000): loss=0.5678256102984303\n",
      "Logistic Regression (1815/5000): loss=0.566867897514106\n",
      "Logistic Regression (1816/5000): loss=0.5666783733549473\n",
      "Logistic Regression (1817/5000): loss=0.5675559887431805\n",
      "Logistic Regression (1818/5000): loss=0.5671106262682415\n",
      "Logistic Regression (1819/5000): loss=0.5671174905507137\n",
      "Logistic Regression (1820/5000): loss=0.5683453269167337\n",
      "Logistic Regression (1821/5000): loss=0.5680058657736548\n",
      "Logistic Regression (1822/5000): loss=0.5700915754439273\n",
      "Logistic Regression (1823/5000): loss=0.5673758288739666\n",
      "Logistic Regression (1824/5000): loss=0.566857226750397\n",
      "Logistic Regression (1825/5000): loss=0.5672915914171592\n",
      "Logistic Regression (1826/5000): loss=0.566808736268886\n",
      "Logistic Regression (1827/5000): loss=0.5734948142842157\n",
      "Logistic Regression (1828/5000): loss=0.5666851949731222\n",
      "Logistic Regression (1829/5000): loss=0.5697429154090603\n",
      "Logistic Regression (1830/5000): loss=0.5681845445492322\n",
      "Logistic Regression (1831/5000): loss=0.5696062446263319\n",
      "Logistic Regression (1832/5000): loss=0.5702924728496558\n",
      "Logistic Regression (1833/5000): loss=0.5672828787630827\n",
      "Logistic Regression (1834/5000): loss=0.5673076202505475\n",
      "Logistic Regression (1835/5000): loss=0.5735931012330763\n",
      "Logistic Regression (1836/5000): loss=0.5670913332133803\n",
      "Logistic Regression (1837/5000): loss=0.5683965459569507\n",
      "Logistic Regression (1838/5000): loss=0.5679653474854622\n",
      "Logistic Regression (1839/5000): loss=0.5728545901882064\n",
      "Logistic Regression (1840/5000): loss=0.5664364450481404\n",
      "Logistic Regression (1841/5000): loss=0.5677683062578491\n",
      "Logistic Regression (1842/5000): loss=0.5666856367638828\n",
      "Logistic Regression (1843/5000): loss=0.5690732573125753\n",
      "Logistic Regression (1844/5000): loss=0.5717727799164064\n",
      "Logistic Regression (1845/5000): loss=0.5737399006653169\n",
      "Logistic Regression (1846/5000): loss=0.5780215530733895\n",
      "Logistic Regression (1847/5000): loss=0.572942518944343\n",
      "Logistic Regression (1848/5000): loss=0.5713646381757075\n",
      "Logistic Regression (1849/5000): loss=0.5665303798337488\n",
      "Logistic Regression (1850/5000): loss=0.5672867764165211\n",
      "Logistic Regression (1851/5000): loss=0.5770731270533609\n",
      "Logistic Regression (1852/5000): loss=0.5670408094882954\n",
      "Logistic Regression (1853/5000): loss=0.5664839327583041\n",
      "Logistic Regression (1854/5000): loss=0.5696634673373985\n",
      "Logistic Regression (1855/5000): loss=0.5667554261691826\n",
      "Logistic Regression (1856/5000): loss=0.5683879532979337\n",
      "Logistic Regression (1857/5000): loss=0.5703897722066742\n",
      "Logistic Regression (1858/5000): loss=0.5714635516477918\n",
      "Logistic Regression (1859/5000): loss=0.5884826753114686\n",
      "Logistic Regression (1860/5000): loss=0.5670716991035955\n",
      "Logistic Regression (1861/5000): loss=0.567353074860277\n",
      "Logistic Regression (1862/5000): loss=0.5721452370586328\n",
      "Logistic Regression (1863/5000): loss=0.5690500508225387\n",
      "Logistic Regression (1864/5000): loss=0.5662893213212565\n",
      "Logistic Regression (1865/5000): loss=0.578646480814298\n",
      "Logistic Regression (1866/5000): loss=0.5680966781977144\n",
      "Logistic Regression (1867/5000): loss=0.5717234942261312\n",
      "Logistic Regression (1868/5000): loss=0.5678970234763943\n",
      "Logistic Regression (1869/5000): loss=0.5785333417061899\n",
      "Logistic Regression (1870/5000): loss=0.5691872479990853\n",
      "Logistic Regression (1871/5000): loss=0.5670980579572882\n",
      "Logistic Regression (1872/5000): loss=0.5662258118313654\n",
      "Logistic Regression (1873/5000): loss=0.5680476676747443\n",
      "Logistic Regression (1874/5000): loss=0.5670169893969457\n",
      "Logistic Regression (1875/5000): loss=0.566699326587145\n",
      "Logistic Regression (1876/5000): loss=0.5674325153851467\n",
      "Logistic Regression (1877/5000): loss=0.5660591777675743\n",
      "Logistic Regression (1878/5000): loss=0.5671922512624644\n",
      "Logistic Regression (1879/5000): loss=0.5672697813657184\n",
      "Logistic Regression (1880/5000): loss=0.5668301933314551\n",
      "Logistic Regression (1881/5000): loss=0.5833935637270689\n",
      "Logistic Regression (1882/5000): loss=0.5781578752967387\n",
      "Logistic Regression (1883/5000): loss=0.566815366783273\n",
      "Logistic Regression (1884/5000): loss=0.5713489959593949\n",
      "Logistic Regression (1885/5000): loss=0.5672135629326294\n",
      "Logistic Regression (1886/5000): loss=0.5666758825298007\n",
      "Logistic Regression (1887/5000): loss=0.5707312329083761\n",
      "Logistic Regression (1888/5000): loss=0.5662655765981879\n",
      "Logistic Regression (1889/5000): loss=0.5702614476624636\n",
      "Logistic Regression (1890/5000): loss=0.586691543142309\n",
      "Logistic Regression (1891/5000): loss=0.5671051334833619\n",
      "Logistic Regression (1892/5000): loss=0.5673321052042256\n",
      "Logistic Regression (1893/5000): loss=0.5659317066874189\n",
      "Logistic Regression (1894/5000): loss=0.5694959036463548\n",
      "Logistic Regression (1895/5000): loss=0.5661555540037431\n",
      "Logistic Regression (1896/5000): loss=0.5658163161723436\n",
      "Logistic Regression (1897/5000): loss=0.5664215086496387\n",
      "Logistic Regression (1898/5000): loss=0.5657536655726806\n",
      "Logistic Regression (1899/5000): loss=0.5659974722693577\n",
      "Logistic Regression (1900/5000): loss=0.5696880235012762\n",
      "Logistic Regression (1901/5000): loss=0.5872205224974036\n",
      "Logistic Regression (1902/5000): loss=0.5662832489971626\n",
      "Logistic Regression (1903/5000): loss=0.5666061424465617\n",
      "Logistic Regression (1904/5000): loss=0.5672956111104464\n",
      "Logistic Regression (1905/5000): loss=0.5765508536868869\n",
      "Logistic Regression (1906/5000): loss=0.5671579996141156\n",
      "Logistic Regression (1907/5000): loss=0.5664252557245396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (1908/5000): loss=0.5836542895437027\n",
      "Logistic Regression (1909/5000): loss=0.5666062472652271\n",
      "Logistic Regression (1910/5000): loss=0.5660360454618846\n",
      "Logistic Regression (1911/5000): loss=0.5657861271203295\n",
      "Logistic Regression (1912/5000): loss=0.5680536311538205\n",
      "Logistic Regression (1913/5000): loss=0.568550228593619\n",
      "Logistic Regression (1914/5000): loss=0.5692245699628534\n",
      "Logistic Regression (1915/5000): loss=0.5670520442447919\n",
      "Logistic Regression (1916/5000): loss=0.57382255754413\n",
      "Logistic Regression (1917/5000): loss=0.5735718918766314\n",
      "Logistic Regression (1918/5000): loss=0.565510881298631\n",
      "Logistic Regression (1919/5000): loss=0.5666893079051964\n",
      "Logistic Regression (1920/5000): loss=0.5662457307390198\n",
      "Logistic Regression (1921/5000): loss=0.56646040055652\n",
      "Logistic Regression (1922/5000): loss=0.5930352791276883\n",
      "Logistic Regression (1923/5000): loss=0.5692249871450358\n",
      "Logistic Regression (1924/5000): loss=0.5738877692345891\n",
      "Logistic Regression (1925/5000): loss=0.566167881870636\n",
      "Logistic Regression (1926/5000): loss=0.5787595875312439\n",
      "Logistic Regression (1927/5000): loss=0.5654646245451448\n",
      "Logistic Regression (1928/5000): loss=0.6177943625199876\n",
      "Logistic Regression (1929/5000): loss=0.5658753102137907\n",
      "Logistic Regression (1930/5000): loss=0.5730281373382894\n",
      "Logistic Regression (1931/5000): loss=0.5656909060068809\n",
      "Logistic Regression (1932/5000): loss=0.5678718611387056\n",
      "Logistic Regression (1933/5000): loss=0.5679408027577438\n",
      "Logistic Regression (1934/5000): loss=0.5676130140055688\n",
      "Logistic Regression (1935/5000): loss=0.5704645792001366\n",
      "Logistic Regression (1936/5000): loss=0.5665280301825566\n",
      "Logistic Regression (1937/5000): loss=0.5667441456166225\n",
      "Logistic Regression (1938/5000): loss=0.5653265041882942\n",
      "Logistic Regression (1939/5000): loss=0.5731281826740002\n",
      "Logistic Regression (1940/5000): loss=0.5659081732022339\n",
      "Logistic Regression (1941/5000): loss=0.5751550227046838\n",
      "Logistic Regression (1942/5000): loss=0.5676156349427477\n",
      "Logistic Regression (1943/5000): loss=0.5668578395559258\n",
      "Logistic Regression (1944/5000): loss=0.5730287709421423\n",
      "Logistic Regression (1945/5000): loss=0.5655969693378097\n",
      "Logistic Regression (1946/5000): loss=0.5746846528550527\n",
      "Logistic Regression (1947/5000): loss=0.5663633185174203\n",
      "Logistic Regression (1948/5000): loss=0.576712494163811\n",
      "Logistic Regression (1949/5000): loss=0.5678563864165529\n",
      "Logistic Regression (1950/5000): loss=0.565969551039147\n",
      "Logistic Regression (1951/5000): loss=0.5693710360603056\n",
      "Logistic Regression (1952/5000): loss=0.566040020442325\n",
      "Logistic Regression (1953/5000): loss=0.5667874074581284\n",
      "Logistic Regression (1954/5000): loss=0.5698509342416451\n",
      "Logistic Regression (1955/5000): loss=0.5682037618036344\n",
      "Logistic Regression (1956/5000): loss=0.5710018331074165\n",
      "Logistic Regression (1957/5000): loss=0.5664872965814574\n",
      "Logistic Regression (1958/5000): loss=0.57095612581021\n",
      "Logistic Regression (1959/5000): loss=0.5701213790260138\n",
      "Logistic Regression (1960/5000): loss=0.570407320831329\n",
      "Logistic Regression (1961/5000): loss=0.5714092167282132\n",
      "Logistic Regression (1962/5000): loss=0.5687061212725334\n",
      "Logistic Regression (1963/5000): loss=0.5708523515865681\n",
      "Logistic Regression (1964/5000): loss=0.5755340233711401\n",
      "Logistic Regression (1965/5000): loss=0.56661478658982\n",
      "Logistic Regression (1966/5000): loss=0.5662350801828975\n",
      "Logistic Regression (1967/5000): loss=0.5676139339535414\n",
      "Logistic Regression (1968/5000): loss=0.5663136520000996\n",
      "Logistic Regression (1969/5000): loss=0.5666337497594961\n",
      "Logistic Regression (1970/5000): loss=0.5667463023338529\n",
      "Logistic Regression (1971/5000): loss=0.5658484211472794\n",
      "Logistic Regression (1972/5000): loss=0.5661756849613906\n",
      "Logistic Regression (1973/5000): loss=0.5771893320157102\n",
      "Logistic Regression (1974/5000): loss=0.5654201291094684\n",
      "Logistic Regression (1975/5000): loss=0.5679966731812394\n",
      "Logistic Regression (1976/5000): loss=0.5692113319508781\n",
      "Logistic Regression (1977/5000): loss=0.5671114745873972\n",
      "Logistic Regression (1978/5000): loss=0.5692949055013627\n",
      "Logistic Regression (1979/5000): loss=0.5892048051530832\n",
      "Logistic Regression (1980/5000): loss=0.5650340044715056\n",
      "Logistic Regression (1981/5000): loss=0.5670318085221788\n",
      "Logistic Regression (1982/5000): loss=0.5811353626663032\n",
      "Logistic Regression (1983/5000): loss=0.5722726357787034\n",
      "Logistic Regression (1984/5000): loss=0.5656011524413427\n",
      "Logistic Regression (1985/5000): loss=0.565094863204921\n",
      "Logistic Regression (1986/5000): loss=0.5834419244145379\n",
      "Logistic Regression (1987/5000): loss=0.5722686339302967\n",
      "Logistic Regression (1988/5000): loss=0.5737284699428399\n",
      "Logistic Regression (1989/5000): loss=0.5651860707036903\n",
      "Logistic Regression (1990/5000): loss=0.5686465834783111\n",
      "Logistic Regression (1991/5000): loss=0.5649983943382586\n",
      "Logistic Regression (1992/5000): loss=0.5820688542444046\n",
      "Logistic Regression (1993/5000): loss=0.5737974136260003\n",
      "Logistic Regression (1994/5000): loss=0.5726293667404455\n",
      "Logistic Regression (1995/5000): loss=0.570160697381056\n",
      "Logistic Regression (1996/5000): loss=0.5783536429220121\n",
      "Logistic Regression (1997/5000): loss=0.5648876157334403\n",
      "Logistic Regression (1998/5000): loss=0.5656601410090834\n",
      "Logistic Regression (1999/5000): loss=0.5880256253053496\n",
      "Logistic Regression (2000/5000): loss=0.5721427669575588\n",
      "Logistic Regression (2001/5000): loss=0.5680102689049702\n",
      "Logistic Regression (2002/5000): loss=0.5663321999350063\n",
      "Logistic Regression (2003/5000): loss=0.5649405954872478\n",
      "Logistic Regression (2004/5000): loss=0.5792097482900022\n",
      "Logistic Regression (2005/5000): loss=0.5649239937412331\n",
      "Logistic Regression (2006/5000): loss=0.5649398139921173\n",
      "Logistic Regression (2007/5000): loss=0.5652845067305016\n",
      "Logistic Regression (2008/5000): loss=0.5666572799907478\n",
      "Logistic Regression (2009/5000): loss=0.5651650633135298\n",
      "Logistic Regression (2010/5000): loss=0.5684493952056502\n",
      "Logistic Regression (2011/5000): loss=0.5650068143677663\n",
      "Logistic Regression (2012/5000): loss=0.5677679835517322\n",
      "Logistic Regression (2013/5000): loss=0.5724448879296069\n",
      "Logistic Regression (2014/5000): loss=0.58076968564133\n",
      "Logistic Regression (2015/5000): loss=0.5948687272289677\n",
      "Logistic Regression (2016/5000): loss=0.5668699779574649\n",
      "Logistic Regression (2017/5000): loss=0.5661229986642822\n",
      "Logistic Regression (2018/5000): loss=0.5672762751340205\n",
      "Logistic Regression (2019/5000): loss=0.5830291454321477\n",
      "Logistic Regression (2020/5000): loss=0.5648520982499777\n",
      "Logistic Regression (2021/5000): loss=0.5670579114766943\n",
      "Logistic Regression (2022/5000): loss=0.5649780778087754\n",
      "Logistic Regression (2023/5000): loss=0.5652711956937267\n",
      "Logistic Regression (2024/5000): loss=0.5679051942612662\n",
      "Logistic Regression (2025/5000): loss=0.5655261898100656\n",
      "Logistic Regression (2026/5000): loss=0.5646702113337925\n",
      "Logistic Regression (2027/5000): loss=0.5645989444288414\n",
      "Logistic Regression (2028/5000): loss=0.5796269766012622\n",
      "Logistic Regression (2029/5000): loss=0.5653282046432979\n",
      "Logistic Regression (2030/5000): loss=0.5691887525230448\n",
      "Logistic Regression (2031/5000): loss=0.5649882031225116\n",
      "Logistic Regression (2032/5000): loss=0.5647587381990623\n",
      "Logistic Regression (2033/5000): loss=0.5647303521028523\n",
      "Logistic Regression (2034/5000): loss=0.5648376993119013\n",
      "Logistic Regression (2035/5000): loss=0.5647566333197109\n",
      "Logistic Regression (2036/5000): loss=0.5711547564270495\n",
      "Logistic Regression (2037/5000): loss=0.5666349477977315\n",
      "Logistic Regression (2038/5000): loss=0.5648599200358364\n",
      "Logistic Regression (2039/5000): loss=0.5683466745835131\n",
      "Logistic Regression (2040/5000): loss=0.5726805461721323\n",
      "Logistic Regression (2041/5000): loss=0.5820880701998828\n",
      "Logistic Regression (2042/5000): loss=0.5702470943914039\n",
      "Logistic Regression (2043/5000): loss=0.5644104869704358\n",
      "Logistic Regression (2044/5000): loss=0.5717844258071915\n",
      "Logistic Regression (2045/5000): loss=0.5722462591172075\n",
      "Logistic Regression (2046/5000): loss=0.5761071231845487\n",
      "Logistic Regression (2047/5000): loss=0.5695321772562176\n",
      "Logistic Regression (2048/5000): loss=0.5659545097400738\n",
      "Logistic Regression (2049/5000): loss=0.5681955283591414\n",
      "Logistic Regression (2050/5000): loss=0.5660252404673062\n",
      "Logistic Regression (2051/5000): loss=0.5728359899066193\n",
      "Logistic Regression (2052/5000): loss=0.5653106228634571\n",
      "Logistic Regression (2053/5000): loss=0.5672274122156972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (2054/5000): loss=0.564719761224299\n",
      "Logistic Regression (2055/5000): loss=0.5648052470746426\n",
      "Logistic Regression (2056/5000): loss=0.5676881872891206\n",
      "Logistic Regression (2057/5000): loss=0.5869108469544617\n",
      "Logistic Regression (2058/5000): loss=0.5644139886941489\n",
      "Logistic Regression (2059/5000): loss=0.5657993805351819\n",
      "Logistic Regression (2060/5000): loss=0.5692792193033153\n",
      "Logistic Regression (2061/5000): loss=0.5650745773710111\n",
      "Logistic Regression (2062/5000): loss=0.5669284846048231\n",
      "Logistic Regression (2063/5000): loss=0.5648136940343985\n",
      "Logistic Regression (2064/5000): loss=0.5681155398567431\n",
      "Logistic Regression (2065/5000): loss=0.5655352107000333\n",
      "Logistic Regression (2066/5000): loss=0.5654654848616842\n",
      "Logistic Regression (2067/5000): loss=0.5652473893733505\n",
      "Logistic Regression (2068/5000): loss=0.5863744807707185\n",
      "Logistic Regression (2069/5000): loss=0.5646809488004111\n",
      "Logistic Regression (2070/5000): loss=0.570936201177674\n",
      "Logistic Regression (2071/5000): loss=0.5901675943337815\n",
      "Logistic Regression (2072/5000): loss=0.565509155255237\n",
      "Logistic Regression (2073/5000): loss=0.5653451081060805\n",
      "Logistic Regression (2074/5000): loss=0.5653459993861982\n",
      "Logistic Regression (2075/5000): loss=0.5643985493257783\n",
      "Logistic Regression (2076/5000): loss=0.5664279164509898\n",
      "Logistic Regression (2077/5000): loss=0.5677016765402482\n",
      "Logistic Regression (2078/5000): loss=0.5675857899099751\n",
      "Logistic Regression (2079/5000): loss=0.5647378583810588\n",
      "Logistic Regression (2080/5000): loss=0.5659426772997067\n",
      "Logistic Regression (2081/5000): loss=0.5702565992910985\n",
      "Logistic Regression (2082/5000): loss=0.56510009895627\n",
      "Logistic Regression (2083/5000): loss=0.5664414446188725\n",
      "Logistic Regression (2084/5000): loss=0.5641823406817053\n",
      "Logistic Regression (2085/5000): loss=0.5643254818005125\n",
      "Logistic Regression (2086/5000): loss=0.5657775768929472\n",
      "Logistic Regression (2087/5000): loss=0.5642129705227128\n",
      "Logistic Regression (2088/5000): loss=0.5696708777652593\n",
      "Logistic Regression (2089/5000): loss=0.5671478319808778\n",
      "Logistic Regression (2090/5000): loss=0.5829918925443626\n",
      "Logistic Regression (2091/5000): loss=0.5642759672167011\n",
      "Logistic Regression (2092/5000): loss=0.5803520269327379\n",
      "Logistic Regression (2093/5000): loss=0.5779675127888878\n",
      "Logistic Regression (2094/5000): loss=0.5641657714890416\n",
      "Logistic Regression (2095/5000): loss=0.5667976900556988\n",
      "Logistic Regression (2096/5000): loss=0.5653728198298935\n",
      "Logistic Regression (2097/5000): loss=0.5665034798137901\n",
      "Logistic Regression (2098/5000): loss=0.5686988124182805\n",
      "Logistic Regression (2099/5000): loss=0.5667648540044699\n",
      "Logistic Regression (2100/5000): loss=0.5717054755919055\n",
      "Logistic Regression (2101/5000): loss=0.5664325187890331\n",
      "Logistic Regression (2102/5000): loss=0.5654429202390092\n",
      "Logistic Regression (2103/5000): loss=0.5661571497767633\n",
      "Logistic Regression (2104/5000): loss=0.5683372374147319\n",
      "Logistic Regression (2105/5000): loss=0.5645743213044236\n",
      "Logistic Regression (2106/5000): loss=0.5668944723376074\n",
      "Logistic Regression (2107/5000): loss=0.5679506662095182\n",
      "Logistic Regression (2108/5000): loss=0.5641569928654124\n",
      "Logistic Regression (2109/5000): loss=0.5696514770339737\n",
      "Logistic Regression (2110/5000): loss=0.5645264090870923\n",
      "Logistic Regression (2111/5000): loss=0.5644601273372479\n",
      "Logistic Regression (2112/5000): loss=0.5663615034762084\n",
      "Logistic Regression (2113/5000): loss=0.5650436076692111\n",
      "Logistic Regression (2114/5000): loss=0.5641551227438586\n",
      "Logistic Regression (2115/5000): loss=0.5645263096951533\n",
      "Logistic Regression (2116/5000): loss=0.5654650638744838\n",
      "Logistic Regression (2117/5000): loss=0.563948027942816\n",
      "Logistic Regression (2118/5000): loss=0.5784531538771778\n",
      "Logistic Regression (2119/5000): loss=0.5640701924086817\n",
      "Logistic Regression (2120/5000): loss=0.5640094587221441\n",
      "Logistic Regression (2121/5000): loss=0.5642478577625669\n",
      "Logistic Regression (2122/5000): loss=0.5651742024058605\n",
      "Logistic Regression (2123/5000): loss=0.5642024813409587\n",
      "Logistic Regression (2124/5000): loss=0.5673853753905427\n",
      "Logistic Regression (2125/5000): loss=0.5713509634635757\n",
      "Logistic Regression (2126/5000): loss=0.5670499991469269\n",
      "Logistic Regression (2127/5000): loss=0.5656133105555681\n",
      "Logistic Regression (2128/5000): loss=0.5652772299629139\n",
      "Logistic Regression (2129/5000): loss=0.5639299250569934\n",
      "Logistic Regression (2130/5000): loss=0.5648863330168462\n",
      "Logistic Regression (2131/5000): loss=0.5724897733826426\n",
      "Logistic Regression (2132/5000): loss=0.5719245733822187\n",
      "Logistic Regression (2133/5000): loss=0.569395215526222\n",
      "Logistic Regression (2134/5000): loss=0.5658063598474453\n",
      "Logistic Regression (2135/5000): loss=0.5644809358040345\n",
      "Logistic Regression (2136/5000): loss=0.5732132748881121\n",
      "Logistic Regression (2137/5000): loss=0.5642855595797657\n",
      "Logistic Regression (2138/5000): loss=0.5708725249828243\n",
      "Logistic Regression (2139/5000): loss=0.5657557626608342\n",
      "Logistic Regression (2140/5000): loss=0.5741791563211165\n",
      "Logistic Regression (2141/5000): loss=0.571780713401669\n",
      "Logistic Regression (2142/5000): loss=0.5642496595125134\n",
      "Logistic Regression (2143/5000): loss=0.5642765784752523\n",
      "Logistic Regression (2144/5000): loss=0.5650594401911531\n",
      "Logistic Regression (2145/5000): loss=0.5664531230099802\n",
      "Logistic Regression (2146/5000): loss=0.567636258063559\n",
      "Logistic Regression (2147/5000): loss=0.5665285878032005\n",
      "Logistic Regression (2148/5000): loss=0.5644710098734527\n",
      "Logistic Regression (2149/5000): loss=0.57591052243196\n",
      "Logistic Regression (2150/5000): loss=0.5649565943470916\n",
      "Logistic Regression (2151/5000): loss=0.5739334254964994\n",
      "Logistic Regression (2152/5000): loss=0.5674627940320611\n",
      "Logistic Regression (2153/5000): loss=0.5715210145791271\n",
      "Logistic Regression (2154/5000): loss=0.5676631282574018\n",
      "Logistic Regression (2155/5000): loss=0.5648185126562915\n",
      "Logistic Regression (2156/5000): loss=0.5643168483719369\n",
      "Logistic Regression (2157/5000): loss=0.5653721722489355\n",
      "Logistic Regression (2158/5000): loss=0.5644099905668284\n",
      "Logistic Regression (2159/5000): loss=0.5741623246757193\n",
      "Logistic Regression (2160/5000): loss=0.564510202860663\n",
      "Logistic Regression (2161/5000): loss=0.5637594912060845\n",
      "Logistic Regression (2162/5000): loss=0.5640467674235878\n",
      "Logistic Regression (2163/5000): loss=0.5637324922129636\n",
      "Logistic Regression (2164/5000): loss=0.5769763373096563\n",
      "Logistic Regression (2165/5000): loss=0.5657286035299914\n",
      "Logistic Regression (2166/5000): loss=0.5665634067576251\n",
      "Logistic Regression (2167/5000): loss=0.5651107105357963\n",
      "Logistic Regression (2168/5000): loss=0.5654700501274748\n",
      "Logistic Regression (2169/5000): loss=0.5670461164841105\n",
      "Logistic Regression (2170/5000): loss=0.5658318723482675\n",
      "Logistic Regression (2171/5000): loss=0.5646211584248187\n",
      "Logistic Regression (2172/5000): loss=0.5650471756120801\n",
      "Logistic Regression (2173/5000): loss=0.5711573804287655\n",
      "Logistic Regression (2174/5000): loss=0.564095894327608\n",
      "Logistic Regression (2175/5000): loss=0.5634660342756723\n",
      "Logistic Regression (2176/5000): loss=0.5636685267146377\n",
      "Logistic Regression (2177/5000): loss=0.5658667899148129\n",
      "Logistic Regression (2178/5000): loss=0.5780463233411725\n",
      "Logistic Regression (2179/5000): loss=0.5634864822513345\n",
      "Logistic Regression (2180/5000): loss=0.5773551151037489\n",
      "Logistic Regression (2181/5000): loss=0.563641001725784\n",
      "Logistic Regression (2182/5000): loss=0.5671458974633069\n",
      "Logistic Regression (2183/5000): loss=0.5890431725002524\n",
      "Logistic Regression (2184/5000): loss=0.565827267483325\n",
      "Logistic Regression (2185/5000): loss=0.570101169932057\n",
      "Logistic Regression (2186/5000): loss=0.5665795654493043\n",
      "Logistic Regression (2187/5000): loss=0.585867464873911\n",
      "Logistic Regression (2188/5000): loss=0.5685556616091791\n",
      "Logistic Regression (2189/5000): loss=0.5647065254449254\n",
      "Logistic Regression (2190/5000): loss=0.5646580662440639\n",
      "Logistic Regression (2191/5000): loss=0.5681497278518806\n",
      "Logistic Regression (2192/5000): loss=0.5667547510343246\n",
      "Logistic Regression (2193/5000): loss=0.5645769666077235\n",
      "Logistic Regression (2194/5000): loss=0.56729431187992\n",
      "Logistic Regression (2195/5000): loss=0.5642399141332909\n",
      "Logistic Regression (2196/5000): loss=0.564281543270502\n",
      "Logistic Regression (2197/5000): loss=0.5645116199018562\n",
      "Logistic Regression (2198/5000): loss=0.5640499070079448\n",
      "Logistic Regression (2199/5000): loss=0.5647261167169532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (2200/5000): loss=0.5636982914031897\n",
      "Logistic Regression (2201/5000): loss=0.5642872931250124\n",
      "Logistic Regression (2202/5000): loss=0.5655500770061902\n",
      "Logistic Regression (2203/5000): loss=0.5768303219168455\n",
      "Logistic Regression (2204/5000): loss=0.5649063415011418\n",
      "Logistic Regression (2205/5000): loss=0.5781205360776527\n",
      "Logistic Regression (2206/5000): loss=0.5718248082993039\n",
      "Logistic Regression (2207/5000): loss=0.5670862971166276\n",
      "Logistic Regression (2208/5000): loss=0.5635433911865279\n",
      "Logistic Regression (2209/5000): loss=0.568554934049601\n",
      "Logistic Regression (2210/5000): loss=0.5716382791794379\n",
      "Logistic Regression (2211/5000): loss=0.5638832132280953\n",
      "Logistic Regression (2212/5000): loss=0.5650189321299506\n",
      "Logistic Regression (2213/5000): loss=0.5655191639184575\n",
      "Logistic Regression (2214/5000): loss=0.5640606488402483\n",
      "Logistic Regression (2215/5000): loss=0.5656955711201341\n",
      "Logistic Regression (2216/5000): loss=0.5700442276448685\n",
      "Logistic Regression (2217/5000): loss=0.5640563322566321\n",
      "Logistic Regression (2218/5000): loss=0.5631539968423357\n",
      "Logistic Regression (2219/5000): loss=0.5830081263485047\n",
      "Logistic Regression (2220/5000): loss=0.567440168419187\n",
      "Logistic Regression (2221/5000): loss=0.5688636630633702\n",
      "Logistic Regression (2222/5000): loss=0.57078574855852\n",
      "Logistic Regression (2223/5000): loss=0.5694342732207112\n",
      "Logistic Regression (2224/5000): loss=0.5684238962369363\n",
      "Logistic Regression (2225/5000): loss=0.5649717299917805\n",
      "Logistic Regression (2226/5000): loss=0.5687346593747515\n",
      "Logistic Regression (2227/5000): loss=0.5638959537399203\n",
      "Logistic Regression (2228/5000): loss=0.5640194234741008\n",
      "Logistic Regression (2229/5000): loss=0.5645018621959756\n",
      "Logistic Regression (2230/5000): loss=0.5649225688527701\n",
      "Logistic Regression (2231/5000): loss=0.5646742370510959\n",
      "Logistic Regression (2232/5000): loss=0.5680067202621991\n",
      "Logistic Regression (2233/5000): loss=0.5635989038713843\n",
      "Logistic Regression (2234/5000): loss=0.563626354037943\n",
      "Logistic Regression (2235/5000): loss=0.5659111750332244\n",
      "Logistic Regression (2236/5000): loss=0.5671568308166932\n",
      "Logistic Regression (2237/5000): loss=0.5630496150061236\n",
      "Logistic Regression (2238/5000): loss=0.5635817212990679\n",
      "Logistic Regression (2239/5000): loss=0.5758591977813569\n",
      "Logistic Regression (2240/5000): loss=0.564637746501307\n",
      "Logistic Regression (2241/5000): loss=0.5712932668195078\n",
      "Logistic Regression (2242/5000): loss=0.5738112319422546\n",
      "Logistic Regression (2243/5000): loss=0.5635869768771027\n",
      "Logistic Regression (2244/5000): loss=0.573636190686335\n",
      "Logistic Regression (2245/5000): loss=0.5664374134247819\n",
      "Logistic Regression (2246/5000): loss=0.5644914462270185\n",
      "Logistic Regression (2247/5000): loss=0.5645290193555746\n",
      "Logistic Regression (2248/5000): loss=0.5638378826594116\n",
      "Logistic Regression (2249/5000): loss=0.5634148699500692\n",
      "Logistic Regression (2250/5000): loss=0.6019087971718078\n",
      "Logistic Regression (2251/5000): loss=0.5654402449033399\n",
      "Logistic Regression (2252/5000): loss=0.5700772867139129\n",
      "Logistic Regression (2253/5000): loss=0.5635489415333\n",
      "Logistic Regression (2254/5000): loss=0.5732396916090711\n",
      "Logistic Regression (2255/5000): loss=0.575706920092783\n",
      "Logistic Regression (2256/5000): loss=0.5672523829718158\n",
      "Logistic Regression (2257/5000): loss=0.5652234833640383\n",
      "Logistic Regression (2258/5000): loss=0.5644730335389299\n",
      "Logistic Regression (2259/5000): loss=0.5669466470594595\n",
      "Logistic Regression (2260/5000): loss=0.5730484615498546\n",
      "Logistic Regression (2261/5000): loss=0.5627765850818525\n",
      "Logistic Regression (2262/5000): loss=0.5627643453729927\n",
      "Logistic Regression (2263/5000): loss=0.5715278002328767\n",
      "Logistic Regression (2264/5000): loss=0.5659212238995426\n",
      "Logistic Regression (2265/5000): loss=0.5717570839360622\n",
      "Logistic Regression (2266/5000): loss=0.5633305269618804\n",
      "Logistic Regression (2267/5000): loss=0.5633380831080151\n",
      "Logistic Regression (2268/5000): loss=0.5631269907529015\n",
      "Logistic Regression (2269/5000): loss=0.5774649331711397\n",
      "Logistic Regression (2270/5000): loss=0.5656567502835453\n",
      "Logistic Regression (2271/5000): loss=0.5787855571341761\n",
      "Logistic Regression (2272/5000): loss=0.5634361582662485\n",
      "Logistic Regression (2273/5000): loss=0.5629702410744593\n",
      "Logistic Regression (2274/5000): loss=0.56513372377898\n",
      "Logistic Regression (2275/5000): loss=0.5809361661539403\n",
      "Logistic Regression (2276/5000): loss=0.5760293144100478\n",
      "Logistic Regression (2277/5000): loss=0.5759988119195713\n",
      "Logistic Regression (2278/5000): loss=0.5871146144066258\n",
      "Logistic Regression (2279/5000): loss=0.5631458147589191\n",
      "Logistic Regression (2280/5000): loss=0.570020209601561\n",
      "Logistic Regression (2281/5000): loss=0.5693263632023836\n",
      "Logistic Regression (2282/5000): loss=0.5651279721052292\n",
      "Logistic Regression (2283/5000): loss=0.5632112992564601\n",
      "Logistic Regression (2284/5000): loss=0.5650109615319163\n",
      "Logistic Regression (2285/5000): loss=0.5679620205669134\n",
      "Logistic Regression (2286/5000): loss=0.5650679279618807\n",
      "Logistic Regression (2287/5000): loss=0.5750581327077635\n",
      "Logistic Regression (2288/5000): loss=0.5632418092349807\n",
      "Logistic Regression (2289/5000): loss=0.5734111181943204\n",
      "Logistic Regression (2290/5000): loss=0.5698398823730101\n",
      "Logistic Regression (2291/5000): loss=0.5852784748196662\n",
      "Logistic Regression (2292/5000): loss=0.5649661181817349\n",
      "Logistic Regression (2293/5000): loss=0.5647486876926392\n",
      "Logistic Regression (2294/5000): loss=0.5691996576283153\n",
      "Logistic Regression (2295/5000): loss=0.5628980689672356\n",
      "Logistic Regression (2296/5000): loss=0.5675229000618781\n",
      "Logistic Regression (2297/5000): loss=0.563147192221116\n",
      "Logistic Regression (2298/5000): loss=0.563080963871044\n",
      "Logistic Regression (2299/5000): loss=0.5639560179080617\n",
      "Logistic Regression (2300/5000): loss=0.5734478951396821\n",
      "Logistic Regression (2301/5000): loss=0.5668205560524637\n",
      "Logistic Regression (2302/5000): loss=0.569704589409016\n",
      "Logistic Regression (2303/5000): loss=0.5648580904125136\n",
      "Logistic Regression (2304/5000): loss=0.5629481467122028\n",
      "Logistic Regression (2305/5000): loss=0.5629595633956128\n",
      "Logistic Regression (2306/5000): loss=0.5705610020864992\n",
      "Logistic Regression (2307/5000): loss=0.5626353736379808\n",
      "Logistic Regression (2308/5000): loss=0.5626618254630957\n",
      "Logistic Regression (2309/5000): loss=0.5747988349447049\n",
      "Logistic Regression (2310/5000): loss=0.5684595967391308\n",
      "Logistic Regression (2311/5000): loss=0.5627976549706057\n",
      "Logistic Regression (2312/5000): loss=0.5649202872502632\n",
      "Logistic Regression (2313/5000): loss=0.5626678532212832\n",
      "Logistic Regression (2314/5000): loss=0.5704983097311237\n",
      "Logistic Regression (2315/5000): loss=0.5625229135608336\n",
      "Logistic Regression (2316/5000): loss=0.5737309091367817\n",
      "Logistic Regression (2317/5000): loss=0.5645743774700674\n",
      "Logistic Regression (2318/5000): loss=0.5655059967434986\n",
      "Logistic Regression (2319/5000): loss=0.5652685842638987\n",
      "Logistic Regression (2320/5000): loss=0.5673596446756565\n",
      "Logistic Regression (2321/5000): loss=0.563916026015835\n",
      "Logistic Regression (2322/5000): loss=0.5759215478165769\n",
      "Logistic Regression (2323/5000): loss=0.5689453519411799\n",
      "Logistic Regression (2324/5000): loss=0.5706849977483529\n",
      "Logistic Regression (2325/5000): loss=0.5623908245150105\n",
      "Logistic Regression (2326/5000): loss=0.5778209344997023\n",
      "Logistic Regression (2327/5000): loss=0.5668768518050293\n",
      "Logistic Regression (2328/5000): loss=0.563479756509291\n",
      "Logistic Regression (2329/5000): loss=0.5629747721720703\n",
      "Logistic Regression (2330/5000): loss=0.5674300427886692\n",
      "Logistic Regression (2331/5000): loss=0.5624322413787778\n",
      "Logistic Regression (2332/5000): loss=0.5625147284024216\n",
      "Logistic Regression (2333/5000): loss=0.5638989351970672\n",
      "Logistic Regression (2334/5000): loss=0.5679286650886107\n",
      "Logistic Regression (2335/5000): loss=0.5624698990954649\n",
      "Logistic Regression (2336/5000): loss=0.5716415173194448\n",
      "Logistic Regression (2337/5000): loss=0.5636440397567282\n",
      "Logistic Regression (2338/5000): loss=0.5635555152352456\n",
      "Logistic Regression (2339/5000): loss=0.5629398568261634\n",
      "Logistic Regression (2340/5000): loss=0.5668993641039056\n",
      "Logistic Regression (2341/5000): loss=0.5624171661447659\n",
      "Logistic Regression (2342/5000): loss=0.5625943334215836\n",
      "Logistic Regression (2343/5000): loss=0.5624774514833488\n",
      "Logistic Regression (2344/5000): loss=0.5649230299411324\n",
      "Logistic Regression (2345/5000): loss=0.5661960364963616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (2346/5000): loss=0.5626477444479817\n",
      "Logistic Regression (2347/5000): loss=0.5642269930216174\n",
      "Logistic Regression (2348/5000): loss=0.5627800442321772\n",
      "Logistic Regression (2349/5000): loss=0.5623078243114914\n",
      "Logistic Regression (2350/5000): loss=0.5627903958843476\n",
      "Logistic Regression (2351/5000): loss=0.5646157533453385\n",
      "Logistic Regression (2352/5000): loss=0.5682172724659479\n",
      "Logistic Regression (2353/5000): loss=0.5635201545458141\n",
      "Logistic Regression (2354/5000): loss=0.562592572857933\n",
      "Logistic Regression (2355/5000): loss=0.5631708351495798\n",
      "Logistic Regression (2356/5000): loss=0.5707894608433365\n",
      "Logistic Regression (2357/5000): loss=0.5752015871296032\n",
      "Logistic Regression (2358/5000): loss=0.5625319661548179\n",
      "Logistic Regression (2359/5000): loss=0.5661617862402851\n",
      "Logistic Regression (2360/5000): loss=0.5622828576351983\n",
      "Logistic Regression (2361/5000): loss=0.5622043147042732\n",
      "Logistic Regression (2362/5000): loss=0.5638091526747578\n",
      "Logistic Regression (2363/5000): loss=0.5689464918289532\n",
      "Logistic Regression (2364/5000): loss=0.5700921497811282\n",
      "Logistic Regression (2365/5000): loss=0.5632339491920815\n",
      "Logistic Regression (2366/5000): loss=0.5647118578654212\n",
      "Logistic Regression (2367/5000): loss=0.5870672415462934\n",
      "Logistic Regression (2368/5000): loss=0.5621100429825255\n",
      "Logistic Regression (2369/5000): loss=0.562004003365924\n",
      "Logistic Regression (2370/5000): loss=0.565246699083586\n",
      "Logistic Regression (2371/5000): loss=0.5621156276979337\n",
      "Logistic Regression (2372/5000): loss=0.5637584429417851\n",
      "Logistic Regression (2373/5000): loss=0.5638741553765888\n",
      "Logistic Regression (2374/5000): loss=0.562889562941742\n",
      "Logistic Regression (2375/5000): loss=0.5634325198906966\n",
      "Logistic Regression (2376/5000): loss=0.5644581650965796\n",
      "Logistic Regression (2377/5000): loss=0.5736659723684454\n",
      "Logistic Regression (2378/5000): loss=0.5646635183424857\n",
      "Logistic Regression (2379/5000): loss=0.5659910472422517\n",
      "Logistic Regression (2380/5000): loss=0.5677961765110539\n",
      "Logistic Regression (2381/5000): loss=0.5708878758203301\n",
      "Logistic Regression (2382/5000): loss=0.5664861541468392\n",
      "Logistic Regression (2383/5000): loss=0.5732945605223588\n",
      "Logistic Regression (2384/5000): loss=0.581461112929554\n",
      "Logistic Regression (2385/5000): loss=0.5622586184312472\n",
      "Logistic Regression (2386/5000): loss=0.5623803629110701\n",
      "Logistic Regression (2387/5000): loss=0.5732570245018217\n",
      "Logistic Regression (2388/5000): loss=0.5664382637408774\n",
      "Logistic Regression (2389/5000): loss=0.5619900182967873\n",
      "Logistic Regression (2390/5000): loss=0.5713549274243717\n",
      "Logistic Regression (2391/5000): loss=0.5669484388507123\n",
      "Logistic Regression (2392/5000): loss=0.565409287650697\n",
      "Logistic Regression (2393/5000): loss=0.5664883369420677\n",
      "Logistic Regression (2394/5000): loss=0.570954591418869\n",
      "Logistic Regression (2395/5000): loss=0.5621258102300758\n",
      "Logistic Regression (2396/5000): loss=0.5621330406100116\n",
      "Logistic Regression (2397/5000): loss=0.5641231292040851\n",
      "Logistic Regression (2398/5000): loss=0.5706206541193036\n",
      "Logistic Regression (2399/5000): loss=0.5746658061129473\n",
      "Logistic Regression (2400/5000): loss=0.5665226183053093\n",
      "Logistic Regression (2401/5000): loss=0.5627845714975516\n",
      "Logistic Regression (2402/5000): loss=0.5640063891714832\n",
      "Logistic Regression (2403/5000): loss=0.5869002900857485\n",
      "Logistic Regression (2404/5000): loss=0.5662328607429363\n",
      "Logistic Regression (2405/5000): loss=0.5620297582366233\n",
      "Logistic Regression (2406/5000): loss=0.5622394480459751\n",
      "Logistic Regression (2407/5000): loss=0.5619423320465583\n",
      "Logistic Regression (2408/5000): loss=0.570816894636128\n",
      "Logistic Regression (2409/5000): loss=0.5638290563460522\n",
      "Logistic Regression (2410/5000): loss=0.5619245801178451\n",
      "Logistic Regression (2411/5000): loss=0.5697691100941824\n",
      "Logistic Regression (2412/5000): loss=0.5656966672796547\n",
      "Logistic Regression (2413/5000): loss=0.573248471530137\n",
      "Logistic Regression (2414/5000): loss=0.5644135126705423\n",
      "Logistic Regression (2415/5000): loss=0.5625730437636891\n",
      "Logistic Regression (2416/5000): loss=0.5621811971277235\n",
      "Logistic Regression (2417/5000): loss=0.5623034851853298\n",
      "Logistic Regression (2418/5000): loss=0.5647307362121735\n",
      "Logistic Regression (2419/5000): loss=0.5618550875371952\n",
      "Logistic Regression (2420/5000): loss=0.5624925028535089\n",
      "Logistic Regression (2421/5000): loss=0.5617243598039183\n",
      "Logistic Regression (2422/5000): loss=0.5624411863754389\n",
      "Logistic Regression (2423/5000): loss=0.5616344928569273\n",
      "Logistic Regression (2424/5000): loss=0.5617530701383785\n",
      "Logistic Regression (2425/5000): loss=0.5618043429731768\n",
      "Logistic Regression (2426/5000): loss=0.5691870130784219\n",
      "Logistic Regression (2427/5000): loss=0.5617281029442905\n",
      "Logistic Regression (2428/5000): loss=0.5638228755518356\n",
      "Logistic Regression (2429/5000): loss=0.5659377603123307\n",
      "Logistic Regression (2430/5000): loss=0.5619398160457239\n",
      "Logistic Regression (2431/5000): loss=0.5787435536603769\n",
      "Logistic Regression (2432/5000): loss=0.564328751162501\n",
      "Logistic Regression (2433/5000): loss=0.5642674010459936\n",
      "Logistic Regression (2434/5000): loss=0.5731865747732555\n",
      "Logistic Regression (2435/5000): loss=0.5697773178699598\n",
      "Logistic Regression (2436/5000): loss=0.6132779615527018\n",
      "Logistic Regression (2437/5000): loss=0.5709593382475894\n",
      "Logistic Regression (2438/5000): loss=0.5627074726352621\n",
      "Logistic Regression (2439/5000): loss=0.5626590084372977\n",
      "Logistic Regression (2440/5000): loss=0.5615389569686984\n",
      "Logistic Regression (2441/5000): loss=0.561450747203828\n",
      "Logistic Regression (2442/5000): loss=0.564312725658494\n",
      "Logistic Regression (2443/5000): loss=0.5620490493803354\n",
      "Logistic Regression (2444/5000): loss=0.5616384992557713\n",
      "Logistic Regression (2445/5000): loss=0.5768322461010567\n",
      "Logistic Regression (2446/5000): loss=0.5628947078296458\n",
      "Logistic Regression (2447/5000): loss=0.5656568072905294\n",
      "Logistic Regression (2448/5000): loss=0.5666597881972918\n",
      "Logistic Regression (2449/5000): loss=0.5707636810911333\n",
      "Logistic Regression (2450/5000): loss=0.5631766872962667\n",
      "Logistic Regression (2451/5000): loss=0.5613166508634974\n",
      "Logistic Regression (2452/5000): loss=0.5645991853066595\n",
      "Logistic Regression (2453/5000): loss=0.5783451824934499\n",
      "Logistic Regression (2454/5000): loss=0.5684940894089286\n",
      "Logistic Regression (2455/5000): loss=0.5615297555773825\n",
      "Logistic Regression (2456/5000): loss=0.5615891689991134\n",
      "Logistic Regression (2457/5000): loss=0.5614146109581061\n",
      "Logistic Regression (2458/5000): loss=0.5613905039654898\n",
      "Logistic Regression (2459/5000): loss=0.5631990828294262\n",
      "Logistic Regression (2460/5000): loss=0.5664059248093357\n",
      "Logistic Regression (2461/5000): loss=0.5654301443861374\n",
      "Logistic Regression (2462/5000): loss=0.5627959720104028\n",
      "Logistic Regression (2463/5000): loss=0.5796897407050287\n",
      "Logistic Regression (2464/5000): loss=0.5615327079954089\n",
      "Logistic Regression (2465/5000): loss=0.5615771252841456\n",
      "Logistic Regression (2466/5000): loss=0.5696774965535911\n",
      "Logistic Regression (2467/5000): loss=0.5702843767196464\n",
      "Logistic Regression (2468/5000): loss=0.5634967739915395\n",
      "Logistic Regression (2469/5000): loss=0.5656345115349684\n",
      "Logistic Regression (2470/5000): loss=0.5619482907003294\n",
      "Logistic Regression (2471/5000): loss=0.5648506760865147\n",
      "Logistic Regression (2472/5000): loss=0.5612058694213682\n",
      "Logistic Regression (2473/5000): loss=0.5641034638548955\n",
      "Logistic Regression (2474/5000): loss=0.5613443114389904\n",
      "Logistic Regression (2475/5000): loss=0.5613716288267107\n",
      "Logistic Regression (2476/5000): loss=0.5887510844126748\n",
      "Logistic Regression (2477/5000): loss=0.5657435585495048\n",
      "Logistic Regression (2478/5000): loss=0.5901140361970287\n",
      "Logistic Regression (2479/5000): loss=0.5615890338246264\n",
      "Logistic Regression (2480/5000): loss=0.5655209854139295\n",
      "Logistic Regression (2481/5000): loss=0.5618398260590166\n",
      "Logistic Regression (2482/5000): loss=0.5701056167354968\n",
      "Logistic Regression (2483/5000): loss=0.5636822631347207\n",
      "Logistic Regression (2484/5000): loss=0.5618023378648083\n",
      "Logistic Regression (2485/5000): loss=0.5710419932867421\n",
      "Logistic Regression (2486/5000): loss=0.5625931049424204\n",
      "Logistic Regression (2487/5000): loss=0.5642902080945063\n",
      "Logistic Regression (2488/5000): loss=0.5804726885724512\n",
      "Logistic Regression (2489/5000): loss=0.5709430442152654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (2490/5000): loss=0.5636168244527345\n",
      "Logistic Regression (2491/5000): loss=0.5615276172409415\n",
      "Logistic Regression (2492/5000): loss=0.5609975856575432\n",
      "Logistic Regression (2493/5000): loss=0.5637658772446731\n",
      "Logistic Regression (2494/5000): loss=0.5901344991791\n",
      "Logistic Regression (2495/5000): loss=0.5613513513815147\n",
      "Logistic Regression (2496/5000): loss=0.5615784706684978\n",
      "Logistic Regression (2497/5000): loss=0.5632889604331595\n",
      "Logistic Regression (2498/5000): loss=0.5617977855033693\n",
      "Logistic Regression (2499/5000): loss=0.5612711784665473\n",
      "Logistic Regression (2500/5000): loss=0.5621173816136865\n",
      "Logistic Regression (2501/5000): loss=0.5609058351682694\n",
      "Logistic Regression (2502/5000): loss=0.5725517653173539\n",
      "Logistic Regression (2503/5000): loss=0.5914092861979803\n",
      "Logistic Regression (2504/5000): loss=0.5628142258674832\n",
      "Logistic Regression (2505/5000): loss=0.5796789551488113\n",
      "Logistic Regression (2506/5000): loss=0.5612589031438772\n",
      "Logistic Regression (2507/5000): loss=0.5616842837059901\n",
      "Logistic Regression (2508/5000): loss=0.5636123118564886\n",
      "Logistic Regression (2509/5000): loss=0.5701604640350499\n",
      "Logistic Regression (2510/5000): loss=0.576008611284823\n",
      "Logistic Regression (2511/5000): loss=0.5620890139885785\n",
      "Logistic Regression (2512/5000): loss=0.5630548778104806\n",
      "Logistic Regression (2513/5000): loss=0.5621145000496109\n",
      "Logistic Regression (2514/5000): loss=0.573726043704231\n",
      "Logistic Regression (2515/5000): loss=0.5659361910545658\n",
      "Logistic Regression (2516/5000): loss=0.563684093580172\n",
      "Logistic Regression (2517/5000): loss=0.5655997301814473\n",
      "Logistic Regression (2518/5000): loss=0.564290311093021\n",
      "Logistic Regression (2519/5000): loss=0.5615860143221849\n",
      "Logistic Regression (2520/5000): loss=0.5632866993475121\n",
      "Logistic Regression (2521/5000): loss=0.5619249163037874\n",
      "Logistic Regression (2522/5000): loss=0.56844278204358\n",
      "Logistic Regression (2523/5000): loss=0.56175806395518\n",
      "Logistic Regression (2524/5000): loss=0.5699239828011969\n",
      "Logistic Regression (2525/5000): loss=0.5833211220302221\n",
      "Logistic Regression (2526/5000): loss=0.5654576214313635\n",
      "Logistic Regression (2527/5000): loss=0.5924710475370092\n",
      "Logistic Regression (2528/5000): loss=0.5676798507237198\n",
      "Logistic Regression (2529/5000): loss=0.5616679241958803\n",
      "Logistic Regression (2530/5000): loss=0.5701925261666431\n",
      "Logistic Regression (2531/5000): loss=0.5617580390841193\n",
      "Logistic Regression (2532/5000): loss=0.5617940794159465\n",
      "Logistic Regression (2533/5000): loss=0.5631954678393116\n",
      "Logistic Regression (2534/5000): loss=0.5618239452500917\n",
      "Logistic Regression (2535/5000): loss=0.5623805278191712\n",
      "Logistic Regression (2536/5000): loss=0.5634913143577431\n",
      "Logistic Regression (2537/5000): loss=0.5759778205718186\n",
      "Logistic Regression (2538/5000): loss=0.5695630948264198\n",
      "Logistic Regression (2539/5000): loss=0.5638410946396201\n",
      "Logistic Regression (2540/5000): loss=0.5619648411981094\n",
      "Logistic Regression (2541/5000): loss=0.5618784127396981\n",
      "Logistic Regression (2542/5000): loss=0.5614604036150965\n",
      "Logistic Regression (2543/5000): loss=0.5615104227046456\n",
      "Logistic Regression (2544/5000): loss=0.5642220728236172\n",
      "Logistic Regression (2545/5000): loss=0.5796198662953638\n",
      "Logistic Regression (2546/5000): loss=0.5635123752953877\n",
      "Logistic Regression (2547/5000): loss=0.5649536337374471\n",
      "Logistic Regression (2548/5000): loss=0.5610141292905952\n",
      "Logistic Regression (2549/5000): loss=0.5612790410676369\n",
      "Logistic Regression (2550/5000): loss=0.5626117683150708\n",
      "Logistic Regression (2551/5000): loss=0.5631790703137025\n",
      "Logistic Regression (2552/5000): loss=0.562823624643779\n",
      "Logistic Regression (2553/5000): loss=0.5624487234195813\n",
      "Logistic Regression (2554/5000): loss=0.5611608607233816\n",
      "Logistic Regression (2555/5000): loss=0.5737450358328672\n",
      "Logistic Regression (2556/5000): loss=0.5611568082315691\n",
      "Logistic Regression (2557/5000): loss=0.5638421500026217\n",
      "Logistic Regression (2558/5000): loss=0.5607049590268681\n",
      "Logistic Regression (2559/5000): loss=0.5632446351868121\n",
      "Logistic Regression (2560/5000): loss=0.5676942626304589\n",
      "Logistic Regression (2561/5000): loss=0.5691683111378356\n",
      "Logistic Regression (2562/5000): loss=0.5608050715325074\n",
      "Logistic Regression (2563/5000): loss=0.5632275010727829\n",
      "Logistic Regression (2564/5000): loss=0.5608025669309386\n",
      "Logistic Regression (2565/5000): loss=0.5682639857757625\n",
      "Logistic Regression (2566/5000): loss=0.5605983994309917\n",
      "Logistic Regression (2567/5000): loss=0.5607079676809898\n",
      "Logistic Regression (2568/5000): loss=0.5638983088701222\n",
      "Logistic Regression (2569/5000): loss=0.5649583092694229\n",
      "Logistic Regression (2570/5000): loss=0.5670248626296813\n",
      "Logistic Regression (2571/5000): loss=0.5614565681431728\n",
      "Logistic Regression (2572/5000): loss=0.565234913329285\n",
      "Logistic Regression (2573/5000): loss=0.5691671712983735\n",
      "Logistic Regression (2574/5000): loss=0.5621153246504548\n",
      "Logistic Regression (2575/5000): loss=0.5655336002925124\n",
      "Logistic Regression (2576/5000): loss=0.5609983221531064\n",
      "Logistic Regression (2577/5000): loss=0.563802469021724\n",
      "Logistic Regression (2578/5000): loss=0.5605796948627897\n",
      "Logistic Regression (2579/5000): loss=0.5609763191407682\n",
      "Logistic Regression (2580/5000): loss=0.5643672782286504\n",
      "Logistic Regression (2581/5000): loss=0.5612654707055875\n",
      "Logistic Regression (2582/5000): loss=0.5605719142185681\n",
      "Logistic Regression (2583/5000): loss=0.5608754379873409\n",
      "Logistic Regression (2584/5000): loss=0.5689455600928381\n",
      "Logistic Regression (2585/5000): loss=0.5606539939406988\n",
      "Logistic Regression (2586/5000): loss=0.564767871319317\n",
      "Logistic Regression (2587/5000): loss=0.5632681229854472\n",
      "Logistic Regression (2588/5000): loss=0.5636273652596887\n",
      "Logistic Regression (2589/5000): loss=0.5629942060376757\n",
      "Logistic Regression (2590/5000): loss=0.5690026905863748\n",
      "Logistic Regression (2591/5000): loss=0.5618628930228791\n",
      "Logistic Regression (2592/5000): loss=0.5609091630861073\n",
      "Logistic Regression (2593/5000): loss=0.561391891798084\n",
      "Logistic Regression (2594/5000): loss=0.5646707500342816\n",
      "Logistic Regression (2595/5000): loss=0.5620206241409874\n",
      "Logistic Regression (2596/5000): loss=0.5601684552001769\n",
      "Logistic Regression (2597/5000): loss=0.564486078750198\n",
      "Logistic Regression (2598/5000): loss=0.5607919747637643\n",
      "Logistic Regression (2599/5000): loss=0.5648416490548185\n",
      "Logistic Regression (2600/5000): loss=0.5605037876437696\n",
      "Logistic Regression (2601/5000): loss=0.5637182975961272\n",
      "Logistic Regression (2602/5000): loss=0.5694367184108233\n",
      "Logistic Regression (2603/5000): loss=0.5652103274665168\n",
      "Logistic Regression (2604/5000): loss=0.5711306214306844\n",
      "Logistic Regression (2605/5000): loss=0.5614520976523864\n",
      "Logistic Regression (2606/5000): loss=0.5881711208282745\n",
      "Logistic Regression (2607/5000): loss=0.6161863294667492\n",
      "Logistic Regression (2608/5000): loss=0.5651072083309844\n",
      "Logistic Regression (2609/5000): loss=0.5657196172796704\n",
      "Logistic Regression (2610/5000): loss=0.5600149062198692\n",
      "Logistic Regression (2611/5000): loss=0.562322201101191\n",
      "Logistic Regression (2612/5000): loss=0.5629349375188539\n",
      "Logistic Regression (2613/5000): loss=0.5652015494927667\n",
      "Logistic Regression (2614/5000): loss=0.5708657848924\n",
      "Logistic Regression (2615/5000): loss=0.577739590308691\n",
      "Logistic Regression (2616/5000): loss=0.5780829114674154\n",
      "Logistic Regression (2617/5000): loss=0.5624236574621262\n",
      "Logistic Regression (2618/5000): loss=0.5610386087224288\n",
      "Logistic Regression (2619/5000): loss=0.560117093510898\n",
      "Logistic Regression (2620/5000): loss=0.5612890288223278\n",
      "Logistic Regression (2621/5000): loss=0.5602725148663696\n",
      "Logistic Regression (2622/5000): loss=0.5761786198177782\n",
      "Logistic Regression (2623/5000): loss=0.5742158684012136\n",
      "Logistic Regression (2624/5000): loss=0.5658129034730424\n",
      "Logistic Regression (2625/5000): loss=0.5716133523217107\n",
      "Logistic Regression (2626/5000): loss=0.571421711165189\n",
      "Logistic Regression (2627/5000): loss=0.5653556146811646\n",
      "Logistic Regression (2628/5000): loss=0.5605509354100907\n",
      "Logistic Regression (2629/5000): loss=0.5602633543627689\n",
      "Logistic Regression (2630/5000): loss=0.5725771469453987\n",
      "Logistic Regression (2631/5000): loss=0.5604903550678971\n",
      "Logistic Regression (2632/5000): loss=0.5603038423221866\n",
      "Logistic Regression (2633/5000): loss=0.5601215161512487\n",
      "Logistic Regression (2634/5000): loss=0.560581935059887\n",
      "Logistic Regression (2635/5000): loss=0.5605884833023131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (2636/5000): loss=0.560706750154069\n",
      "Logistic Regression (2637/5000): loss=0.5684757598487018\n",
      "Logistic Regression (2638/5000): loss=0.5615085701507466\n",
      "Logistic Regression (2639/5000): loss=0.5620252125376809\n",
      "Logistic Regression (2640/5000): loss=0.5611024798961093\n",
      "Logistic Regression (2641/5000): loss=0.5685038528375915\n",
      "Logistic Regression (2642/5000): loss=0.5638980920419508\n",
      "Logistic Regression (2643/5000): loss=0.5628053642213166\n",
      "Logistic Regression (2644/5000): loss=0.5624875952195392\n",
      "Logistic Regression (2645/5000): loss=0.5609590276429017\n",
      "Logistic Regression (2646/5000): loss=0.5602167670686029\n",
      "Logistic Regression (2647/5000): loss=0.5623387977236309\n",
      "Logistic Regression (2648/5000): loss=0.5602500223316572\n",
      "Logistic Regression (2649/5000): loss=0.5620366615481844\n",
      "Logistic Regression (2650/5000): loss=0.574469928722749\n",
      "Logistic Regression (2651/5000): loss=0.563302250546756\n",
      "Logistic Regression (2652/5000): loss=0.5681253394910427\n",
      "Logistic Regression (2653/5000): loss=0.5647731368215579\n",
      "Logistic Regression (2654/5000): loss=0.5644188829442656\n",
      "Logistic Regression (2655/5000): loss=0.5648480873789549\n",
      "Logistic Regression (2656/5000): loss=0.5643375208632693\n",
      "Logistic Regression (2657/5000): loss=0.5598885934212111\n",
      "Logistic Regression (2658/5000): loss=0.5608912379746016\n",
      "Logistic Regression (2659/5000): loss=0.5639139695991359\n",
      "Logistic Regression (2660/5000): loss=0.5694577769578798\n",
      "Logistic Regression (2661/5000): loss=0.5618434915628229\n",
      "Logistic Regression (2662/5000): loss=0.5690863681108927\n",
      "Logistic Regression (2663/5000): loss=0.581026658995932\n",
      "Logistic Regression (2664/5000): loss=0.5657228260880437\n",
      "Logistic Regression (2665/5000): loss=0.559736059484645\n",
      "Logistic Regression (2666/5000): loss=0.5597163951385604\n",
      "Logistic Regression (2667/5000): loss=0.5687420572518529\n",
      "Logistic Regression (2668/5000): loss=0.5732017079832299\n",
      "Logistic Regression (2669/5000): loss=0.5611086625583988\n",
      "Logistic Regression (2670/5000): loss=0.5609899000811304\n",
      "Logistic Regression (2671/5000): loss=0.5657648106826736\n",
      "Logistic Regression (2672/5000): loss=0.5634111975855393\n",
      "Logistic Regression (2673/5000): loss=0.5597269907314179\n",
      "Logistic Regression (2674/5000): loss=0.5802791980549651\n",
      "Logistic Regression (2675/5000): loss=0.5617783185229251\n",
      "Logistic Regression (2676/5000): loss=0.5617778621746817\n",
      "Logistic Regression (2677/5000): loss=0.56005359619097\n",
      "Logistic Regression (2678/5000): loss=0.5612952516408388\n",
      "Logistic Regression (2679/5000): loss=0.577559218510393\n",
      "Logistic Regression (2680/5000): loss=0.594178795797599\n",
      "Logistic Regression (2681/5000): loss=0.5863770130779217\n",
      "Logistic Regression (2682/5000): loss=0.562301658281648\n",
      "Logistic Regression (2683/5000): loss=0.5658665505516066\n",
      "Logistic Regression (2684/5000): loss=0.5609492666382302\n",
      "Logistic Regression (2685/5000): loss=0.5783810591050853\n",
      "Logistic Regression (2686/5000): loss=0.560223366013157\n",
      "Logistic Regression (2687/5000): loss=0.5725252179825441\n",
      "Logistic Regression (2688/5000): loss=0.560942516524639\n",
      "Logistic Regression (2689/5000): loss=0.5595142791645985\n",
      "Logistic Regression (2690/5000): loss=0.5596740974678924\n",
      "Logistic Regression (2691/5000): loss=0.56136435122974\n",
      "Logistic Regression (2692/5000): loss=0.5646605528086173\n",
      "Logistic Regression (2693/5000): loss=0.5596687579582118\n",
      "Logistic Regression (2694/5000): loss=0.5608822905141989\n",
      "Logistic Regression (2695/5000): loss=0.568288000595238\n",
      "Logistic Regression (2696/5000): loss=0.5661442462627639\n",
      "Logistic Regression (2697/5000): loss=0.5604291777446986\n",
      "Logistic Regression (2698/5000): loss=0.5703974467026693\n",
      "Logistic Regression (2699/5000): loss=0.5594188071083809\n",
      "Logistic Regression (2700/5000): loss=0.5602702189538122\n",
      "Logistic Regression (2701/5000): loss=0.5859958122620207\n",
      "Logistic Regression (2702/5000): loss=0.5597425914343628\n",
      "Logistic Regression (2703/5000): loss=0.5594388598523675\n",
      "Logistic Regression (2704/5000): loss=0.5611058699087682\n",
      "Logistic Regression (2705/5000): loss=0.560874484435475\n",
      "Logistic Regression (2706/5000): loss=0.5603676549442531\n",
      "Logistic Regression (2707/5000): loss=0.5595788369398531\n",
      "Logistic Regression (2708/5000): loss=0.5595320557286761\n",
      "Logistic Regression (2709/5000): loss=0.5624081444982506\n",
      "Logistic Regression (2710/5000): loss=0.5686280690849482\n",
      "Logistic Regression (2711/5000): loss=0.5752597812597313\n",
      "Logistic Regression (2712/5000): loss=0.5659581236787586\n",
      "Logistic Regression (2713/5000): loss=0.5645481495838258\n",
      "Logistic Regression (2714/5000): loss=0.5718681213513338\n",
      "Logistic Regression (2715/5000): loss=0.5650247829685262\n",
      "Logistic Regression (2716/5000): loss=0.5602402773498489\n",
      "Logistic Regression (2717/5000): loss=0.5627928129949197\n",
      "Logistic Regression (2718/5000): loss=0.5603406759709514\n",
      "Logistic Regression (2719/5000): loss=0.5769414445783406\n",
      "Logistic Regression (2720/5000): loss=0.5645555877459975\n",
      "Logistic Regression (2721/5000): loss=0.5611333732950068\n",
      "Logistic Regression (2722/5000): loss=0.5723041944938876\n",
      "Logistic Regression (2723/5000): loss=0.560753208832585\n",
      "Logistic Regression (2724/5000): loss=0.5593369921286085\n",
      "Logistic Regression (2725/5000): loss=0.5594358942613972\n",
      "Logistic Regression (2726/5000): loss=0.5642780449930194\n",
      "Logistic Regression (2727/5000): loss=0.5601599947021796\n",
      "Logistic Regression (2728/5000): loss=0.5629492215069504\n",
      "Logistic Regression (2729/5000): loss=0.5651647083591466\n",
      "Logistic Regression (2730/5000): loss=0.5869218322775173\n",
      "Logistic Regression (2731/5000): loss=0.5594649182017636\n",
      "Logistic Regression (2732/5000): loss=0.5619741767063263\n",
      "Logistic Regression (2733/5000): loss=0.5617473704069647\n",
      "Logistic Regression (2734/5000): loss=0.5606859928922822\n",
      "Logistic Regression (2735/5000): loss=0.5595722793187617\n",
      "Logistic Regression (2736/5000): loss=0.5630754363461253\n",
      "Logistic Regression (2737/5000): loss=0.5594928381622322\n",
      "Logistic Regression (2738/5000): loss=0.5625901812626054\n",
      "Logistic Regression (2739/5000): loss=0.5597034158365071\n",
      "Logistic Regression (2740/5000): loss=0.5641352402147457\n",
      "Logistic Regression (2741/5000): loss=0.5601520771179567\n",
      "Logistic Regression (2742/5000): loss=0.5616689557163435\n",
      "Logistic Regression (2743/5000): loss=0.5597990280513258\n",
      "Logistic Regression (2744/5000): loss=0.5592460583055449\n",
      "Logistic Regression (2745/5000): loss=0.5614180327138826\n",
      "Logistic Regression (2746/5000): loss=0.5627969847463932\n",
      "Logistic Regression (2747/5000): loss=0.567333619655443\n",
      "Logistic Regression (2748/5000): loss=0.5593246633732506\n",
      "Logistic Regression (2749/5000): loss=0.5592593780021458\n",
      "Logistic Regression (2750/5000): loss=0.5641825129726866\n",
      "Logistic Regression (2751/5000): loss=0.5605757673444209\n",
      "Logistic Regression (2752/5000): loss=0.5638742255968006\n",
      "Logistic Regression (2753/5000): loss=0.5648785059121465\n",
      "Logistic Regression (2754/5000): loss=0.5595280861315385\n",
      "Logistic Regression (2755/5000): loss=0.570021713319018\n",
      "Logistic Regression (2756/5000): loss=0.5603629313828331\n",
      "Logistic Regression (2757/5000): loss=0.5650176576755445\n",
      "Logistic Regression (2758/5000): loss=0.559723267998271\n",
      "Logistic Regression (2759/5000): loss=0.5678398916576087\n",
      "Logistic Regression (2760/5000): loss=0.559390489094406\n",
      "Logistic Regression (2761/5000): loss=0.5627565857033625\n",
      "Logistic Regression (2762/5000): loss=0.5613500246256288\n",
      "Logistic Regression (2763/5000): loss=0.5601844341856831\n",
      "Logistic Regression (2764/5000): loss=0.568746631064112\n",
      "Logistic Regression (2765/5000): loss=0.5590706903884652\n",
      "Logistic Regression (2766/5000): loss=0.5592331887029254\n",
      "Logistic Regression (2767/5000): loss=0.5604200983030538\n",
      "Logistic Regression (2768/5000): loss=0.5590592894105835\n",
      "Logistic Regression (2769/5000): loss=0.563278490523941\n",
      "Logistic Regression (2770/5000): loss=0.5726537068691685\n",
      "Logistic Regression (2771/5000): loss=0.5599699466879128\n",
      "Logistic Regression (2772/5000): loss=0.5624275222104169\n",
      "Logistic Regression (2773/5000): loss=0.5626362530284241\n",
      "Logistic Regression (2774/5000): loss=0.5607589040789064\n",
      "Logistic Regression (2775/5000): loss=0.5603770026302567\n",
      "Logistic Regression (2776/5000): loss=0.5602968948335924\n",
      "Logistic Regression (2777/5000): loss=0.5595262741452506\n",
      "Logistic Regression (2778/5000): loss=0.5598703626668825\n",
      "Logistic Regression (2779/5000): loss=0.5597253564604616\n",
      "Logistic Regression (2780/5000): loss=0.5609009565050118\n",
      "Logistic Regression (2781/5000): loss=0.5600268818496206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (2782/5000): loss=0.5591459749070822\n",
      "Logistic Regression (2783/5000): loss=0.5649575293916869\n",
      "Logistic Regression (2784/5000): loss=0.5592925552101603\n",
      "Logistic Regression (2785/5000): loss=0.5663996164783218\n",
      "Logistic Regression (2786/5000): loss=0.5654222086732055\n",
      "Logistic Regression (2787/5000): loss=0.5617837802550387\n",
      "Logistic Regression (2788/5000): loss=0.5613337878487555\n",
      "Logistic Regression (2789/5000): loss=0.5591572658803835\n",
      "Logistic Regression (2790/5000): loss=0.5821705429085724\n",
      "Logistic Regression (2791/5000): loss=0.5600408249898386\n",
      "Logistic Regression (2792/5000): loss=0.5592788354216844\n",
      "Logistic Regression (2793/5000): loss=0.5780633103042339\n",
      "Logistic Regression (2794/5000): loss=0.5601047080245527\n",
      "Logistic Regression (2795/5000): loss=0.5592273023400987\n",
      "Logistic Regression (2796/5000): loss=0.5609634523113066\n",
      "Logistic Regression (2797/5000): loss=0.5591467931993364\n",
      "Logistic Regression (2798/5000): loss=0.5639783249188993\n",
      "Logistic Regression (2799/5000): loss=0.5624611251632655\n",
      "Logistic Regression (2800/5000): loss=0.5606551242775235\n",
      "Logistic Regression (2801/5000): loss=0.573403251179304\n",
      "Logistic Regression (2802/5000): loss=0.5670769897748578\n",
      "Logistic Regression (2803/5000): loss=0.5617389185709939\n",
      "Logistic Regression (2804/5000): loss=0.5604972295973093\n",
      "Logistic Regression (2805/5000): loss=0.5664951587995669\n",
      "Logistic Regression (2806/5000): loss=0.5664190472471542\n",
      "Logistic Regression (2807/5000): loss=0.5593644113162076\n",
      "Logistic Regression (2808/5000): loss=0.5637781038725131\n",
      "Logistic Regression (2809/5000): loss=0.5588546339889765\n",
      "Logistic Regression (2810/5000): loss=0.5606521752000829\n",
      "Logistic Regression (2811/5000): loss=0.5608065318327607\n",
      "Logistic Regression (2812/5000): loss=0.5636459503948154\n",
      "Logistic Regression (2813/5000): loss=0.5758173636133792\n",
      "Logistic Regression (2814/5000): loss=0.5648622173283332\n",
      "Logistic Regression (2815/5000): loss=0.5650786114838389\n",
      "Logistic Regression (2816/5000): loss=0.5617104542283394\n",
      "Logistic Regression (2817/5000): loss=0.5610792505858677\n",
      "Logistic Regression (2818/5000): loss=0.5593202190490936\n",
      "Logistic Regression (2819/5000): loss=0.559371893853525\n",
      "Logistic Regression (2820/5000): loss=0.5622319240141428\n",
      "Logistic Regression (2821/5000): loss=0.5593387303039261\n",
      "Logistic Regression (2822/5000): loss=0.560749022546292\n",
      "Logistic Regression (2823/5000): loss=0.5612310820784521\n",
      "Logistic Regression (2824/5000): loss=0.5610535018159264\n",
      "Logistic Regression (2825/5000): loss=0.5587462471744624\n",
      "Logistic Regression (2826/5000): loss=0.5589517346795791\n",
      "Logistic Regression (2827/5000): loss=0.559671442174164\n",
      "Logistic Regression (2828/5000): loss=0.56098301575103\n",
      "Logistic Regression (2829/5000): loss=0.5610480353192232\n",
      "Logistic Regression (2830/5000): loss=0.5592650723393171\n",
      "Logistic Regression (2831/5000): loss=0.5651567254132577\n",
      "Logistic Regression (2832/5000): loss=0.5595872543724744\n",
      "Logistic Regression (2833/5000): loss=0.560644602541256\n",
      "Logistic Regression (2834/5000): loss=0.5674965715388353\n",
      "Logistic Regression (2835/5000): loss=0.5656207251315785\n",
      "Logistic Regression (2836/5000): loss=0.560276938716522\n",
      "Logistic Regression (2837/5000): loss=0.5587438412041532\n",
      "Logistic Regression (2838/5000): loss=0.5594082305099755\n",
      "Logistic Regression (2839/5000): loss=0.5595586786262708\n",
      "Logistic Regression (2840/5000): loss=0.5585299767719755\n",
      "Logistic Regression (2841/5000): loss=0.5605009806594832\n",
      "Logistic Regression (2842/5000): loss=0.55881023957695\n",
      "Logistic Regression (2843/5000): loss=0.5588932806610069\n",
      "Logistic Regression (2844/5000): loss=0.5587123561112953\n",
      "Logistic Regression (2845/5000): loss=0.5651125253684914\n",
      "Logistic Regression (2846/5000): loss=0.5584833423220965\n",
      "Logistic Regression (2847/5000): loss=0.5598723997146718\n",
      "Logistic Regression (2848/5000): loss=0.5742606143053514\n",
      "Logistic Regression (2849/5000): loss=0.5716124927377663\n",
      "Logistic Regression (2850/5000): loss=0.5658118615719567\n",
      "Logistic Regression (2851/5000): loss=0.5709496938046825\n",
      "Logistic Regression (2852/5000): loss=0.5641901639280892\n",
      "Logistic Regression (2853/5000): loss=0.55899318981214\n",
      "Logistic Regression (2854/5000): loss=0.5658167143096852\n",
      "Logistic Regression (2855/5000): loss=0.5584633633211816\n",
      "Logistic Regression (2856/5000): loss=0.562048106620609\n",
      "Logistic Regression (2857/5000): loss=0.5656832920419629\n",
      "Logistic Regression (2858/5000): loss=0.5597221222712014\n",
      "Logistic Regression (2859/5000): loss=0.558900930335316\n",
      "Logistic Regression (2860/5000): loss=0.5605954472949585\n",
      "Logistic Regression (2861/5000): loss=0.5608392218333377\n",
      "Logistic Regression (2862/5000): loss=0.5617600422961404\n",
      "Logistic Regression (2863/5000): loss=0.5600045638041461\n",
      "Logistic Regression (2864/5000): loss=0.5597142438090618\n",
      "Logistic Regression (2865/5000): loss=0.5726017456477799\n",
      "Logistic Regression (2866/5000): loss=0.5616360278771023\n",
      "Logistic Regression (2867/5000): loss=0.5636340633032337\n",
      "Logistic Regression (2868/5000): loss=0.5612521939543643\n",
      "Logistic Regression (2869/5000): loss=0.567878087288307\n",
      "Logistic Regression (2870/5000): loss=0.5604165014532121\n",
      "Logistic Regression (2871/5000): loss=0.5702969679973787\n",
      "Logistic Regression (2872/5000): loss=0.5642902476574974\n",
      "Logistic Regression (2873/5000): loss=0.5604564462406499\n",
      "Logistic Regression (2874/5000): loss=0.5671347555100713\n",
      "Logistic Regression (2875/5000): loss=0.5642744818078655\n",
      "Logistic Regression (2876/5000): loss=0.56883013939319\n",
      "Logistic Regression (2877/5000): loss=0.5625753388571962\n",
      "Logistic Regression (2878/5000): loss=0.5587313003772026\n",
      "Logistic Regression (2879/5000): loss=0.5587567718204062\n",
      "Logistic Regression (2880/5000): loss=0.558863116745617\n",
      "Logistic Regression (2881/5000): loss=0.5597270119971984\n",
      "Logistic Regression (2882/5000): loss=0.5597084496508961\n",
      "Logistic Regression (2883/5000): loss=0.5624722378858369\n",
      "Logistic Regression (2884/5000): loss=0.5588487109200322\n",
      "Logistic Regression (2885/5000): loss=0.5617514017580734\n",
      "Logistic Regression (2886/5000): loss=0.5773046004696496\n",
      "Logistic Regression (2887/5000): loss=0.563641492525551\n",
      "Logistic Regression (2888/5000): loss=0.5628265124165927\n",
      "Logistic Regression (2889/5000): loss=0.5586588978443259\n",
      "Logistic Regression (2890/5000): loss=0.5841430466008704\n",
      "Logistic Regression (2891/5000): loss=0.5624105059100254\n",
      "Logistic Regression (2892/5000): loss=0.5583582453224564\n",
      "Logistic Regression (2893/5000): loss=0.5584981766244562\n",
      "Logistic Regression (2894/5000): loss=0.5590702711061822\n",
      "Logistic Regression (2895/5000): loss=0.5622307753397797\n",
      "Logistic Regression (2896/5000): loss=0.5581308760319206\n",
      "Logistic Regression (2897/5000): loss=0.5633632119797085\n",
      "Logistic Regression (2898/5000): loss=0.5599461889693363\n",
      "Logistic Regression (2899/5000): loss=0.5604949352750181\n",
      "Logistic Regression (2900/5000): loss=0.5582523384430241\n",
      "Logistic Regression (2901/5000): loss=0.5617061736359548\n",
      "Logistic Regression (2902/5000): loss=0.5598203642210569\n",
      "Logistic Regression (2903/5000): loss=0.5606649970567668\n",
      "Logistic Regression (2904/5000): loss=0.5580325225864058\n",
      "Logistic Regression (2905/5000): loss=0.5650676053269719\n",
      "Logistic Regression (2906/5000): loss=0.5685228105010555\n",
      "Logistic Regression (2907/5000): loss=0.5591443689786005\n",
      "Logistic Regression (2908/5000): loss=0.561123834310193\n",
      "Logistic Regression (2909/5000): loss=0.5601428177342226\n",
      "Logistic Regression (2910/5000): loss=0.558016002221886\n",
      "Logistic Regression (2911/5000): loss=0.5593523376098887\n",
      "Logistic Regression (2912/5000): loss=0.557997219750882\n",
      "Logistic Regression (2913/5000): loss=0.561721064860797\n",
      "Logistic Regression (2914/5000): loss=0.5613470439224735\n",
      "Logistic Regression (2915/5000): loss=0.563538944828797\n",
      "Logistic Regression (2916/5000): loss=0.5603847743599791\n",
      "Logistic Regression (2917/5000): loss=0.5580119422095717\n",
      "Logistic Regression (2918/5000): loss=0.5578771310881204\n",
      "Logistic Regression (2919/5000): loss=0.5595573941825212\n",
      "Logistic Regression (2920/5000): loss=0.5589663949227099\n",
      "Logistic Regression (2921/5000): loss=0.5578842481347965\n",
      "Logistic Regression (2922/5000): loss=0.565134933907786\n",
      "Logistic Regression (2923/5000): loss=0.5581127617302677\n",
      "Logistic Regression (2924/5000): loss=0.5580661046720432\n",
      "Logistic Regression (2925/5000): loss=0.5623207277483948\n",
      "Logistic Regression (2926/5000): loss=0.5640852793228585\n",
      "Logistic Regression (2927/5000): loss=0.5657201683414006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (2928/5000): loss=0.5629369462704691\n",
      "Logistic Regression (2929/5000): loss=0.5668699649492488\n",
      "Logistic Regression (2930/5000): loss=0.5585816030462891\n",
      "Logistic Regression (2931/5000): loss=0.5797925399719587\n",
      "Logistic Regression (2932/5000): loss=0.5589749396617496\n",
      "Logistic Regression (2933/5000): loss=0.5592700121231634\n",
      "Logistic Regression (2934/5000): loss=0.5584490792372077\n",
      "Logistic Regression (2935/5000): loss=0.5586075476761851\n",
      "Logistic Regression (2936/5000): loss=0.5583606947239944\n",
      "Logistic Regression (2937/5000): loss=0.5597494359626117\n",
      "Logistic Regression (2938/5000): loss=0.5584202977978328\n",
      "Logistic Regression (2939/5000): loss=0.5589308395837609\n",
      "Logistic Regression (2940/5000): loss=0.5630454508367073\n",
      "Logistic Regression (2941/5000): loss=0.5584941964723685\n",
      "Logistic Regression (2942/5000): loss=0.5600899835919821\n",
      "Logistic Regression (2943/5000): loss=0.5593403546412488\n",
      "Logistic Regression (2944/5000): loss=0.5708904039830762\n",
      "Logistic Regression (2945/5000): loss=0.5579978917310384\n",
      "Logistic Regression (2946/5000): loss=0.5605239293519251\n",
      "Logistic Regression (2947/5000): loss=0.562321115399245\n",
      "Logistic Regression (2948/5000): loss=0.5579008731304773\n",
      "Logistic Regression (2949/5000): loss=0.5579790744721418\n",
      "Logistic Regression (2950/5000): loss=0.5772424696034839\n",
      "Logistic Regression (2951/5000): loss=0.5602255247451947\n",
      "Logistic Regression (2952/5000): loss=0.5580049508885842\n",
      "Logistic Regression (2953/5000): loss=0.5578271341562562\n",
      "Logistic Regression (2954/5000): loss=0.5583598730616162\n",
      "Logistic Regression (2955/5000): loss=0.5648895734868215\n",
      "Logistic Regression (2956/5000): loss=0.5643110449392286\n",
      "Logistic Regression (2957/5000): loss=0.5623362243491482\n",
      "Logistic Regression (2958/5000): loss=0.5579266560509487\n",
      "Logistic Regression (2959/5000): loss=0.5596721573441769\n",
      "Logistic Regression (2960/5000): loss=0.5577870388617504\n",
      "Logistic Regression (2961/5000): loss=0.5581798353595984\n",
      "Logistic Regression (2962/5000): loss=0.5575780000089819\n",
      "Logistic Regression (2963/5000): loss=0.560098332413989\n",
      "Logistic Regression (2964/5000): loss=0.5577162551805791\n",
      "Logistic Regression (2965/5000): loss=0.5620178613620931\n",
      "Logistic Regression (2966/5000): loss=0.5582863346693161\n",
      "Logistic Regression (2967/5000): loss=0.5579995300696259\n",
      "Logistic Regression (2968/5000): loss=0.5582969298137526\n",
      "Logistic Regression (2969/5000): loss=0.5616769607458445\n",
      "Logistic Regression (2970/5000): loss=0.5582275288644868\n",
      "Logistic Regression (2971/5000): loss=0.558052069059222\n",
      "Logistic Regression (2972/5000): loss=0.5583990254241037\n",
      "Logistic Regression (2973/5000): loss=0.558378090794427\n",
      "Logistic Regression (2974/5000): loss=0.5602493868125829\n",
      "Logistic Regression (2975/5000): loss=0.5741273662083537\n",
      "Logistic Regression (2976/5000): loss=0.5581005117428511\n",
      "Logistic Regression (2977/5000): loss=0.5579845100453181\n",
      "Logistic Regression (2978/5000): loss=0.5585944404647147\n",
      "Logistic Regression (2979/5000): loss=0.56691006402086\n",
      "Logistic Regression (2980/5000): loss=0.5575923869962867\n",
      "Logistic Regression (2981/5000): loss=0.560038410457417\n",
      "Logistic Regression (2982/5000): loss=0.561754418216705\n",
      "Logistic Regression (2983/5000): loss=0.5575940005905979\n",
      "Logistic Regression (2984/5000): loss=0.5575220703160337\n",
      "Logistic Regression (2985/5000): loss=0.5576528278434993\n",
      "Logistic Regression (2986/5000): loss=0.5574729067707831\n",
      "Logistic Regression (2987/5000): loss=0.5759597801071152\n",
      "Logistic Regression (2988/5000): loss=0.5768249145552895\n",
      "Logistic Regression (2989/5000): loss=0.5699611062521309\n",
      "Logistic Regression (2990/5000): loss=0.5604910453011746\n",
      "Logistic Regression (2991/5000): loss=0.5589906411318114\n",
      "Logistic Regression (2992/5000): loss=0.558401179608998\n",
      "Logistic Regression (2993/5000): loss=0.5594898169986162\n",
      "Logistic Regression (2994/5000): loss=0.5624807737617328\n",
      "Logistic Regression (2995/5000): loss=0.562468249557353\n",
      "Logistic Regression (2996/5000): loss=0.5593703270055037\n",
      "Logistic Regression (2997/5000): loss=0.5844525833694033\n",
      "Logistic Regression (2998/5000): loss=0.5674007056159037\n",
      "Logistic Regression (2999/5000): loss=0.5592853051231751\n",
      "Logistic Regression (3000/5000): loss=0.5574579613978506\n",
      "Logistic Regression (3001/5000): loss=0.55947136211476\n",
      "Logistic Regression (3002/5000): loss=0.5577294842625145\n",
      "Logistic Regression (3003/5000): loss=0.5580893850499395\n",
      "Logistic Regression (3004/5000): loss=0.5586289591567485\n",
      "Logistic Regression (3005/5000): loss=0.5622196269297223\n",
      "Logistic Regression (3006/5000): loss=0.558061534486628\n",
      "Logistic Regression (3007/5000): loss=0.5614754451209572\n",
      "Logistic Regression (3008/5000): loss=0.5573093979645699\n",
      "Logistic Regression (3009/5000): loss=0.5613629165301164\n",
      "Logistic Regression (3010/5000): loss=0.5585795426635491\n",
      "Logistic Regression (3011/5000): loss=0.5622322306025406\n",
      "Logistic Regression (3012/5000): loss=0.5620125750808795\n",
      "Logistic Regression (3013/5000): loss=0.5577778347204263\n",
      "Logistic Regression (3014/5000): loss=0.5737537093825635\n",
      "Logistic Regression (3015/5000): loss=0.5660951908985343\n",
      "Logistic Regression (3016/5000): loss=0.5589678270186144\n",
      "Logistic Regression (3017/5000): loss=0.5583620780051252\n",
      "Logistic Regression (3018/5000): loss=0.5598147899702572\n",
      "Logistic Regression (3019/5000): loss=0.5621958409736084\n",
      "Logistic Regression (3020/5000): loss=0.5730239692365481\n",
      "Logistic Regression (3021/5000): loss=0.5586258166333852\n",
      "Logistic Regression (3022/5000): loss=0.5601331248820502\n",
      "Logistic Regression (3023/5000): loss=0.5616524555561143\n",
      "Logistic Regression (3024/5000): loss=0.5663715909882324\n",
      "Logistic Regression (3025/5000): loss=0.5622463529758263\n",
      "Logistic Regression (3026/5000): loss=0.5657693084056945\n",
      "Logistic Regression (3027/5000): loss=0.5589441750069566\n",
      "Logistic Regression (3028/5000): loss=0.5669886712272678\n",
      "Logistic Regression (3029/5000): loss=0.5633354668032473\n",
      "Logistic Regression (3030/5000): loss=0.5594462250587815\n",
      "Logistic Regression (3031/5000): loss=0.5599539198203005\n",
      "Logistic Regression (3032/5000): loss=0.5575399227493433\n",
      "Logistic Regression (3033/5000): loss=0.5578692669669384\n",
      "Logistic Regression (3034/5000): loss=0.5575293054603131\n",
      "Logistic Regression (3035/5000): loss=0.5590231284814247\n",
      "Logistic Regression (3036/5000): loss=0.5574820559291577\n",
      "Logistic Regression (3037/5000): loss=0.557466736121867\n",
      "Logistic Regression (3038/5000): loss=0.5657640510252526\n",
      "Logistic Regression (3039/5000): loss=0.5588020287903189\n",
      "Logistic Regression (3040/5000): loss=0.5581136988843485\n",
      "Logistic Regression (3041/5000): loss=0.5613279541764404\n",
      "Logistic Regression (3042/5000): loss=0.5605710788076347\n",
      "Logistic Regression (3043/5000): loss=0.5590688251661208\n",
      "Logistic Regression (3044/5000): loss=0.5573331008249904\n",
      "Logistic Regression (3045/5000): loss=0.5602057534189845\n",
      "Logistic Regression (3046/5000): loss=0.5573475904849344\n",
      "Logistic Regression (3047/5000): loss=0.5570639058454564\n",
      "Logistic Regression (3048/5000): loss=0.5597225695492578\n",
      "Logistic Regression (3049/5000): loss=0.5678322603231689\n",
      "Logistic Regression (3050/5000): loss=0.5614007558479285\n",
      "Logistic Regression (3051/5000): loss=0.5581746543438676\n",
      "Logistic Regression (3052/5000): loss=0.5591739540836367\n",
      "Logistic Regression (3053/5000): loss=0.5584167041193239\n",
      "Logistic Regression (3054/5000): loss=0.5663118746953474\n",
      "Logistic Regression (3055/5000): loss=0.5572965749055062\n",
      "Logistic Regression (3056/5000): loss=0.5584089979004857\n",
      "Logistic Regression (3057/5000): loss=0.5596605316500326\n",
      "Logistic Regression (3058/5000): loss=0.5571872732474006\n",
      "Logistic Regression (3059/5000): loss=0.5583108522177183\n",
      "Logistic Regression (3060/5000): loss=0.5572602987474452\n",
      "Logistic Regression (3061/5000): loss=0.5580266684215857\n",
      "Logistic Regression (3062/5000): loss=0.5571926053683928\n",
      "Logistic Regression (3063/5000): loss=0.5572153597737765\n",
      "Logistic Regression (3064/5000): loss=0.5593337656862468\n",
      "Logistic Regression (3065/5000): loss=0.564292117442856\n",
      "Logistic Regression (3066/5000): loss=0.5578344345131729\n",
      "Logistic Regression (3067/5000): loss=0.5596504999773745\n",
      "Logistic Regression (3068/5000): loss=0.5570395289476573\n",
      "Logistic Regression (3069/5000): loss=0.5617048487470119\n",
      "Logistic Regression (3070/5000): loss=0.566225068631498\n",
      "Logistic Regression (3071/5000): loss=0.5626992450403221\n",
      "Logistic Regression (3072/5000): loss=0.5610902089336733\n",
      "Logistic Regression (3073/5000): loss=0.5639164334562168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (3074/5000): loss=0.5585358895821185\n",
      "Logistic Regression (3075/5000): loss=0.5578261136958464\n",
      "Logistic Regression (3076/5000): loss=0.5656748388891287\n",
      "Logistic Regression (3077/5000): loss=0.557612224872038\n",
      "Logistic Regression (3078/5000): loss=0.5589895972137108\n",
      "Logistic Regression (3079/5000): loss=0.5573263850709193\n",
      "Logistic Regression (3080/5000): loss=0.5589981486565873\n",
      "Logistic Regression (3081/5000): loss=0.5574706652093795\n",
      "Logistic Regression (3082/5000): loss=0.5656419667992597\n",
      "Logistic Regression (3083/5000): loss=0.5608475362949265\n",
      "Logistic Regression (3084/5000): loss=0.5584430262307577\n",
      "Logistic Regression (3085/5000): loss=0.5609540967040334\n",
      "Logistic Regression (3086/5000): loss=0.5659294698373893\n",
      "Logistic Regression (3087/5000): loss=0.5661452334027471\n",
      "Logistic Regression (3088/5000): loss=0.5586240214104893\n",
      "Logistic Regression (3089/5000): loss=0.5596979035341973\n",
      "Logistic Regression (3090/5000): loss=0.5594432632329698\n",
      "Logistic Regression (3091/5000): loss=0.5706017200733261\n",
      "Logistic Regression (3092/5000): loss=0.5589633655188101\n",
      "Logistic Regression (3093/5000): loss=0.5573657674374134\n",
      "Logistic Regression (3094/5000): loss=0.5587094796101285\n",
      "Logistic Regression (3095/5000): loss=0.5573779568680564\n",
      "Logistic Regression (3096/5000): loss=0.5573774995037678\n",
      "Logistic Regression (3097/5000): loss=0.5611642531649998\n",
      "Logistic Regression (3098/5000): loss=0.5574984513852627\n",
      "Logistic Regression (3099/5000): loss=0.556945985173416\n",
      "Logistic Regression (3100/5000): loss=0.5618022083598285\n",
      "Logistic Regression (3101/5000): loss=0.560645684051046\n",
      "Logistic Regression (3102/5000): loss=0.5646738636006527\n",
      "Logistic Regression (3103/5000): loss=0.5702260186106354\n",
      "Logistic Regression (3104/5000): loss=0.5575371960073655\n",
      "Logistic Regression (3105/5000): loss=0.5588765357234015\n",
      "Logistic Regression (3106/5000): loss=0.5695968427016899\n",
      "Logistic Regression (3107/5000): loss=0.5713407950110188\n",
      "Logistic Regression (3108/5000): loss=0.5592443235799313\n",
      "Logistic Regression (3109/5000): loss=0.5596120861636653\n",
      "Logistic Regression (3110/5000): loss=0.5597580593472149\n",
      "Logistic Regression (3111/5000): loss=0.5571967940759693\n",
      "Logistic Regression (3112/5000): loss=0.5609935921691759\n",
      "Logistic Regression (3113/5000): loss=0.5753630449641468\n",
      "Logistic Regression (3114/5000): loss=0.5571560538485071\n",
      "Logistic Regression (3115/5000): loss=0.5586489491136307\n",
      "Logistic Regression (3116/5000): loss=0.5567450459683222\n",
      "Logistic Regression (3117/5000): loss=0.5601171805540328\n",
      "Logistic Regression (3118/5000): loss=0.5579996799476108\n",
      "Logistic Regression (3119/5000): loss=0.5590328573555131\n",
      "Logistic Regression (3120/5000): loss=0.5636922292810478\n",
      "Logistic Regression (3121/5000): loss=0.5579538587196355\n",
      "Logistic Regression (3122/5000): loss=0.556947720158409\n",
      "Logistic Regression (3123/5000): loss=0.5569810832282251\n",
      "Logistic Regression (3124/5000): loss=0.5581679303907414\n",
      "Logistic Regression (3125/5000): loss=0.5595920142844623\n",
      "Logistic Regression (3126/5000): loss=0.5604834755973718\n",
      "Logistic Regression (3127/5000): loss=0.5566897556749931\n",
      "Logistic Regression (3128/5000): loss=0.5650214380203996\n",
      "Logistic Regression (3129/5000): loss=0.5567216124540393\n",
      "Logistic Regression (3130/5000): loss=0.5566480015331913\n",
      "Logistic Regression (3131/5000): loss=0.5566952610206177\n",
      "Logistic Regression (3132/5000): loss=0.5566021478764289\n",
      "Logistic Regression (3133/5000): loss=0.5566358051922682\n",
      "Logistic Regression (3134/5000): loss=0.5566291752792717\n",
      "Logistic Regression (3135/5000): loss=0.5586183843442063\n",
      "Logistic Regression (3136/5000): loss=0.5716591743015886\n",
      "Logistic Regression (3137/5000): loss=0.5623128640160001\n",
      "Logistic Regression (3138/5000): loss=0.5567726486472077\n",
      "Logistic Regression (3139/5000): loss=0.5619480628717031\n",
      "Logistic Regression (3140/5000): loss=0.5565101941817377\n",
      "Logistic Regression (3141/5000): loss=0.5678429142624667\n",
      "Logistic Regression (3142/5000): loss=0.5610671608351783\n",
      "Logistic Regression (3143/5000): loss=0.5672235104876187\n",
      "Logistic Regression (3144/5000): loss=0.5567259051334013\n",
      "Logistic Regression (3145/5000): loss=0.5606968020334444\n",
      "Logistic Regression (3146/5000): loss=0.556470705403769\n",
      "Logistic Regression (3147/5000): loss=0.5588292256444385\n",
      "Logistic Regression (3148/5000): loss=0.5675710255332477\n",
      "Logistic Regression (3149/5000): loss=0.5619790680227523\n",
      "Logistic Regression (3150/5000): loss=0.566342777502315\n",
      "Logistic Regression (3151/5000): loss=0.5565769687405069\n",
      "Logistic Regression (3152/5000): loss=0.5595169294565366\n",
      "Logistic Regression (3153/5000): loss=0.5644346525606246\n",
      "Logistic Regression (3154/5000): loss=0.5569357732071865\n",
      "Logistic Regression (3155/5000): loss=0.5831850605164638\n",
      "Logistic Regression (3156/5000): loss=0.5564603416088476\n",
      "Logistic Regression (3157/5000): loss=0.558929744106931\n",
      "Logistic Regression (3158/5000): loss=0.5687545224019068\n",
      "Logistic Regression (3159/5000): loss=0.557334206120752\n",
      "Logistic Regression (3160/5000): loss=0.5572875715599002\n",
      "Logistic Regression (3161/5000): loss=0.5585452545006694\n",
      "Logistic Regression (3162/5000): loss=0.5851262913826497\n",
      "Logistic Regression (3163/5000): loss=0.5755737350162244\n",
      "Logistic Regression (3164/5000): loss=0.5692724626434301\n",
      "Logistic Regression (3165/5000): loss=0.5621037762895605\n",
      "Logistic Regression (3166/5000): loss=0.5604421840138684\n",
      "Logistic Regression (3167/5000): loss=0.5565461853363873\n",
      "Logistic Regression (3168/5000): loss=0.5632994186529489\n",
      "Logistic Regression (3169/5000): loss=0.5576265359904159\n",
      "Logistic Regression (3170/5000): loss=0.5690296187380717\n",
      "Logistic Regression (3171/5000): loss=0.5705652920881764\n",
      "Logistic Regression (3172/5000): loss=0.5915236944679747\n",
      "Logistic Regression (3173/5000): loss=0.5564977287428227\n",
      "Logistic Regression (3174/5000): loss=0.5578317587407005\n",
      "Logistic Regression (3175/5000): loss=0.5618983433678753\n",
      "Logistic Regression (3176/5000): loss=0.5580694847269216\n",
      "Logistic Regression (3177/5000): loss=0.5574696259473337\n",
      "Logistic Regression (3178/5000): loss=0.5683809218002089\n",
      "Logistic Regression (3179/5000): loss=0.5567484982268632\n",
      "Logistic Regression (3180/5000): loss=0.5564720451840586\n",
      "Logistic Regression (3181/5000): loss=0.5568379472418963\n",
      "Logistic Regression (3182/5000): loss=0.5563902340034524\n",
      "Logistic Regression (3183/5000): loss=0.5608805745325366\n",
      "Logistic Regression (3184/5000): loss=0.5563255260401627\n",
      "Logistic Regression (3185/5000): loss=0.5711564979413659\n",
      "Logistic Regression (3186/5000): loss=0.556549534690071\n",
      "Logistic Regression (3187/5000): loss=0.5619466260859246\n",
      "Logistic Regression (3188/5000): loss=0.5563127411464442\n",
      "Logistic Regression (3189/5000): loss=0.5606973089400173\n",
      "Logistic Regression (3190/5000): loss=0.5576536527197694\n",
      "Logistic Regression (3191/5000): loss=0.5658435982624775\n",
      "Logistic Regression (3192/5000): loss=0.5573521740221338\n",
      "Logistic Regression (3193/5000): loss=0.5563704208359251\n",
      "Logistic Regression (3194/5000): loss=0.561629820568907\n",
      "Logistic Regression (3195/5000): loss=0.5562414059463456\n",
      "Logistic Regression (3196/5000): loss=0.5564058026105568\n",
      "Logistic Regression (3197/5000): loss=0.558809377364061\n",
      "Logistic Regression (3198/5000): loss=0.5591153633604089\n",
      "Logistic Regression (3199/5000): loss=0.557356463307445\n",
      "Logistic Regression (3200/5000): loss=0.5561672692962865\n",
      "Logistic Regression (3201/5000): loss=0.5572654761178591\n",
      "Logistic Regression (3202/5000): loss=0.5702684095638262\n",
      "Logistic Regression (3203/5000): loss=0.5578933442454815\n",
      "Logistic Regression (3204/5000): loss=0.5564601106003487\n",
      "Logistic Regression (3205/5000): loss=0.5578388663872169\n",
      "Logistic Regression (3206/5000): loss=0.5562355864047793\n",
      "Logistic Regression (3207/5000): loss=0.556234959299245\n",
      "Logistic Regression (3208/5000): loss=0.5607194653854425\n",
      "Logistic Regression (3209/5000): loss=0.5724046681702497\n",
      "Logistic Regression (3210/5000): loss=0.5563874357013189\n",
      "Logistic Regression (3211/5000): loss=0.5592942822463456\n",
      "Logistic Regression (3212/5000): loss=0.5578927090616417\n",
      "Logistic Regression (3213/5000): loss=0.5574396637382765\n",
      "Logistic Regression (3214/5000): loss=0.5564292607674799\n",
      "Logistic Regression (3215/5000): loss=0.5574410409670928\n",
      "Logistic Regression (3216/5000): loss=0.5622333737543345\n",
      "Logistic Regression (3217/5000): loss=0.5561173710174749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (3218/5000): loss=0.5727954110745271\n",
      "Logistic Regression (3219/5000): loss=0.5761769793198456\n",
      "Logistic Regression (3220/5000): loss=0.5560720141115234\n",
      "Logistic Regression (3221/5000): loss=0.5574980943402219\n",
      "Logistic Regression (3222/5000): loss=0.5561714747145454\n",
      "Logistic Regression (3223/5000): loss=0.5560010722166076\n",
      "Logistic Regression (3224/5000): loss=0.556516799766895\n",
      "Logistic Regression (3225/5000): loss=0.556793840010744\n",
      "Logistic Regression (3226/5000): loss=0.5561489310579713\n",
      "Logistic Regression (3227/5000): loss=0.5567043630315844\n",
      "Logistic Regression (3228/5000): loss=0.5758581324617154\n",
      "Logistic Regression (3229/5000): loss=0.5599770854405969\n",
      "Logistic Regression (3230/5000): loss=0.5655444424343554\n",
      "Logistic Regression (3231/5000): loss=0.5669519921861887\n",
      "Logistic Regression (3232/5000): loss=0.5564076802511481\n",
      "Logistic Regression (3233/5000): loss=0.5640592525631374\n",
      "Logistic Regression (3234/5000): loss=0.5678555307400474\n",
      "Logistic Regression (3235/5000): loss=0.5707115979327165\n",
      "Logistic Regression (3236/5000): loss=0.562326101855794\n",
      "Logistic Regression (3237/5000): loss=0.5626303658735257\n",
      "Logistic Regression (3238/5000): loss=0.5568147625897771\n",
      "Logistic Regression (3239/5000): loss=0.5618058726319421\n",
      "Logistic Regression (3240/5000): loss=0.5568262142182967\n",
      "Logistic Regression (3241/5000): loss=0.5619614153302259\n",
      "Logistic Regression (3242/5000): loss=0.5571392915068101\n",
      "Logistic Regression (3243/5000): loss=0.5597900227570327\n",
      "Logistic Regression (3244/5000): loss=0.5609331616917426\n",
      "Logistic Regression (3245/5000): loss=0.5622877421286292\n",
      "Logistic Regression (3246/5000): loss=0.5619725866681394\n",
      "Logistic Regression (3247/5000): loss=0.556272148570248\n",
      "Logistic Regression (3248/5000): loss=0.5589598282341378\n",
      "Logistic Regression (3249/5000): loss=0.5566904214232269\n",
      "Logistic Regression (3250/5000): loss=0.5579253578946523\n",
      "Logistic Regression (3251/5000): loss=0.5607036039656835\n",
      "Logistic Regression (3252/5000): loss=0.560841180443882\n",
      "Logistic Regression (3253/5000): loss=0.5569916312864366\n",
      "Logistic Regression (3254/5000): loss=0.5559163401393754\n",
      "Logistic Regression (3255/5000): loss=0.5572137419643949\n",
      "Logistic Regression (3256/5000): loss=0.557209509557473\n",
      "Logistic Regression (3257/5000): loss=0.5579786042199982\n",
      "Logistic Regression (3258/5000): loss=0.5644943111271093\n",
      "Logistic Regression (3259/5000): loss=0.5560897752558454\n",
      "Logistic Regression (3260/5000): loss=0.5558423782644605\n",
      "Logistic Regression (3261/5000): loss=0.5560392052409999\n",
      "Logistic Regression (3262/5000): loss=0.5558727501802951\n",
      "Logistic Regression (3263/5000): loss=0.5640178638632157\n",
      "Logistic Regression (3264/5000): loss=0.5570430254936634\n",
      "Logistic Regression (3265/5000): loss=0.5675555545590871\n",
      "Logistic Regression (3266/5000): loss=0.5694490413031296\n",
      "Logistic Regression (3267/5000): loss=0.5559716429361783\n",
      "Logistic Regression (3268/5000): loss=0.5559113777555755\n",
      "Logistic Regression (3269/5000): loss=0.5621332941805901\n",
      "Logistic Regression (3270/5000): loss=0.5559556416259206\n",
      "Logistic Regression (3271/5000): loss=0.5560581202278629\n",
      "Logistic Regression (3272/5000): loss=0.5567255341576165\n",
      "Logistic Regression (3273/5000): loss=0.5678254388382231\n",
      "Logistic Regression (3274/5000): loss=0.5685504911573819\n",
      "Logistic Regression (3275/5000): loss=0.5593425280216964\n",
      "Logistic Regression (3276/5000): loss=0.5565131929721736\n",
      "Logistic Regression (3277/5000): loss=0.5565905507241534\n",
      "Logistic Regression (3278/5000): loss=0.5561969564403029\n",
      "Logistic Regression (3279/5000): loss=0.5614170953265234\n",
      "Logistic Regression (3280/5000): loss=0.5656782023720386\n",
      "Logistic Regression (3281/5000): loss=0.5557779141666107\n",
      "Logistic Regression (3282/5000): loss=0.5557014568539044\n",
      "Logistic Regression (3283/5000): loss=0.5718648425058398\n",
      "Logistic Regression (3284/5000): loss=0.5562406541469556\n",
      "Logistic Regression (3285/5000): loss=0.5573966737097217\n",
      "Logistic Regression (3286/5000): loss=0.5565783758276721\n",
      "Logistic Regression (3287/5000): loss=0.5773207583533558\n",
      "Logistic Regression (3288/5000): loss=0.5559258650402716\n",
      "Logistic Regression (3289/5000): loss=0.5563156429599125\n",
      "Logistic Regression (3290/5000): loss=0.5589270595907537\n",
      "Logistic Regression (3291/5000): loss=0.5595423753883618\n",
      "Logistic Regression (3292/5000): loss=0.5557874102049015\n",
      "Logistic Regression (3293/5000): loss=0.5581716787100586\n",
      "Logistic Regression (3294/5000): loss=0.5556487511041382\n",
      "Logistic Regression (3295/5000): loss=0.562348482706276\n",
      "Logistic Regression (3296/5000): loss=0.5558152034059115\n",
      "Logistic Regression (3297/5000): loss=0.5745870875174102\n",
      "Logistic Regression (3298/5000): loss=0.5571008600614688\n",
      "Logistic Regression (3299/5000): loss=0.5561851586388978\n",
      "Logistic Regression (3300/5000): loss=0.5557638730673954\n",
      "Logistic Regression (3301/5000): loss=0.5557947399098371\n",
      "Logistic Regression (3302/5000): loss=0.556666798443934\n",
      "Logistic Regression (3303/5000): loss=0.5556788956779474\n",
      "Logistic Regression (3304/5000): loss=0.5632227471505876\n",
      "Logistic Regression (3305/5000): loss=0.5596780867069034\n",
      "Logistic Regression (3306/5000): loss=0.5568462062745902\n",
      "Logistic Regression (3307/5000): loss=0.5592503568544982\n",
      "Logistic Regression (3308/5000): loss=0.5578276038058766\n",
      "Logistic Regression (3309/5000): loss=0.5580086705217296\n",
      "Logistic Regression (3310/5000): loss=0.5562383157515159\n",
      "Logistic Regression (3311/5000): loss=0.5672748732979047\n",
      "Logistic Regression (3312/5000): loss=0.5607586859747388\n",
      "Logistic Regression (3313/5000): loss=0.5610752809540238\n",
      "Logistic Regression (3314/5000): loss=0.5697759599005179\n",
      "Logistic Regression (3315/5000): loss=0.5558373734692749\n",
      "Logistic Regression (3316/5000): loss=0.5562365624428677\n",
      "Logistic Regression (3317/5000): loss=0.560150551149902\n",
      "Logistic Regression (3318/5000): loss=0.555693526327107\n",
      "Logistic Regression (3319/5000): loss=0.5571287686438255\n",
      "Logistic Regression (3320/5000): loss=0.5581813600373566\n",
      "Logistic Regression (3321/5000): loss=0.5586356418981532\n",
      "Logistic Regression (3322/5000): loss=0.5580510828653139\n",
      "Logistic Regression (3323/5000): loss=0.5705851709118898\n",
      "Logistic Regression (3324/5000): loss=0.5562701657557606\n",
      "Logistic Regression (3325/5000): loss=0.5590810930204836\n",
      "Logistic Regression (3326/5000): loss=0.5824343493115774\n",
      "Logistic Regression (3327/5000): loss=0.557466737654625\n",
      "Logistic Regression (3328/5000): loss=0.5583686021712655\n",
      "Logistic Regression (3329/5000): loss=0.5578574581174812\n",
      "Logistic Regression (3330/5000): loss=0.5564019027690834\n",
      "Logistic Regression (3331/5000): loss=0.5658112762635477\n",
      "Logistic Regression (3332/5000): loss=0.557006412697338\n",
      "Logistic Regression (3333/5000): loss=0.5585521667484414\n",
      "Logistic Regression (3334/5000): loss=0.556386280599325\n",
      "Logistic Regression (3335/5000): loss=0.5601240329454154\n",
      "Logistic Regression (3336/5000): loss=0.5560455769344911\n",
      "Logistic Regression (3337/5000): loss=0.5662527578140278\n",
      "Logistic Regression (3338/5000): loss=0.5565437024772567\n",
      "Logistic Regression (3339/5000): loss=0.5559555125326533\n",
      "Logistic Regression (3340/5000): loss=0.5690512838646948\n",
      "Logistic Regression (3341/5000): loss=0.5555356435508011\n",
      "Logistic Regression (3342/5000): loss=0.5636237472888014\n",
      "Logistic Regression (3343/5000): loss=0.5562480270612629\n",
      "Logistic Regression (3344/5000): loss=0.5556069755280113\n",
      "Logistic Regression (3345/5000): loss=0.5557172796187168\n",
      "Logistic Regression (3346/5000): loss=0.5560589434653025\n",
      "Logistic Regression (3347/5000): loss=0.5556878879177581\n",
      "Logistic Regression (3348/5000): loss=0.5563099503023444\n",
      "Logistic Regression (3349/5000): loss=0.5608192080028966\n",
      "Logistic Regression (3350/5000): loss=0.5626165295326786\n",
      "Logistic Regression (3351/5000): loss=0.5564436506401582\n",
      "Logistic Regression (3352/5000): loss=0.5558285918908341\n",
      "Logistic Regression (3353/5000): loss=0.5579829102614848\n",
      "Logistic Regression (3354/5000): loss=0.5774707648448537\n",
      "Logistic Regression (3355/5000): loss=0.5554402027497342\n",
      "Logistic Regression (3356/5000): loss=0.559576542516617\n",
      "Logistic Regression (3357/5000): loss=0.5561038345242925\n",
      "Logistic Regression (3358/5000): loss=0.5555275180158571\n",
      "Logistic Regression (3359/5000): loss=0.5552982785849694\n",
      "Logistic Regression (3360/5000): loss=0.5570028586478312\n",
      "Logistic Regression (3361/5000): loss=0.5587620180415109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (3362/5000): loss=0.5617102494345757\n",
      "Logistic Regression (3363/5000): loss=0.5592785436459097\n",
      "Logistic Regression (3364/5000): loss=0.5594608821693237\n",
      "Logistic Regression (3365/5000): loss=0.5559101466552856\n",
      "Logistic Regression (3366/5000): loss=0.5563273115804124\n",
      "Logistic Regression (3367/5000): loss=0.5636439168826669\n",
      "Logistic Regression (3368/5000): loss=0.5552504740579677\n",
      "Logistic Regression (3369/5000): loss=0.560699863050348\n",
      "Logistic Regression (3370/5000): loss=0.557458635036781\n",
      "Logistic Regression (3371/5000): loss=0.5552597011177589\n",
      "Logistic Regression (3372/5000): loss=0.5567942519380402\n",
      "Logistic Regression (3373/5000): loss=0.5620423646444227\n",
      "Logistic Regression (3374/5000): loss=0.5553588204141484\n",
      "Logistic Regression (3375/5000): loss=0.5555502266013664\n",
      "Logistic Regression (3376/5000): loss=0.5554481719336087\n",
      "Logistic Regression (3377/5000): loss=0.5574603146059502\n",
      "Logistic Regression (3378/5000): loss=0.5581641467751931\n",
      "Logistic Regression (3379/5000): loss=0.5563247787875515\n",
      "Logistic Regression (3380/5000): loss=0.5605908727422932\n",
      "Logistic Regression (3381/5000): loss=0.5559408753899536\n",
      "Logistic Regression (3382/5000): loss=0.5555400294595\n",
      "Logistic Regression (3383/5000): loss=0.5580005449385074\n",
      "Logistic Regression (3384/5000): loss=0.5574223962469753\n",
      "Logistic Regression (3385/5000): loss=0.5568016360673681\n",
      "Logistic Regression (3386/5000): loss=0.5552332432625766\n",
      "Logistic Regression (3387/5000): loss=0.5550771462937529\n",
      "Logistic Regression (3388/5000): loss=0.5600624318366625\n",
      "Logistic Regression (3389/5000): loss=0.5657929491246413\n",
      "Logistic Regression (3390/5000): loss=0.5552765001480647\n",
      "Logistic Regression (3391/5000): loss=0.5566215384956029\n",
      "Logistic Regression (3392/5000): loss=0.559124450024765\n",
      "Logistic Regression (3393/5000): loss=0.5551549336235115\n",
      "Logistic Regression (3394/5000): loss=0.5556693418304578\n",
      "Logistic Regression (3395/5000): loss=0.5550846659576312\n",
      "Logistic Regression (3396/5000): loss=0.5776519111258568\n",
      "Logistic Regression (3397/5000): loss=0.612754418911482\n",
      "Logistic Regression (3398/5000): loss=0.5636328378088588\n",
      "Logistic Regression (3399/5000): loss=0.5607251843941174\n",
      "Logistic Regression (3400/5000): loss=0.5554635520540955\n",
      "Logistic Regression (3401/5000): loss=0.5558680756005604\n",
      "Logistic Regression (3402/5000): loss=0.5618561471273832\n",
      "Logistic Regression (3403/5000): loss=0.5569447045495987\n",
      "Logistic Regression (3404/5000): loss=0.555493815308369\n",
      "Logistic Regression (3405/5000): loss=0.5662072726829737\n",
      "Logistic Regression (3406/5000): loss=0.5570453074304494\n",
      "Logistic Regression (3407/5000): loss=0.566224886635481\n",
      "Logistic Regression (3408/5000): loss=0.5554870382860364\n",
      "Logistic Regression (3409/5000): loss=0.5702465296205784\n",
      "Logistic Regression (3410/5000): loss=0.5552684525088012\n",
      "Logistic Regression (3411/5000): loss=0.5601927004089142\n",
      "Logistic Regression (3412/5000): loss=0.5551955073048611\n",
      "Logistic Regression (3413/5000): loss=0.56701630092278\n",
      "Logistic Regression (3414/5000): loss=0.5550063633900072\n",
      "Logistic Regression (3415/5000): loss=0.5661979450796136\n",
      "Logistic Regression (3416/5000): loss=0.5709393779668124\n",
      "Logistic Regression (3417/5000): loss=0.5551027481088098\n",
      "Logistic Regression (3418/5000): loss=0.5562693949621914\n",
      "Logistic Regression (3419/5000): loss=0.5593672346631745\n",
      "Logistic Regression (3420/5000): loss=0.5623461029536899\n",
      "Logistic Regression (3421/5000): loss=0.5549781607159279\n",
      "Logistic Regression (3422/5000): loss=0.5582436979253976\n",
      "Logistic Regression (3423/5000): loss=0.5621991552024093\n",
      "Logistic Regression (3424/5000): loss=0.5551709853908859\n",
      "Logistic Regression (3425/5000): loss=0.5569155473180886\n",
      "Logistic Regression (3426/5000): loss=0.5604889454242504\n",
      "Logistic Regression (3427/5000): loss=0.5555837778975039\n",
      "Logistic Regression (3428/5000): loss=0.5557500424630293\n",
      "Logistic Regression (3429/5000): loss=0.5564418506264234\n",
      "Logistic Regression (3430/5000): loss=0.5549580675267568\n",
      "Logistic Regression (3431/5000): loss=0.5550479166249884\n",
      "Logistic Regression (3432/5000): loss=0.559513820831137\n",
      "Logistic Regression (3433/5000): loss=0.5556133266049135\n",
      "Logistic Regression (3434/5000): loss=0.5548259628622928\n",
      "Logistic Regression (3435/5000): loss=0.5559615036615705\n",
      "Logistic Regression (3436/5000): loss=0.556440466224769\n",
      "Logistic Regression (3437/5000): loss=0.5548827012050387\n",
      "Logistic Regression (3438/5000): loss=0.557642609546404\n",
      "Logistic Regression (3439/5000): loss=0.5558896525182787\n",
      "Logistic Regression (3440/5000): loss=0.5548195311625591\n",
      "Logistic Regression (3441/5000): loss=0.5590363038525101\n",
      "Logistic Regression (3442/5000): loss=0.556372586469271\n",
      "Logistic Regression (3443/5000): loss=0.5574389073994884\n",
      "Logistic Regression (3444/5000): loss=0.5547794566833097\n",
      "Logistic Regression (3445/5000): loss=0.5662413503140501\n",
      "Logistic Regression (3446/5000): loss=0.5598811052803877\n",
      "Logistic Regression (3447/5000): loss=0.5553973723829982\n",
      "Logistic Regression (3448/5000): loss=0.5589852161347623\n",
      "Logistic Regression (3449/5000): loss=0.5556288900608491\n",
      "Logistic Regression (3450/5000): loss=0.5547928569032234\n",
      "Logistic Regression (3451/5000): loss=0.5671710861998573\n",
      "Logistic Regression (3452/5000): loss=0.5548121311350157\n",
      "Logistic Regression (3453/5000): loss=0.5553121235207\n",
      "Logistic Regression (3454/5000): loss=0.55470344699193\n",
      "Logistic Regression (3455/5000): loss=0.5555054591405485\n",
      "Logistic Regression (3456/5000): loss=0.5731239787586324\n",
      "Logistic Regression (3457/5000): loss=0.5596012596883247\n",
      "Logistic Regression (3458/5000): loss=0.554692179338923\n",
      "Logistic Regression (3459/5000): loss=0.5548893752007505\n",
      "Logistic Regression (3460/5000): loss=0.5568286913225865\n",
      "Logistic Regression (3461/5000): loss=0.5632997757633188\n",
      "Logistic Regression (3462/5000): loss=0.555407567606879\n",
      "Logistic Regression (3463/5000): loss=0.5572640876023457\n",
      "Logistic Regression (3464/5000): loss=0.5557479963778373\n",
      "Logistic Regression (3465/5000): loss=0.5547194934691956\n",
      "Logistic Regression (3466/5000): loss=0.5550679818147576\n",
      "Logistic Regression (3467/5000): loss=0.5560533267832378\n",
      "Logistic Regression (3468/5000): loss=0.5551257466946076\n",
      "Logistic Regression (3469/5000): loss=0.5598893638115803\n",
      "Logistic Regression (3470/5000): loss=0.5557446100087273\n",
      "Logistic Regression (3471/5000): loss=0.5552055596936234\n",
      "Logistic Regression (3472/5000): loss=0.5569100573996741\n",
      "Logistic Regression (3473/5000): loss=0.5553768967835252\n",
      "Logistic Regression (3474/5000): loss=0.5627274552468062\n",
      "Logistic Regression (3475/5000): loss=0.5557068303599935\n",
      "Logistic Regression (3476/5000): loss=0.5577602162849449\n",
      "Logistic Regression (3477/5000): loss=0.5564713674014897\n",
      "Logistic Regression (3478/5000): loss=0.5689859056316452\n",
      "Logistic Regression (3479/5000): loss=0.5559409677346804\n",
      "Logistic Regression (3480/5000): loss=0.5557590614902215\n",
      "Logistic Regression (3481/5000): loss=0.5560937762133386\n",
      "Logistic Regression (3482/5000): loss=0.5592669513677448\n",
      "Logistic Regression (3483/5000): loss=0.5602045114177469\n",
      "Logistic Regression (3484/5000): loss=0.5552628708442723\n",
      "Logistic Regression (3485/5000): loss=0.554927503777001\n",
      "Logistic Regression (3486/5000): loss=0.5559691750662024\n",
      "Logistic Regression (3487/5000): loss=0.5565628682560934\n",
      "Logistic Regression (3488/5000): loss=0.5566638515117198\n",
      "Logistic Regression (3489/5000): loss=0.5607208467132552\n",
      "Logistic Regression (3490/5000): loss=0.5554367759390672\n",
      "Logistic Regression (3491/5000): loss=0.556971730580829\n",
      "Logistic Regression (3492/5000): loss=0.5547914850757875\n",
      "Logistic Regression (3493/5000): loss=0.5550181971432989\n",
      "Logistic Regression (3494/5000): loss=0.5569256813905644\n",
      "Logistic Regression (3495/5000): loss=0.554723452871621\n",
      "Logistic Regression (3496/5000): loss=0.5547001959404974\n",
      "Logistic Regression (3497/5000): loss=0.5552730691756551\n",
      "Logistic Regression (3498/5000): loss=0.5573137685172664\n",
      "Logistic Regression (3499/5000): loss=0.5602825821222651\n",
      "Logistic Regression (3500/5000): loss=0.557294667744019\n",
      "Logistic Regression (3501/5000): loss=0.5548272264043583\n",
      "Logistic Regression (3502/5000): loss=0.565392093090752\n",
      "Logistic Regression (3503/5000): loss=0.5591055229932398\n",
      "Logistic Regression (3504/5000): loss=0.555271869156543\n",
      "Logistic Regression (3505/5000): loss=0.5548011262625689\n",
      "Logistic Regression (3506/5000): loss=0.5592206145576214\n",
      "Logistic Regression (3507/5000): loss=0.5664910826256242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (3508/5000): loss=0.5547980869239132\n",
      "Logistic Regression (3509/5000): loss=0.5579276732844962\n",
      "Logistic Regression (3510/5000): loss=0.5545148903922452\n",
      "Logistic Regression (3511/5000): loss=0.5666003360022275\n",
      "Logistic Regression (3512/5000): loss=0.5590703568406499\n",
      "Logistic Regression (3513/5000): loss=0.568473797326135\n",
      "Logistic Regression (3514/5000): loss=0.557435111271635\n",
      "Logistic Regression (3515/5000): loss=0.5545375758775267\n",
      "Logistic Regression (3516/5000): loss=0.5655215447281307\n",
      "Logistic Regression (3517/5000): loss=0.5692851673717002\n",
      "Logistic Regression (3518/5000): loss=0.5593028639683661\n",
      "Logistic Regression (3519/5000): loss=0.5544249020429695\n",
      "Logistic Regression (3520/5000): loss=0.5547426084215948\n",
      "Logistic Regression (3521/5000): loss=0.5597302062485938\n",
      "Logistic Regression (3522/5000): loss=0.5547525825373089\n",
      "Logistic Regression (3523/5000): loss=0.5577176157035909\n",
      "Logistic Regression (3524/5000): loss=0.554441159403478\n",
      "Logistic Regression (3525/5000): loss=0.5556638549800264\n",
      "Logistic Regression (3526/5000): loss=0.5694625779973731\n",
      "Logistic Regression (3527/5000): loss=0.587939197935964\n",
      "Logistic Regression (3528/5000): loss=0.5548568387841694\n",
      "Logistic Regression (3529/5000): loss=0.5577015485966494\n",
      "Logistic Regression (3530/5000): loss=0.5544618026946726\n",
      "Logistic Regression (3531/5000): loss=0.5568451487861211\n",
      "Logistic Regression (3532/5000): loss=0.5584362084938126\n",
      "Logistic Regression (3533/5000): loss=0.5568087449117746\n",
      "Logistic Regression (3534/5000): loss=0.5543979877071394\n",
      "Logistic Regression (3535/5000): loss=0.5547651746569923\n",
      "Logistic Regression (3536/5000): loss=0.5582355474895772\n",
      "Logistic Regression (3537/5000): loss=0.5547375013004454\n",
      "Logistic Regression (3538/5000): loss=0.5549739594258123\n",
      "Logistic Regression (3539/5000): loss=0.5564969551318372\n",
      "Logistic Regression (3540/5000): loss=0.5618805055506504\n",
      "Logistic Regression (3541/5000): loss=0.5565815489263816\n",
      "Logistic Regression (3542/5000): loss=0.5625492862904762\n",
      "Logistic Regression (3543/5000): loss=0.5565005506090118\n",
      "Logistic Regression (3544/5000): loss=0.5567956969028222\n",
      "Logistic Regression (3545/5000): loss=0.5654677753425362\n",
      "Logistic Regression (3546/5000): loss=0.5551633053550297\n",
      "Logistic Regression (3547/5000): loss=0.5622693736365603\n",
      "Logistic Regression (3548/5000): loss=0.5550789643032851\n",
      "Logistic Regression (3549/5000): loss=0.5596162468279278\n",
      "Logistic Regression (3550/5000): loss=0.5603077914901221\n",
      "Logistic Regression (3551/5000): loss=0.5675439813574051\n",
      "Logistic Regression (3552/5000): loss=0.5557583826758951\n",
      "Logistic Regression (3553/5000): loss=0.5543482479930162\n",
      "Logistic Regression (3554/5000): loss=0.5554600638116692\n",
      "Logistic Regression (3555/5000): loss=0.5579873707251192\n",
      "Logistic Regression (3556/5000): loss=0.5601682977399818\n",
      "Logistic Regression (3557/5000): loss=0.5541998435165107\n",
      "Logistic Regression (3558/5000): loss=0.5543344648002033\n",
      "Logistic Regression (3559/5000): loss=0.5746335299129535\n",
      "Logistic Regression (3560/5000): loss=0.5554940027169506\n",
      "Logistic Regression (3561/5000): loss=0.5574387038461178\n",
      "Logistic Regression (3562/5000): loss=0.5595098716290013\n",
      "Logistic Regression (3563/5000): loss=0.555029838128022\n",
      "Logistic Regression (3564/5000): loss=0.5561527911845257\n",
      "Logistic Regression (3565/5000): loss=0.5782743374228358\n",
      "Logistic Regression (3566/5000): loss=0.5736932005387096\n",
      "Logistic Regression (3567/5000): loss=0.5601774350133777\n",
      "Logistic Regression (3568/5000): loss=0.5548969131754765\n",
      "Logistic Regression (3569/5000): loss=0.5551575782591576\n",
      "Logistic Regression (3570/5000): loss=0.5571047399615491\n",
      "Logistic Regression (3571/5000): loss=0.556829754094142\n",
      "Logistic Regression (3572/5000): loss=0.5555554206206764\n",
      "Logistic Regression (3573/5000): loss=0.5546895024353087\n",
      "Logistic Regression (3574/5000): loss=0.5595445606179481\n",
      "Logistic Regression (3575/5000): loss=0.5648725259415648\n",
      "Logistic Regression (3576/5000): loss=0.5556135332750467\n",
      "Logistic Regression (3577/5000): loss=0.5588930796101986\n",
      "Logistic Regression (3578/5000): loss=0.5590853881750897\n",
      "Logistic Regression (3579/5000): loss=0.5584225150286897\n",
      "Logistic Regression (3580/5000): loss=0.5696952522262969\n",
      "Logistic Regression (3581/5000): loss=0.5667524183387286\n",
      "Logistic Regression (3582/5000): loss=0.5547833919715157\n",
      "Logistic Regression (3583/5000): loss=0.5572222584854228\n",
      "Logistic Regression (3584/5000): loss=0.5649410391551427\n",
      "Logistic Regression (3585/5000): loss=0.5627256227614124\n",
      "Logistic Regression (3586/5000): loss=0.5545076417327686\n",
      "Logistic Regression (3587/5000): loss=0.5541188077988038\n",
      "Logistic Regression (3588/5000): loss=0.559466428787427\n",
      "Logistic Regression (3589/5000): loss=0.5545405811423482\n",
      "Logistic Regression (3590/5000): loss=0.5542239552764031\n",
      "Logistic Regression (3591/5000): loss=0.5544817770344463\n",
      "Logistic Regression (3592/5000): loss=0.5549800048021742\n",
      "Logistic Regression (3593/5000): loss=0.5547457301632733\n",
      "Logistic Regression (3594/5000): loss=0.5551039526721478\n",
      "Logistic Regression (3595/5000): loss=0.5562121806150275\n",
      "Logistic Regression (3596/5000): loss=0.5578465041645101\n",
      "Logistic Regression (3597/5000): loss=0.5578151691725652\n",
      "Logistic Regression (3598/5000): loss=0.5602134633530099\n",
      "Logistic Regression (3599/5000): loss=0.5603453992403378\n",
      "Logistic Regression (3600/5000): loss=0.561669151459642\n",
      "Logistic Regression (3601/5000): loss=0.5547006104945427\n",
      "Logistic Regression (3602/5000): loss=0.554804358097163\n",
      "Logistic Regression (3603/5000): loss=0.5611372361020822\n",
      "Logistic Regression (3604/5000): loss=0.5578634468088298\n",
      "Logistic Regression (3605/5000): loss=0.5616308489407126\n",
      "Logistic Regression (3606/5000): loss=0.5561656734614265\n",
      "Logistic Regression (3607/5000): loss=0.5620320072920546\n",
      "Logistic Regression (3608/5000): loss=0.5588462567416688\n",
      "Logistic Regression (3609/5000): loss=0.5550900486166372\n",
      "Logistic Regression (3610/5000): loss=0.5550905849364329\n",
      "Logistic Regression (3611/5000): loss=0.5553641471631313\n",
      "Logistic Regression (3612/5000): loss=0.5554743146420841\n",
      "Logistic Regression (3613/5000): loss=0.5548758519386848\n",
      "Logistic Regression (3614/5000): loss=0.5558353092246596\n",
      "Logistic Regression (3615/5000): loss=0.5809911783656034\n",
      "Logistic Regression (3616/5000): loss=0.5615302341872869\n",
      "Logistic Regression (3617/5000): loss=0.5566662277121298\n",
      "Logistic Regression (3618/5000): loss=0.5557589411638575\n",
      "Logistic Regression (3619/5000): loss=0.5562230247824101\n",
      "Logistic Regression (3620/5000): loss=0.5634624902950033\n",
      "Logistic Regression (3621/5000): loss=0.5642683630953135\n",
      "Logistic Regression (3622/5000): loss=0.5583002001270787\n",
      "Logistic Regression (3623/5000): loss=0.5689017672323687\n",
      "Logistic Regression (3624/5000): loss=0.555089102150031\n",
      "Logistic Regression (3625/5000): loss=0.5651149065985285\n",
      "Logistic Regression (3626/5000): loss=0.5541017055560866\n",
      "Logistic Regression (3627/5000): loss=0.5877938609100204\n",
      "Logistic Regression (3628/5000): loss=0.5548576109767417\n",
      "Logistic Regression (3629/5000): loss=0.5549859467122669\n",
      "Logistic Regression (3630/5000): loss=0.5577664626476065\n",
      "Logistic Regression (3631/5000): loss=0.5542109997579865\n",
      "Logistic Regression (3632/5000): loss=0.55724134306568\n",
      "Logistic Regression (3633/5000): loss=0.5540473698552082\n",
      "Logistic Regression (3634/5000): loss=0.5652809079712035\n",
      "Logistic Regression (3635/5000): loss=0.5563502338476252\n",
      "Logistic Regression (3636/5000): loss=0.5581063075414255\n",
      "Logistic Regression (3637/5000): loss=0.5657053911980273\n",
      "Logistic Regression (3638/5000): loss=0.5639051150697036\n",
      "Logistic Regression (3639/5000): loss=0.5549623416883175\n",
      "Logistic Regression (3640/5000): loss=0.5546301355079316\n",
      "Logistic Regression (3641/5000): loss=0.5545092153290002\n",
      "Logistic Regression (3642/5000): loss=0.5574325030996716\n",
      "Logistic Regression (3643/5000): loss=0.5549403562307815\n",
      "Logistic Regression (3644/5000): loss=0.5546813955840411\n",
      "Logistic Regression (3645/5000): loss=0.554255254299507\n",
      "Logistic Regression (3646/5000): loss=0.5537879943001824\n",
      "Logistic Regression (3647/5000): loss=0.5621821014934967\n",
      "Logistic Regression (3648/5000): loss=0.5553426172928432\n",
      "Logistic Regression (3649/5000): loss=0.5606968049160672\n",
      "Logistic Regression (3650/5000): loss=0.5560272162965487\n",
      "Logistic Regression (3651/5000): loss=0.5599662402758061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (3652/5000): loss=0.5540075029721794\n",
      "Logistic Regression (3653/5000): loss=0.563802740382887\n",
      "Logistic Regression (3654/5000): loss=0.5566568421458546\n",
      "Logistic Regression (3655/5000): loss=0.5545652365418297\n",
      "Logistic Regression (3656/5000): loss=0.5563091323791653\n",
      "Logistic Regression (3657/5000): loss=0.5546542162790079\n",
      "Logistic Regression (3658/5000): loss=0.5537686715364208\n",
      "Logistic Regression (3659/5000): loss=0.5564435596640217\n",
      "Logistic Regression (3660/5000): loss=0.5544929877154838\n",
      "Logistic Regression (3661/5000): loss=0.5541163455403441\n",
      "Logistic Regression (3662/5000): loss=0.5564307981628965\n",
      "Logistic Regression (3663/5000): loss=0.5765308345993465\n",
      "Logistic Regression (3664/5000): loss=0.5536956958359031\n",
      "Logistic Regression (3665/5000): loss=0.5540249233339737\n",
      "Logistic Regression (3666/5000): loss=0.5541979521576661\n",
      "Logistic Regression (3667/5000): loss=0.5545615627373898\n",
      "Logistic Regression (3668/5000): loss=0.5565844760222465\n",
      "Logistic Regression (3669/5000): loss=0.5585825069613892\n",
      "Logistic Regression (3670/5000): loss=0.5552681641303739\n",
      "Logistic Regression (3671/5000): loss=0.5557282011898331\n",
      "Logistic Regression (3672/5000): loss=0.5548719425208819\n",
      "Logistic Regression (3673/5000): loss=0.5643628054390734\n",
      "Logistic Regression (3674/5000): loss=0.5624576307746536\n",
      "Logistic Regression (3675/5000): loss=0.5598074479281745\n",
      "Logistic Regression (3676/5000): loss=0.5606659544130101\n",
      "Logistic Regression (3677/5000): loss=0.5561367309335514\n",
      "Logistic Regression (3678/5000): loss=0.5545801636436366\n",
      "Logistic Regression (3679/5000): loss=0.553926865950166\n",
      "Logistic Regression (3680/5000): loss=0.5572283402699163\n",
      "Logistic Regression (3681/5000): loss=0.5559045142374925\n",
      "Logistic Regression (3682/5000): loss=0.5768931960211875\n",
      "Logistic Regression (3683/5000): loss=0.5642344493313172\n",
      "Logistic Regression (3684/5000): loss=0.5719442252968977\n",
      "Logistic Regression (3685/5000): loss=0.5538509230532229\n",
      "Logistic Regression (3686/5000): loss=0.5549572798746485\n",
      "Logistic Regression (3687/5000): loss=0.5592112017149414\n",
      "Logistic Regression (3688/5000): loss=0.5548490965198277\n",
      "Logistic Regression (3689/5000): loss=0.5587098906285864\n",
      "Logistic Regression (3690/5000): loss=0.553718508134944\n",
      "Logistic Regression (3691/5000): loss=0.553609475758839\n",
      "Logistic Regression (3692/5000): loss=0.5580742607922913\n",
      "Logistic Regression (3693/5000): loss=0.5540244524983304\n",
      "Logistic Regression (3694/5000): loss=0.5550729378423765\n",
      "Logistic Regression (3695/5000): loss=0.5538906842949762\n",
      "Logistic Regression (3696/5000): loss=0.5706154908699986\n",
      "Logistic Regression (3697/5000): loss=0.5571143995953983\n",
      "Logistic Regression (3698/5000): loss=0.5543171464263017\n",
      "Logistic Regression (3699/5000): loss=0.5579278098142791\n",
      "Logistic Regression (3700/5000): loss=0.5535587143114274\n",
      "Logistic Regression (3701/5000): loss=0.5566279087538613\n",
      "Logistic Regression (3702/5000): loss=0.5600873782910015\n",
      "Logistic Regression (3703/5000): loss=0.5602302906213403\n",
      "Logistic Regression (3704/5000): loss=0.5538793633520886\n",
      "Logistic Regression (3705/5000): loss=0.5658786652206581\n",
      "Logistic Regression (3706/5000): loss=0.5535784367142776\n",
      "Logistic Regression (3707/5000): loss=0.5535650251273634\n",
      "Logistic Regression (3708/5000): loss=0.5535723850798254\n",
      "Logistic Regression (3709/5000): loss=0.5541107611481075\n",
      "Logistic Regression (3710/5000): loss=0.556663875541379\n",
      "Logistic Regression (3711/5000): loss=0.5535486808401914\n",
      "Logistic Regression (3712/5000): loss=0.5573943980504701\n",
      "Logistic Regression (3713/5000): loss=0.5543329025695971\n",
      "Logistic Regression (3714/5000): loss=0.5545479629198812\n",
      "Logistic Regression (3715/5000): loss=0.5586652608853012\n",
      "Logistic Regression (3716/5000): loss=0.5562792463253509\n",
      "Logistic Regression (3717/5000): loss=0.5586898099595955\n",
      "Logistic Regression (3718/5000): loss=0.5536656620826703\n",
      "Logistic Regression (3719/5000): loss=0.5537490989669509\n",
      "Logistic Regression (3720/5000): loss=0.5536137645725576\n",
      "Logistic Regression (3721/5000): loss=0.5621909949203876\n",
      "Logistic Regression (3722/5000): loss=0.5567135585191997\n",
      "Logistic Regression (3723/5000): loss=0.5543100256777692\n",
      "Logistic Regression (3724/5000): loss=0.5589367694956757\n",
      "Logistic Regression (3725/5000): loss=0.5614854166752253\n",
      "Logistic Regression (3726/5000): loss=0.5542080358494924\n",
      "Logistic Regression (3727/5000): loss=0.5602553556418711\n",
      "Logistic Regression (3728/5000): loss=0.5719818135610077\n",
      "Logistic Regression (3729/5000): loss=0.5558370519251493\n",
      "Logistic Regression (3730/5000): loss=0.5538954657547636\n",
      "Logistic Regression (3731/5000): loss=0.5542594446812141\n",
      "Logistic Regression (3732/5000): loss=0.5561701541107635\n",
      "Logistic Regression (3733/5000): loss=0.5547483034105416\n",
      "Logistic Regression (3734/5000): loss=0.5570816444538239\n",
      "Logistic Regression (3735/5000): loss=0.5624546857666624\n",
      "Logistic Regression (3736/5000): loss=0.5625512161218034\n",
      "Logistic Regression (3737/5000): loss=0.5547349928149511\n",
      "Logistic Regression (3738/5000): loss=0.5547398587431694\n",
      "Logistic Regression (3739/5000): loss=0.5556876590940973\n",
      "Logistic Regression (3740/5000): loss=0.5589543804354111\n",
      "Logistic Regression (3741/5000): loss=0.5567212165409333\n",
      "Logistic Regression (3742/5000): loss=0.5663976624686906\n",
      "Logistic Regression (3743/5000): loss=0.5589038214785194\n",
      "Logistic Regression (3744/5000): loss=0.5535567284420713\n",
      "Logistic Regression (3745/5000): loss=0.553394144443231\n",
      "Logistic Regression (3746/5000): loss=0.563527920165164\n",
      "Logistic Regression (3747/5000): loss=0.5736115112781407\n",
      "Logistic Regression (3748/5000): loss=0.5610172265835299\n",
      "Logistic Regression (3749/5000): loss=0.553495084125501\n",
      "Logistic Regression (3750/5000): loss=0.5682662536227171\n",
      "Logistic Regression (3751/5000): loss=0.5626594403224886\n",
      "Logistic Regression (3752/5000): loss=0.5537586154912038\n",
      "Logistic Regression (3753/5000): loss=0.5562518871372272\n",
      "Logistic Regression (3754/5000): loss=0.5553845344244671\n",
      "Logistic Regression (3755/5000): loss=0.5595541356461984\n",
      "Logistic Regression (3756/5000): loss=0.556130495655248\n",
      "Logistic Regression (3757/5000): loss=0.5559195598233907\n",
      "Logistic Regression (3758/5000): loss=0.5565158457948087\n",
      "Logistic Regression (3759/5000): loss=0.5553748593948618\n",
      "Logistic Regression (3760/5000): loss=0.5585863285977867\n",
      "Logistic Regression (3761/5000): loss=0.5539224486703359\n",
      "Logistic Regression (3762/5000): loss=0.5539798562858267\n",
      "Logistic Regression (3763/5000): loss=0.5542252843516837\n",
      "Logistic Regression (3764/5000): loss=0.5542895907683837\n",
      "Logistic Regression (3765/5000): loss=0.55730281628524\n",
      "Logistic Regression (3766/5000): loss=0.5550816093778274\n",
      "Logistic Regression (3767/5000): loss=0.5537866222359847\n",
      "Logistic Regression (3768/5000): loss=0.5607260134885542\n",
      "Logistic Regression (3769/5000): loss=0.5548097591447497\n",
      "Logistic Regression (3770/5000): loss=0.5540996741316158\n",
      "Logistic Regression (3771/5000): loss=0.5718342551984072\n",
      "Logistic Regression (3772/5000): loss=0.560234778703541\n",
      "Logistic Regression (3773/5000): loss=0.5584028725428118\n",
      "Logistic Regression (3774/5000): loss=0.5534253792757791\n",
      "Logistic Regression (3775/5000): loss=0.5538368636202565\n",
      "Logistic Regression (3776/5000): loss=0.559077555998404\n",
      "Logistic Regression (3777/5000): loss=0.5537478723019494\n",
      "Logistic Regression (3778/5000): loss=0.5537383269403336\n",
      "Logistic Regression (3779/5000): loss=0.5548886257125919\n",
      "Logistic Regression (3780/5000): loss=0.557328523772042\n",
      "Logistic Regression (3781/5000): loss=0.5626307702112344\n",
      "Logistic Regression (3782/5000): loss=0.563465288475524\n",
      "Logistic Regression (3783/5000): loss=0.5593558487212907\n",
      "Logistic Regression (3784/5000): loss=0.5556928383558125\n",
      "Logistic Regression (3785/5000): loss=0.5531157393821041\n",
      "Logistic Regression (3786/5000): loss=0.5534465708971894\n",
      "Logistic Regression (3787/5000): loss=0.5542632910274704\n",
      "Logistic Regression (3788/5000): loss=0.554247380071627\n",
      "Logistic Regression (3789/5000): loss=0.5623485302727278\n",
      "Logistic Regression (3790/5000): loss=0.555893622243379\n",
      "Logistic Regression (3791/5000): loss=0.5685886970353436\n",
      "Logistic Regression (3792/5000): loss=0.5531144377637068\n",
      "Logistic Regression (3793/5000): loss=0.553031610668552\n",
      "Logistic Regression (3794/5000): loss=0.5594373450787061\n",
      "Logistic Regression (3795/5000): loss=0.5558591726871058\n",
      "Logistic Regression (3796/5000): loss=0.5532152411221654\n",
      "Logistic Regression (3797/5000): loss=0.553979131211898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (3798/5000): loss=0.5537797354041799\n",
      "Logistic Regression (3799/5000): loss=0.5557248307363514\n",
      "Logistic Regression (3800/5000): loss=0.5585090755224709\n",
      "Logistic Regression (3801/5000): loss=0.558753012907761\n",
      "Logistic Regression (3802/5000): loss=0.5541143252187433\n",
      "Logistic Regression (3803/5000): loss=0.5648781429694752\n",
      "Logistic Regression (3804/5000): loss=0.5542268705263171\n",
      "Logistic Regression (3805/5000): loss=0.5532442256131958\n",
      "Logistic Regression (3806/5000): loss=0.5530711460726875\n",
      "Logistic Regression (3807/5000): loss=0.5548981837109752\n",
      "Logistic Regression (3808/5000): loss=0.5531989635596581\n",
      "Logistic Regression (3809/5000): loss=0.5539844215831582\n",
      "Logistic Regression (3810/5000): loss=0.5612097329103591\n",
      "Logistic Regression (3811/5000): loss=0.5551418416008035\n",
      "Logistic Regression (3812/5000): loss=0.5567731392866336\n",
      "Logistic Regression (3813/5000): loss=0.5545845165097011\n",
      "Logistic Regression (3814/5000): loss=0.5530407370709892\n",
      "Logistic Regression (3815/5000): loss=0.566560938735905\n",
      "Logistic Regression (3816/5000): loss=0.5595465098349692\n",
      "Logistic Regression (3817/5000): loss=0.5534173130791502\n",
      "Logistic Regression (3818/5000): loss=0.5547901587900663\n",
      "Logistic Regression (3819/5000): loss=0.5551668421844608\n",
      "Logistic Regression (3820/5000): loss=0.5616848094897354\n",
      "Logistic Regression (3821/5000): loss=0.5670558334273709\n",
      "Logistic Regression (3822/5000): loss=0.5530133937843997\n",
      "Logistic Regression (3823/5000): loss=0.5571081787489551\n",
      "Logistic Regression (3824/5000): loss=0.5537417540661886\n",
      "Logistic Regression (3825/5000): loss=0.5658442846089704\n",
      "Logistic Regression (3826/5000): loss=0.5552546185221138\n",
      "Logistic Regression (3827/5000): loss=0.553260724017072\n",
      "Logistic Regression (3828/5000): loss=0.5529761833898448\n",
      "Logistic Regression (3829/5000): loss=0.5542469990598553\n",
      "Logistic Regression (3830/5000): loss=0.5570216986618288\n",
      "Logistic Regression (3831/5000): loss=0.554697503607107\n",
      "Logistic Regression (3832/5000): loss=0.5580760707961957\n",
      "Logistic Regression (3833/5000): loss=0.5626533159587316\n",
      "Logistic Regression (3834/5000): loss=0.565486173490377\n",
      "Logistic Regression (3835/5000): loss=0.5633450043231437\n",
      "Logistic Regression (3836/5000): loss=0.5562866942480065\n",
      "Logistic Regression (3837/5000): loss=0.5531870721160085\n",
      "Logistic Regression (3838/5000): loss=0.5550662535527175\n",
      "Logistic Regression (3839/5000): loss=0.5545120750716686\n",
      "Logistic Regression (3840/5000): loss=0.5531998586758163\n",
      "Logistic Regression (3841/5000): loss=0.5554892239263247\n",
      "Logistic Regression (3842/5000): loss=0.5532893530497884\n",
      "Logistic Regression (3843/5000): loss=0.5742316588007291\n",
      "Logistic Regression (3844/5000): loss=0.5581068801233182\n",
      "Logistic Regression (3845/5000): loss=0.5535367356532431\n",
      "Logistic Regression (3846/5000): loss=0.5584184054614091\n",
      "Logistic Regression (3847/5000): loss=0.5532549563178805\n",
      "Logistic Regression (3848/5000): loss=0.5528842036475166\n",
      "Logistic Regression (3849/5000): loss=0.5536808251407247\n",
      "Logistic Regression (3850/5000): loss=0.5668927173537122\n",
      "Logistic Regression (3851/5000): loss=0.5531589685401853\n",
      "Logistic Regression (3852/5000): loss=0.5639063931182045\n",
      "Logistic Regression (3853/5000): loss=0.5543516867346453\n",
      "Logistic Regression (3854/5000): loss=0.5539837091664864\n",
      "Logistic Regression (3855/5000): loss=0.5529881615712091\n",
      "Logistic Regression (3856/5000): loss=0.5529227982233988\n",
      "Logistic Regression (3857/5000): loss=0.5569389099786471\n",
      "Logistic Regression (3858/5000): loss=0.5528276789254625\n",
      "Logistic Regression (3859/5000): loss=0.5573706878396771\n",
      "Logistic Regression (3860/5000): loss=0.5548648938597852\n",
      "Logistic Regression (3861/5000): loss=0.5574749769034132\n",
      "Logistic Regression (3862/5000): loss=0.5530659225521358\n",
      "Logistic Regression (3863/5000): loss=0.5544313098464638\n",
      "Logistic Regression (3864/5000): loss=0.555200221727594\n",
      "Logistic Regression (3865/5000): loss=0.5545517086544562\n",
      "Logistic Regression (3866/5000): loss=0.5534764680122664\n",
      "Logistic Regression (3867/5000): loss=0.55711527193285\n",
      "Logistic Regression (3868/5000): loss=0.5528609476676625\n",
      "Logistic Regression (3869/5000): loss=0.554025842876191\n",
      "Logistic Regression (3870/5000): loss=0.5528257756173883\n",
      "Logistic Regression (3871/5000): loss=0.5527589141253056\n",
      "Logistic Regression (3872/5000): loss=0.5532881260621508\n",
      "Logistic Regression (3873/5000): loss=0.5542710925839165\n",
      "Logistic Regression (3874/5000): loss=0.560988094575962\n",
      "Logistic Regression (3875/5000): loss=0.5856524067740445\n",
      "Logistic Regression (3876/5000): loss=0.5547275379323107\n",
      "Logistic Regression (3877/5000): loss=0.55554983151378\n",
      "Logistic Regression (3878/5000): loss=0.553773718038122\n",
      "Logistic Regression (3879/5000): loss=0.5645246089635458\n",
      "Logistic Regression (3880/5000): loss=0.5591613695032532\n",
      "Logistic Regression (3881/5000): loss=0.5527323490413838\n",
      "Logistic Regression (3882/5000): loss=0.5548672600444199\n",
      "Logistic Regression (3883/5000): loss=0.5527106217722569\n",
      "Logistic Regression (3884/5000): loss=0.5643188255441106\n",
      "Logistic Regression (3885/5000): loss=0.5544122423802473\n",
      "Logistic Regression (3886/5000): loss=0.5529304860626549\n",
      "Logistic Regression (3887/5000): loss=0.5527194120525739\n",
      "Logistic Regression (3888/5000): loss=0.5525946369587402\n",
      "Logistic Regression (3889/5000): loss=0.5580639240152208\n",
      "Logistic Regression (3890/5000): loss=0.5531135635650802\n",
      "Logistic Regression (3891/5000): loss=0.5544567720026301\n",
      "Logistic Regression (3892/5000): loss=0.5534127884043448\n",
      "Logistic Regression (3893/5000): loss=0.5588131293907883\n",
      "Logistic Regression (3894/5000): loss=0.5591431318281462\n",
      "Logistic Regression (3895/5000): loss=0.5669532209510658\n",
      "Logistic Regression (3896/5000): loss=0.5545300129959341\n",
      "Logistic Regression (3897/5000): loss=0.5577142026444478\n",
      "Logistic Regression (3898/5000): loss=0.5570066730619159\n",
      "Logistic Regression (3899/5000): loss=0.5533151013524833\n",
      "Logistic Regression (3900/5000): loss=0.5752651490779416\n",
      "Logistic Regression (3901/5000): loss=0.5582149565513336\n",
      "Logistic Regression (3902/5000): loss=0.5735638136831264\n",
      "Logistic Regression (3903/5000): loss=0.5542267345409786\n",
      "Logistic Regression (3904/5000): loss=0.5527015071166197\n",
      "Logistic Regression (3905/5000): loss=0.5664208065848746\n",
      "Logistic Regression (3906/5000): loss=0.5528743146137185\n",
      "Logistic Regression (3907/5000): loss=0.5556516633423656\n",
      "Logistic Regression (3908/5000): loss=0.5528606082989799\n",
      "Logistic Regression (3909/5000): loss=0.5529478557233117\n",
      "Logistic Regression (3910/5000): loss=0.5600655944164223\n",
      "Logistic Regression (3911/5000): loss=0.5528976239320068\n",
      "Logistic Regression (3912/5000): loss=0.5598510028992952\n",
      "Logistic Regression (3913/5000): loss=0.5568024037776494\n",
      "Logistic Regression (3914/5000): loss=0.5525782566020494\n",
      "Logistic Regression (3915/5000): loss=0.5542611517009527\n",
      "Logistic Regression (3916/5000): loss=0.5531572713769154\n",
      "Logistic Regression (3917/5000): loss=0.5526903071479814\n",
      "Logistic Regression (3918/5000): loss=0.5697796106850902\n",
      "Logistic Regression (3919/5000): loss=0.5529645181409946\n",
      "Logistic Regression (3920/5000): loss=0.5546760024240718\n",
      "Logistic Regression (3921/5000): loss=0.5570213035821114\n",
      "Logistic Regression (3922/5000): loss=0.5528087698391891\n",
      "Logistic Regression (3923/5000): loss=0.5526055203962826\n",
      "Logistic Regression (3924/5000): loss=0.5529156105070022\n",
      "Logistic Regression (3925/5000): loss=0.5529257727087514\n",
      "Logistic Regression (3926/5000): loss=0.5529985869671835\n",
      "Logistic Regression (3927/5000): loss=0.5535557622276795\n",
      "Logistic Regression (3928/5000): loss=0.55733099231419\n",
      "Logistic Regression (3929/5000): loss=0.5555535009814209\n",
      "Logistic Regression (3930/5000): loss=0.5526399079891291\n",
      "Logistic Regression (3931/5000): loss=0.553753549362938\n",
      "Logistic Regression (3932/5000): loss=0.5528790547906269\n",
      "Logistic Regression (3933/5000): loss=0.5525058937079823\n",
      "Logistic Regression (3934/5000): loss=0.5594603479560462\n",
      "Logistic Regression (3935/5000): loss=0.5559717659299065\n",
      "Logistic Regression (3936/5000): loss=0.5560984463679844\n",
      "Logistic Regression (3937/5000): loss=0.5525691411413067\n",
      "Logistic Regression (3938/5000): loss=0.5616123081819835\n",
      "Logistic Regression (3939/5000): loss=0.552869480813106\n",
      "Logistic Regression (3940/5000): loss=0.5539728399533893\n",
      "Logistic Regression (3941/5000): loss=0.552733711703912\n",
      "Logistic Regression (3942/5000): loss=0.5527179318018486\n",
      "Logistic Regression (3943/5000): loss=0.5558378518320031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (3944/5000): loss=0.5528081367247981\n",
      "Logistic Regression (3945/5000): loss=0.5611431419906256\n",
      "Logistic Regression (3946/5000): loss=0.5625859548399963\n",
      "Logistic Regression (3947/5000): loss=0.5569077000478976\n",
      "Logistic Regression (3948/5000): loss=0.5525884687275918\n",
      "Logistic Regression (3949/5000): loss=0.5553294738697184\n",
      "Logistic Regression (3950/5000): loss=0.5532580011894552\n",
      "Logistic Regression (3951/5000): loss=0.5577100230079901\n",
      "Logistic Regression (3952/5000): loss=0.5528937330735292\n",
      "Logistic Regression (3953/5000): loss=0.5567763823026372\n",
      "Logistic Regression (3954/5000): loss=0.5554555239205345\n",
      "Logistic Regression (3955/5000): loss=0.5781470172255748\n",
      "Logistic Regression (3956/5000): loss=0.552426532658796\n",
      "Logistic Regression (3957/5000): loss=0.5605086417586119\n",
      "Logistic Regression (3958/5000): loss=0.5599094208641828\n",
      "Logistic Regression (3959/5000): loss=0.5533028835325617\n",
      "Logistic Regression (3960/5000): loss=0.5658512151300993\n",
      "Logistic Regression (3961/5000): loss=0.5535661250808771\n",
      "Logistic Regression (3962/5000): loss=0.5547079568253261\n",
      "Logistic Regression (3963/5000): loss=0.5529083313388649\n",
      "Logistic Regression (3964/5000): loss=0.5524306136940818\n",
      "Logistic Regression (3965/5000): loss=0.5523260525650406\n",
      "Logistic Regression (3966/5000): loss=0.5531915568446728\n",
      "Logistic Regression (3967/5000): loss=0.5556882626379671\n",
      "Logistic Regression (3968/5000): loss=0.5553080043010816\n",
      "Logistic Regression (3969/5000): loss=0.5549579211888255\n",
      "Logistic Regression (3970/5000): loss=0.5522503787966944\n",
      "Logistic Regression (3971/5000): loss=0.552406553692002\n",
      "Logistic Regression (3972/5000): loss=0.5692058115035077\n",
      "Logistic Regression (3973/5000): loss=0.5564496024872065\n",
      "Logistic Regression (3974/5000): loss=0.5566994757879042\n",
      "Logistic Regression (3975/5000): loss=0.5529974532776524\n",
      "Logistic Regression (3976/5000): loss=0.5592217237579185\n",
      "Logistic Regression (3977/5000): loss=0.5576288344717767\n",
      "Logistic Regression (3978/5000): loss=0.5523946755747011\n",
      "Logistic Regression (3979/5000): loss=0.5543940025246676\n",
      "Logistic Regression (3980/5000): loss=0.5521053213670848\n",
      "Logistic Regression (3981/5000): loss=0.5526497389072393\n",
      "Logistic Regression (3982/5000): loss=0.5624990234341187\n",
      "Logistic Regression (3983/5000): loss=0.5581609943784598\n",
      "Logistic Regression (3984/5000): loss=0.5547944952637873\n",
      "Logistic Regression (3985/5000): loss=0.5541107866514807\n",
      "Logistic Regression (3986/5000): loss=0.5524364421810886\n",
      "Logistic Regression (3987/5000): loss=0.5537518664446384\n",
      "Logistic Regression (3988/5000): loss=0.5528264823257183\n",
      "Logistic Regression (3989/5000): loss=0.5576076619518021\n",
      "Logistic Regression (3990/5000): loss=0.5525696023502049\n",
      "Logistic Regression (3991/5000): loss=0.5595943333881743\n",
      "Logistic Regression (3992/5000): loss=0.5540231177112773\n",
      "Logistic Regression (3993/5000): loss=0.5525227863691652\n",
      "Logistic Regression (3994/5000): loss=0.5522709586394865\n",
      "Logistic Regression (3995/5000): loss=0.568311666056112\n",
      "Logistic Regression (3996/5000): loss=0.564573503812707\n",
      "Logistic Regression (3997/5000): loss=0.5541134258369038\n",
      "Logistic Regression (3998/5000): loss=0.555186732693859\n",
      "Logistic Regression (3999/5000): loss=0.5546805593946987\n",
      "Logistic Regression (4000/5000): loss=0.5566706742700279\n",
      "Logistic Regression (4001/5000): loss=0.5538467322067265\n",
      "Logistic Regression (4002/5000): loss=0.5545109978259843\n",
      "Logistic Regression (4003/5000): loss=0.5529110802630129\n",
      "Logistic Regression (4004/5000): loss=0.5584191619749406\n",
      "Logistic Regression (4005/5000): loss=0.553426865514712\n",
      "Logistic Regression (4006/5000): loss=0.553820651164416\n",
      "Logistic Regression (4007/5000): loss=0.5524780220355073\n",
      "Logistic Regression (4008/5000): loss=0.5538468094222906\n",
      "Logistic Regression (4009/5000): loss=0.5524053959601608\n",
      "Logistic Regression (4010/5000): loss=0.5533501897903145\n",
      "Logistic Regression (4011/5000): loss=0.5547460962489731\n",
      "Logistic Regression (4012/5000): loss=0.5537398703093692\n",
      "Logistic Regression (4013/5000): loss=0.5622599055762557\n",
      "Logistic Regression (4014/5000): loss=0.5578173973196042\n",
      "Logistic Regression (4015/5000): loss=0.5558347500792751\n",
      "Logistic Regression (4016/5000): loss=0.5532592643518649\n",
      "Logistic Regression (4017/5000): loss=0.5553020575473077\n",
      "Logistic Regression (4018/5000): loss=0.5636747737454397\n",
      "Logistic Regression (4019/5000): loss=0.552084198636553\n",
      "Logistic Regression (4020/5000): loss=0.5634072024647574\n",
      "Logistic Regression (4021/5000): loss=0.5533951451720888\n",
      "Logistic Regression (4022/5000): loss=0.5529876614210979\n",
      "Logistic Regression (4023/5000): loss=0.5586009019219914\n",
      "Logistic Regression (4024/5000): loss=0.5537754633096542\n",
      "Logistic Regression (4025/5000): loss=0.5521390013307684\n",
      "Logistic Regression (4026/5000): loss=0.552475129808735\n",
      "Logistic Regression (4027/5000): loss=0.5519771868106443\n",
      "Logistic Regression (4028/5000): loss=0.5520568251124613\n",
      "Logistic Regression (4029/5000): loss=0.5550482632887648\n",
      "Logistic Regression (4030/5000): loss=0.5562430402466058\n",
      "Logistic Regression (4031/5000): loss=0.5527669400062671\n",
      "Logistic Regression (4032/5000): loss=0.5547264074890988\n",
      "Logistic Regression (4033/5000): loss=0.5526560905876404\n",
      "Logistic Regression (4034/5000): loss=0.5556041686174028\n",
      "Logistic Regression (4035/5000): loss=0.5653822900804731\n",
      "Logistic Regression (4036/5000): loss=0.5533992316153167\n",
      "Logistic Regression (4037/5000): loss=0.5655626951351689\n",
      "Logistic Regression (4038/5000): loss=0.5523807212349531\n",
      "Logistic Regression (4039/5000): loss=0.5602153133435216\n",
      "Logistic Regression (4040/5000): loss=0.5753710325318423\n",
      "Logistic Regression (4041/5000): loss=0.552700401072322\n",
      "Logistic Regression (4042/5000): loss=0.561255876699964\n",
      "Logistic Regression (4043/5000): loss=0.5758105912906921\n",
      "Logistic Regression (4044/5000): loss=0.557053606652505\n",
      "Logistic Regression (4045/5000): loss=0.560713775841974\n",
      "Logistic Regression (4046/5000): loss=0.5578781625449496\n",
      "Logistic Regression (4047/5000): loss=0.5521653278540005\n",
      "Logistic Regression (4048/5000): loss=0.5525982346355648\n",
      "Logistic Regression (4049/5000): loss=0.5524850187429221\n",
      "Logistic Regression (4050/5000): loss=0.5527061371803123\n",
      "Logistic Regression (4051/5000): loss=0.5570202524310992\n",
      "Logistic Regression (4052/5000): loss=0.5526613964758512\n",
      "Logistic Regression (4053/5000): loss=0.5549778559893535\n",
      "Logistic Regression (4054/5000): loss=0.5520166083191197\n",
      "Logistic Regression (4055/5000): loss=0.5546945933711492\n",
      "Logistic Regression (4056/5000): loss=0.5541594104422001\n",
      "Logistic Regression (4057/5000): loss=0.5517438803727235\n",
      "Logistic Regression (4058/5000): loss=0.5517750118991495\n",
      "Logistic Regression (4059/5000): loss=0.5518270889576248\n",
      "Logistic Regression (4060/5000): loss=0.5521699479457447\n",
      "Logistic Regression (4061/5000): loss=0.5529333497129963\n",
      "Logistic Regression (4062/5000): loss=0.5555399844345882\n",
      "Logistic Regression (4063/5000): loss=0.5517323063842529\n",
      "Logistic Regression (4064/5000): loss=0.5536736623876606\n",
      "Logistic Regression (4065/5000): loss=0.553236060269231\n",
      "Logistic Regression (4066/5000): loss=0.5517636776052356\n",
      "Logistic Regression (4067/5000): loss=0.5526317915319339\n",
      "Logistic Regression (4068/5000): loss=0.555042345809611\n",
      "Logistic Regression (4069/5000): loss=0.5545792367730624\n",
      "Logistic Regression (4070/5000): loss=0.5653217014160509\n",
      "Logistic Regression (4071/5000): loss=0.5520397072404395\n",
      "Logistic Regression (4072/5000): loss=0.5548409213575216\n",
      "Logistic Regression (4073/5000): loss=0.5528905882284929\n",
      "Logistic Regression (4074/5000): loss=0.5522972668757722\n",
      "Logistic Regression (4075/5000): loss=0.5559262511464783\n",
      "Logistic Regression (4076/5000): loss=0.554574561336059\n",
      "Logistic Regression (4077/5000): loss=0.5528296971319596\n",
      "Logistic Regression (4078/5000): loss=0.5663304975493676\n",
      "Logistic Regression (4079/5000): loss=0.5541227716989587\n",
      "Logistic Regression (4080/5000): loss=0.5559549978358896\n",
      "Logistic Regression (4081/5000): loss=0.5559147143807339\n",
      "Logistic Regression (4082/5000): loss=0.5518536534187513\n",
      "Logistic Regression (4083/5000): loss=0.5592292717918567\n",
      "Logistic Regression (4084/5000): loss=0.5521151882259627\n",
      "Logistic Regression (4085/5000): loss=0.553333781153644\n",
      "Logistic Regression (4086/5000): loss=0.5527748311530413\n",
      "Logistic Regression (4087/5000): loss=0.55201268189188\n",
      "Logistic Regression (4088/5000): loss=0.5568619562897063\n",
      "Logistic Regression (4089/5000): loss=0.5569904212043866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (4090/5000): loss=0.5649080991062185\n",
      "Logistic Regression (4091/5000): loss=0.5520303248119693\n",
      "Logistic Regression (4092/5000): loss=0.5987603437536578\n",
      "Logistic Regression (4093/5000): loss=0.5521019437953615\n",
      "Logistic Regression (4094/5000): loss=0.5569511128601431\n",
      "Logistic Regression (4095/5000): loss=0.5518345990194173\n",
      "Logistic Regression (4096/5000): loss=0.553404288838879\n",
      "Logistic Regression (4097/5000): loss=0.5520079463154824\n",
      "Logistic Regression (4098/5000): loss=0.5573296571529393\n",
      "Logistic Regression (4099/5000): loss=0.5559978544541969\n",
      "Logistic Regression (4100/5000): loss=0.5596662971806605\n",
      "Logistic Regression (4101/5000): loss=0.5580161636230538\n",
      "Logistic Regression (4102/5000): loss=0.552985614932102\n",
      "Logistic Regression (4103/5000): loss=0.5525099796431853\n",
      "Logistic Regression (4104/5000): loss=0.5517864167071231\n",
      "Logistic Regression (4105/5000): loss=0.5516753352345457\n",
      "Logistic Regression (4106/5000): loss=0.5602846493916311\n",
      "Logistic Regression (4107/5000): loss=0.5599364155932068\n",
      "Logistic Regression (4108/5000): loss=0.5515751929057066\n",
      "Logistic Regression (4109/5000): loss=0.5516042504577714\n",
      "Logistic Regression (4110/5000): loss=0.5569327840525035\n",
      "Logistic Regression (4111/5000): loss=0.5536691968504817\n",
      "Logistic Regression (4112/5000): loss=0.555258550850386\n",
      "Logistic Regression (4113/5000): loss=0.5597331001161395\n",
      "Logistic Regression (4114/5000): loss=0.5542857131941717\n",
      "Logistic Regression (4115/5000): loss=0.5599900143241012\n",
      "Logistic Regression (4116/5000): loss=0.5550303503794627\n",
      "Logistic Regression (4117/5000): loss=0.554116803411822\n",
      "Logistic Regression (4118/5000): loss=0.5531984124551927\n",
      "Logistic Regression (4119/5000): loss=0.5518779866343944\n",
      "Logistic Regression (4120/5000): loss=0.5517839049570814\n",
      "Logistic Regression (4121/5000): loss=0.5515465973038854\n",
      "Logistic Regression (4122/5000): loss=0.556231966651917\n",
      "Logistic Regression (4123/5000): loss=0.5525488707169997\n",
      "Logistic Regression (4124/5000): loss=0.5517656288061744\n",
      "Logistic Regression (4125/5000): loss=0.55620554886847\n",
      "Logistic Regression (4126/5000): loss=0.5575947552077897\n",
      "Logistic Regression (4127/5000): loss=0.5517379860966485\n",
      "Logistic Regression (4128/5000): loss=0.5524833575671688\n",
      "Logistic Regression (4129/5000): loss=0.5522428140377766\n",
      "Logistic Regression (4130/5000): loss=0.5738021593665785\n",
      "Logistic Regression (4131/5000): loss=0.5522326078672442\n",
      "Logistic Regression (4132/5000): loss=0.552779323714996\n",
      "Logistic Regression (4133/5000): loss=0.552752287389477\n",
      "Logistic Regression (4134/5000): loss=0.5538298515021156\n",
      "Logistic Regression (4135/5000): loss=0.5590335838529537\n",
      "Logistic Regression (4136/5000): loss=0.5540326081316708\n",
      "Logistic Regression (4137/5000): loss=0.5542240232538866\n",
      "Logistic Regression (4138/5000): loss=0.5605227570415132\n",
      "Logistic Regression (4139/5000): loss=0.5535769778965521\n",
      "Logistic Regression (4140/5000): loss=0.5534848632364413\n",
      "Logistic Regression (4141/5000): loss=0.5530108229828288\n",
      "Logistic Regression (4142/5000): loss=0.559161266459982\n",
      "Logistic Regression (4143/5000): loss=0.555485143933136\n",
      "Logistic Regression (4144/5000): loss=0.5530029269341317\n",
      "Logistic Regression (4145/5000): loss=0.5535374834252907\n",
      "Logistic Regression (4146/5000): loss=0.5560516972435016\n",
      "Logistic Regression (4147/5000): loss=0.5534066576498543\n",
      "Logistic Regression (4148/5000): loss=0.5592942677748557\n",
      "Logistic Regression (4149/5000): loss=0.5721098813222077\n",
      "Logistic Regression (4150/5000): loss=0.6067621900221194\n",
      "Logistic Regression (4151/5000): loss=0.5719967313066899\n",
      "Logistic Regression (4152/5000): loss=0.5549660042236062\n",
      "Logistic Regression (4153/5000): loss=0.5638726559037538\n",
      "Logistic Regression (4154/5000): loss=0.5590845359745705\n",
      "Logistic Regression (4155/5000): loss=0.5524513148860203\n",
      "Logistic Regression (4156/5000): loss=0.5536861909225774\n",
      "Logistic Regression (4157/5000): loss=0.5515515648520481\n",
      "Logistic Regression (4158/5000): loss=0.5546042286816206\n",
      "Logistic Regression (4159/5000): loss=0.5519132176394099\n",
      "Logistic Regression (4160/5000): loss=0.5579462482794905\n",
      "Logistic Regression (4161/5000): loss=0.5612934210697693\n",
      "Logistic Regression (4162/5000): loss=0.5514272867807253\n",
      "Logistic Regression (4163/5000): loss=0.5533338634245553\n",
      "Logistic Regression (4164/5000): loss=0.554907428758223\n",
      "Logistic Regression (4165/5000): loss=0.5671801925742661\n",
      "Logistic Regression (4166/5000): loss=0.551433463622774\n",
      "Logistic Regression (4167/5000): loss=0.5591347367924214\n",
      "Logistic Regression (4168/5000): loss=0.5515465059132947\n",
      "Logistic Regression (4169/5000): loss=0.5513803262969573\n",
      "Logistic Regression (4170/5000): loss=0.5522148413893069\n",
      "Logistic Regression (4171/5000): loss=0.5554073052670907\n",
      "Logistic Regression (4172/5000): loss=0.5516785782360023\n",
      "Logistic Regression (4173/5000): loss=0.5539650535819122\n",
      "Logistic Regression (4174/5000): loss=0.5552747371925896\n",
      "Logistic Regression (4175/5000): loss=0.551479029598484\n",
      "Logistic Regression (4176/5000): loss=0.5512530959306221\n",
      "Logistic Regression (4177/5000): loss=0.5512777930710202\n",
      "Logistic Regression (4178/5000): loss=0.5540476120118522\n",
      "Logistic Regression (4179/5000): loss=0.5521392826100715\n",
      "Logistic Regression (4180/5000): loss=0.5519368550406242\n",
      "Logistic Regression (4181/5000): loss=0.5515881902132991\n",
      "Logistic Regression (4182/5000): loss=0.562553576925407\n",
      "Logistic Regression (4183/5000): loss=0.5512390405409477\n",
      "Logistic Regression (4184/5000): loss=0.553401609908988\n",
      "Logistic Regression (4185/5000): loss=0.5516787983265452\n",
      "Logistic Regression (4186/5000): loss=0.5672006397575015\n",
      "Logistic Regression (4187/5000): loss=0.5522973931035355\n",
      "Logistic Regression (4188/5000): loss=0.5550902222039683\n",
      "Logistic Regression (4189/5000): loss=0.5754577245582941\n",
      "Logistic Regression (4190/5000): loss=0.5523615314422202\n",
      "Logistic Regression (4191/5000): loss=0.5581745662211155\n",
      "Logistic Regression (4192/5000): loss=0.5530317018988856\n",
      "Logistic Regression (4193/5000): loss=0.553618909518746\n",
      "Logistic Regression (4194/5000): loss=0.5516583590055487\n",
      "Logistic Regression (4195/5000): loss=0.5607890024523945\n",
      "Logistic Regression (4196/5000): loss=0.557826451150398\n",
      "Logistic Regression (4197/5000): loss=0.5523696121644782\n",
      "Logistic Regression (4198/5000): loss=0.5512383971228456\n",
      "Logistic Regression (4199/5000): loss=0.5532528930980392\n",
      "Logistic Regression (4200/5000): loss=0.5516707759274109\n",
      "Logistic Regression (4201/5000): loss=0.5512896508530412\n",
      "Logistic Regression (4202/5000): loss=0.5511811299339917\n",
      "Logistic Regression (4203/5000): loss=0.5517846907592896\n",
      "Logistic Regression (4204/5000): loss=0.5541956106780049\n",
      "Logistic Regression (4205/5000): loss=0.5546451628800293\n",
      "Logistic Regression (4206/5000): loss=0.5540270956184232\n",
      "Logistic Regression (4207/5000): loss=0.5524131964719571\n",
      "Logistic Regression (4208/5000): loss=0.5587534666445184\n",
      "Logistic Regression (4209/5000): loss=0.5572158764997514\n",
      "Logistic Regression (4210/5000): loss=0.5523665314230265\n",
      "Logistic Regression (4211/5000): loss=0.5566084635789738\n",
      "Logistic Regression (4212/5000): loss=0.5518012707571082\n",
      "Logistic Regression (4213/5000): loss=0.5528639962392294\n",
      "Logistic Regression (4214/5000): loss=0.5519712444288276\n",
      "Logistic Regression (4215/5000): loss=0.5530488117731645\n",
      "Logistic Regression (4216/5000): loss=0.5547795986928659\n",
      "Logistic Regression (4217/5000): loss=0.5517615088196668\n",
      "Logistic Regression (4218/5000): loss=0.5540670587002704\n",
      "Logistic Regression (4219/5000): loss=0.5582211458819196\n",
      "Logistic Regression (4220/5000): loss=0.551004672719781\n",
      "Logistic Regression (4221/5000): loss=0.559479342023085\n",
      "Logistic Regression (4222/5000): loss=0.5511887769522659\n",
      "Logistic Regression (4223/5000): loss=0.551635444491468\n",
      "Logistic Regression (4224/5000): loss=0.5515484852843242\n",
      "Logistic Regression (4225/5000): loss=0.560622513419469\n",
      "Logistic Regression (4226/5000): loss=0.5540246783412529\n",
      "Logistic Regression (4227/5000): loss=0.5534821014354987\n",
      "Logistic Regression (4228/5000): loss=0.5531577419623832\n",
      "Logistic Regression (4229/5000): loss=0.553629773940957\n",
      "Logistic Regression (4230/5000): loss=0.5520970074589785\n",
      "Logistic Regression (4231/5000): loss=0.5521270000877696\n",
      "Logistic Regression (4232/5000): loss=0.5515515772216059\n",
      "Logistic Regression (4233/5000): loss=0.5511049910823662\n",
      "Logistic Regression (4234/5000): loss=0.5518720747577626\n",
      "Logistic Regression (4235/5000): loss=0.5514884036117438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (4236/5000): loss=0.5531938156157687\n",
      "Logistic Regression (4237/5000): loss=0.5518654783731616\n",
      "Logistic Regression (4238/5000): loss=0.5553100853755998\n",
      "Logistic Regression (4239/5000): loss=0.5576474341964804\n",
      "Logistic Regression (4240/5000): loss=0.5704634018849534\n",
      "Logistic Regression (4241/5000): loss=0.5514533819836444\n",
      "Logistic Regression (4242/5000): loss=0.5609639510821809\n",
      "Logistic Regression (4243/5000): loss=0.5510711773464883\n",
      "Logistic Regression (4244/5000): loss=0.5560912934333616\n",
      "Logistic Regression (4245/5000): loss=0.5545970783009793\n",
      "Logistic Regression (4246/5000): loss=0.5512084623424698\n",
      "Logistic Regression (4247/5000): loss=0.5510182431277536\n",
      "Logistic Regression (4248/5000): loss=0.5554526690926916\n",
      "Logistic Regression (4249/5000): loss=0.5524286687692048\n",
      "Logistic Regression (4250/5000): loss=0.552258636316744\n",
      "Logistic Regression (4251/5000): loss=0.5524149162166639\n",
      "Logistic Regression (4252/5000): loss=0.554438741572554\n",
      "Logistic Regression (4253/5000): loss=0.5522694739226096\n",
      "Logistic Regression (4254/5000): loss=0.5522991231298802\n",
      "Logistic Regression (4255/5000): loss=0.5541470297450115\n",
      "Logistic Regression (4256/5000): loss=0.5510819814031169\n",
      "Logistic Regression (4257/5000): loss=0.5511620540593143\n",
      "Logistic Regression (4258/5000): loss=0.5565456142500692\n",
      "Logistic Regression (4259/5000): loss=0.5513209590162261\n",
      "Logistic Regression (4260/5000): loss=0.551059707128775\n",
      "Logistic Regression (4261/5000): loss=0.560282418153408\n",
      "Logistic Regression (4262/5000): loss=0.5528281128303041\n",
      "Logistic Regression (4263/5000): loss=0.5545061323027799\n",
      "Logistic Regression (4264/5000): loss=0.555687880967389\n",
      "Logistic Regression (4265/5000): loss=0.5510410808330332\n",
      "Logistic Regression (4266/5000): loss=0.557965516661755\n",
      "Logistic Regression (4267/5000): loss=0.554841563455474\n",
      "Logistic Regression (4268/5000): loss=0.5572653156868981\n",
      "Logistic Regression (4269/5000): loss=0.5529883390810318\n",
      "Logistic Regression (4270/5000): loss=0.551041633831309\n",
      "Logistic Regression (4271/5000): loss=0.5509799328011658\n",
      "Logistic Regression (4272/5000): loss=0.5605638563631558\n",
      "Logistic Regression (4273/5000): loss=0.5512022218210105\n",
      "Logistic Regression (4274/5000): loss=0.5544982077059248\n",
      "Logistic Regression (4275/5000): loss=0.5522150060737062\n",
      "Logistic Regression (4276/5000): loss=0.5512943167448706\n",
      "Logistic Regression (4277/5000): loss=0.5519266439419162\n",
      "Logistic Regression (4278/5000): loss=0.5514685742053341\n",
      "Logistic Regression (4279/5000): loss=0.5627200770177214\n",
      "Logistic Regression (4280/5000): loss=0.5521321514201895\n",
      "Logistic Regression (4281/5000): loss=0.5521250334678879\n",
      "Logistic Regression (4282/5000): loss=0.5579506641313461\n",
      "Logistic Regression (4283/5000): loss=0.5591095781919471\n",
      "Logistic Regression (4284/5000): loss=0.5517313334248993\n",
      "Logistic Regression (4285/5000): loss=0.5509478028973525\n",
      "Logistic Regression (4286/5000): loss=0.557003297702798\n",
      "Logistic Regression (4287/5000): loss=0.5550952109841304\n",
      "Logistic Regression (4288/5000): loss=0.5507822242008511\n",
      "Logistic Regression (4289/5000): loss=0.5507172693885487\n",
      "Logistic Regression (4290/5000): loss=0.5615210225376418\n",
      "Logistic Regression (4291/5000): loss=0.5596628511123531\n",
      "Logistic Regression (4292/5000): loss=0.5510448407315626\n",
      "Logistic Regression (4293/5000): loss=0.5510583090073534\n",
      "Logistic Regression (4294/5000): loss=0.5518363358717191\n",
      "Logistic Regression (4295/5000): loss=0.5766708514505977\n",
      "Logistic Regression (4296/5000): loss=0.553446819513868\n",
      "Logistic Regression (4297/5000): loss=0.5560523915092384\n",
      "Logistic Regression (4298/5000): loss=0.5558858956593923\n",
      "Logistic Regression (4299/5000): loss=0.5511529557465871\n",
      "Logistic Regression (4300/5000): loss=0.5511170542236241\n",
      "Logistic Regression (4301/5000): loss=0.5591155557108369\n",
      "Logistic Regression (4302/5000): loss=0.5522307142510386\n",
      "Logistic Regression (4303/5000): loss=0.5541874846966901\n",
      "Logistic Regression (4304/5000): loss=0.5512666577378623\n",
      "Logistic Regression (4305/5000): loss=0.5509066283976192\n",
      "Logistic Regression (4306/5000): loss=0.5520903630591513\n",
      "Logistic Regression (4307/5000): loss=0.5518101818741625\n",
      "Logistic Regression (4308/5000): loss=0.5535513466715435\n",
      "Logistic Regression (4309/5000): loss=0.5516656981210425\n",
      "Logistic Regression (4310/5000): loss=0.5531886734906827\n",
      "Logistic Regression (4311/5000): loss=0.5545557843634218\n",
      "Logistic Regression (4312/5000): loss=0.5615778423466916\n",
      "Logistic Regression (4313/5000): loss=0.5525483575086904\n",
      "Logistic Regression (4314/5000): loss=0.561662895312792\n",
      "Logistic Regression (4315/5000): loss=0.5516967200089777\n",
      "Logistic Regression (4316/5000): loss=0.5590500063603123\n",
      "Logistic Regression (4317/5000): loss=0.5560914315945849\n",
      "Logistic Regression (4318/5000): loss=0.5525355049904628\n",
      "Logistic Regression (4319/5000): loss=0.5513559634908245\n",
      "Logistic Regression (4320/5000): loss=0.5598635474278882\n",
      "Logistic Regression (4321/5000): loss=0.5518643618775771\n",
      "Logistic Regression (4322/5000): loss=0.5509896810787507\n",
      "Logistic Regression (4323/5000): loss=0.5597386338380755\n",
      "Logistic Regression (4324/5000): loss=0.5533571485166602\n",
      "Logistic Regression (4325/5000): loss=0.5523038003652307\n",
      "Logistic Regression (4326/5000): loss=0.5509885876075629\n",
      "Logistic Regression (4327/5000): loss=0.5510467157028691\n",
      "Logistic Regression (4328/5000): loss=0.5647678379085286\n",
      "Logistic Regression (4329/5000): loss=0.5513221049896153\n",
      "Logistic Regression (4330/5000): loss=0.5518552372053744\n",
      "Logistic Regression (4331/5000): loss=0.5720276964529873\n",
      "Logistic Regression (4332/5000): loss=0.5507172953486069\n",
      "Logistic Regression (4333/5000): loss=0.5512949242083277\n",
      "Logistic Regression (4334/5000): loss=0.5552966045432419\n",
      "Logistic Regression (4335/5000): loss=0.5552841745661288\n",
      "Logistic Regression (4336/5000): loss=0.5514528872392931\n",
      "Logistic Regression (4337/5000): loss=0.5519493986821745\n",
      "Logistic Regression (4338/5000): loss=0.5520104754639612\n",
      "Logistic Regression (4339/5000): loss=0.5505833013170296\n",
      "Logistic Regression (4340/5000): loss=0.5505142263049753\n",
      "Logistic Regression (4341/5000): loss=0.5538291122831774\n",
      "Logistic Regression (4342/5000): loss=0.552103032358611\n",
      "Logistic Regression (4343/5000): loss=0.5540474648501137\n",
      "Logistic Regression (4344/5000): loss=0.550877945526104\n",
      "Logistic Regression (4345/5000): loss=0.5570134374628066\n",
      "Logistic Regression (4346/5000): loss=0.5509675836852778\n",
      "Logistic Regression (4347/5000): loss=0.5517137675336429\n",
      "Logistic Regression (4348/5000): loss=0.5531343419729146\n",
      "Logistic Regression (4349/5000): loss=0.5507409242131872\n",
      "Logistic Regression (4350/5000): loss=0.5537421027698753\n",
      "Logistic Regression (4351/5000): loss=0.5508320056325615\n",
      "Logistic Regression (4352/5000): loss=0.551019234975069\n",
      "Logistic Regression (4353/5000): loss=0.5507460262725133\n",
      "Logistic Regression (4354/5000): loss=0.5521707185744932\n",
      "Logistic Regression (4355/5000): loss=0.552519626654982\n",
      "Logistic Regression (4356/5000): loss=0.5515362420104608\n",
      "Logistic Regression (4357/5000): loss=0.551925271549333\n",
      "Logistic Regression (4358/5000): loss=0.5514788433858548\n",
      "Logistic Regression (4359/5000): loss=0.6045443621747206\n",
      "Logistic Regression (4360/5000): loss=0.5558876467902152\n",
      "Logistic Regression (4361/5000): loss=0.5582672237816279\n",
      "Logistic Regression (4362/5000): loss=0.5676519418247915\n",
      "Logistic Regression (4363/5000): loss=0.5720392057780299\n",
      "Logistic Regression (4364/5000): loss=0.5505168045711594\n",
      "Logistic Regression (4365/5000): loss=0.5537133898870356\n",
      "Logistic Regression (4366/5000): loss=0.5506335742941364\n",
      "Logistic Regression (4367/5000): loss=0.5630394940313869\n",
      "Logistic Regression (4368/5000): loss=0.5600582434307605\n",
      "Logistic Regression (4369/5000): loss=0.5524579002790938\n",
      "Logistic Regression (4370/5000): loss=0.5548443348685945\n",
      "Logistic Regression (4371/5000): loss=0.5514311359962802\n",
      "Logistic Regression (4372/5000): loss=0.5552956302554838\n",
      "Logistic Regression (4373/5000): loss=0.5571909754475824\n",
      "Logistic Regression (4374/5000): loss=0.5519387035731783\n",
      "Logistic Regression (4375/5000): loss=0.5533866603109483\n",
      "Logistic Regression (4376/5000): loss=0.5503466283947597\n",
      "Logistic Regression (4377/5000): loss=0.5552467392603744\n",
      "Logistic Regression (4378/5000): loss=0.5515436814804978\n",
      "Logistic Regression (4379/5000): loss=0.556766483779874\n",
      "Logistic Regression (4380/5000): loss=0.5531354070689234\n",
      "Logistic Regression (4381/5000): loss=0.5506958186700579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (4382/5000): loss=0.5566634802891384\n",
      "Logistic Regression (4383/5000): loss=0.5530156828812661\n",
      "Logistic Regression (4384/5000): loss=0.5518316068039057\n",
      "Logistic Regression (4385/5000): loss=0.5512815082223438\n",
      "Logistic Regression (4386/5000): loss=0.5534881158437549\n",
      "Logistic Regression (4387/5000): loss=0.5508979833497626\n",
      "Logistic Regression (4388/5000): loss=0.5578622586952644\n",
      "Logistic Regression (4389/5000): loss=0.5510426624540312\n",
      "Logistic Regression (4390/5000): loss=0.5502817291505969\n",
      "Logistic Regression (4391/5000): loss=0.5543225472559486\n",
      "Logistic Regression (4392/5000): loss=0.552263260847675\n",
      "Logistic Regression (4393/5000): loss=0.5502376126558891\n",
      "Logistic Regression (4394/5000): loss=0.5502983507027838\n",
      "Logistic Regression (4395/5000): loss=0.5502112255266758\n",
      "Logistic Regression (4396/5000): loss=0.5501848867614806\n",
      "Logistic Regression (4397/5000): loss=0.5512550860181523\n",
      "Logistic Regression (4398/5000): loss=0.5505686194122845\n",
      "Logistic Regression (4399/5000): loss=0.5507336173045714\n",
      "Logistic Regression (4400/5000): loss=0.555416603891949\n",
      "Logistic Regression (4401/5000): loss=0.5512176374169211\n",
      "Logistic Regression (4402/5000): loss=0.5501664720937492\n",
      "Logistic Regression (4403/5000): loss=0.5679265433194919\n",
      "Logistic Regression (4404/5000): loss=0.5834338154965065\n",
      "Logistic Regression (4405/5000): loss=0.5570839162505165\n",
      "Logistic Regression (4406/5000): loss=0.565005157117628\n",
      "Logistic Regression (4407/5000): loss=0.5575395041592371\n",
      "Logistic Regression (4408/5000): loss=0.5576911344863326\n",
      "Logistic Regression (4409/5000): loss=0.5529472554662781\n",
      "Logistic Regression (4410/5000): loss=0.5595986646485537\n",
      "Logistic Regression (4411/5000): loss=0.5501290794500824\n",
      "Logistic Regression (4412/5000): loss=0.5532239989001932\n",
      "Logistic Regression (4413/5000): loss=0.5505126872427223\n",
      "Logistic Regression (4414/5000): loss=0.5653518507648767\n",
      "Logistic Regression (4415/5000): loss=0.5574278669389281\n",
      "Logistic Regression (4416/5000): loss=0.5509484536968968\n",
      "Logistic Regression (4417/5000): loss=0.5502489452893025\n",
      "Logistic Regression (4418/5000): loss=0.5524567678146226\n",
      "Logistic Regression (4419/5000): loss=0.5500989560762709\n",
      "Logistic Regression (4420/5000): loss=0.5513948638339328\n",
      "Logistic Regression (4421/5000): loss=0.5506130396147682\n",
      "Logistic Regression (4422/5000): loss=0.5595058207810598\n",
      "Logistic Regression (4423/5000): loss=0.5524190892596738\n",
      "Logistic Regression (4424/5000): loss=0.552600617580837\n",
      "Logistic Regression (4425/5000): loss=0.5512387196609538\n",
      "Logistic Regression (4426/5000): loss=0.5535713749549184\n",
      "Logistic Regression (4427/5000): loss=0.5553208866177447\n",
      "Logistic Regression (4428/5000): loss=0.553821570725155\n",
      "Logistic Regression (4429/5000): loss=0.5512989742267487\n",
      "Logistic Regression (4430/5000): loss=0.5504384906047295\n",
      "Logistic Regression (4431/5000): loss=0.5640366563799701\n",
      "Logistic Regression (4432/5000): loss=0.5513978479694569\n",
      "Logistic Regression (4433/5000): loss=0.5544289324547167\n",
      "Logistic Regression (4434/5000): loss=0.5527334124960754\n",
      "Logistic Regression (4435/5000): loss=0.5535671679630255\n",
      "Logistic Regression (4436/5000): loss=0.5503977992782145\n",
      "Logistic Regression (4437/5000): loss=0.5511669951202959\n",
      "Logistic Regression (4438/5000): loss=0.5510362616439308\n",
      "Logistic Regression (4439/5000): loss=0.5514534644291371\n",
      "Logistic Regression (4440/5000): loss=0.5519530684235323\n",
      "Logistic Regression (4441/5000): loss=0.5558225223333618\n",
      "Logistic Regression (4442/5000): loss=0.5516029983678977\n",
      "Logistic Regression (4443/5000): loss=0.5604363217657872\n",
      "Logistic Regression (4444/5000): loss=0.5725921312160882\n",
      "Logistic Regression (4445/5000): loss=0.5604697635681647\n",
      "Logistic Regression (4446/5000): loss=0.5511035913866342\n",
      "Logistic Regression (4447/5000): loss=0.553134573838799\n",
      "Logistic Regression (4448/5000): loss=0.5507870530647732\n",
      "Logistic Regression (4449/5000): loss=0.5656358644773134\n",
      "Logistic Regression (4450/5000): loss=0.5508309097896713\n",
      "Logistic Regression (4451/5000): loss=0.5571178578878974\n",
      "Logistic Regression (4452/5000): loss=0.5508935301615753\n",
      "Logistic Regression (4453/5000): loss=0.5521194964349705\n",
      "Logistic Regression (4454/5000): loss=0.552257763100637\n",
      "Logistic Regression (4455/5000): loss=0.5631262294776399\n",
      "Logistic Regression (4456/5000): loss=0.5559081294765672\n",
      "Logistic Regression (4457/5000): loss=0.5557945965694324\n",
      "Logistic Regression (4458/5000): loss=0.5541055984091471\n",
      "Logistic Regression (4459/5000): loss=0.5518134458306465\n",
      "Logistic Regression (4460/5000): loss=0.5530192367293436\n",
      "Logistic Regression (4461/5000): loss=0.5609256398749317\n",
      "Logistic Regression (4462/5000): loss=0.5531852846489063\n",
      "Logistic Regression (4463/5000): loss=0.5499454127192661\n",
      "Logistic Regression (4464/5000): loss=0.5505041150824632\n",
      "Logistic Regression (4465/5000): loss=0.5507808740482439\n",
      "Logistic Regression (4466/5000): loss=0.5508446200296846\n",
      "Logistic Regression (4467/5000): loss=0.5505268131745354\n",
      "Logistic Regression (4468/5000): loss=0.5502120996640002\n",
      "Logistic Regression (4469/5000): loss=0.550099141368004\n",
      "Logistic Regression (4470/5000): loss=0.5501486112904851\n",
      "Logistic Regression (4471/5000): loss=0.550152089279977\n",
      "Logistic Regression (4472/5000): loss=0.5502992693159421\n",
      "Logistic Regression (4473/5000): loss=0.5538051172684582\n",
      "Logistic Regression (4474/5000): loss=0.5520113683266286\n",
      "Logistic Regression (4475/5000): loss=0.5536965770110014\n",
      "Logistic Regression (4476/5000): loss=0.551853756615855\n",
      "Logistic Regression (4477/5000): loss=0.5571342363417091\n",
      "Logistic Regression (4478/5000): loss=0.5499463157388983\n",
      "Logistic Regression (4479/5000): loss=0.5543936806287272\n",
      "Logistic Regression (4480/5000): loss=0.5551781614019419\n",
      "Logistic Regression (4481/5000): loss=0.5634161070244774\n",
      "Logistic Regression (4482/5000): loss=0.5791339262358303\n",
      "Logistic Regression (4483/5000): loss=0.5676701877359189\n",
      "Logistic Regression (4484/5000): loss=0.5506258283889913\n",
      "Logistic Regression (4485/5000): loss=0.5584235310615538\n",
      "Logistic Regression (4486/5000): loss=0.5504408917150558\n",
      "Logistic Regression (4487/5000): loss=0.5625627580092416\n",
      "Logistic Regression (4488/5000): loss=0.554973771911546\n",
      "Logistic Regression (4489/5000): loss=0.5532016914330972\n",
      "Logistic Regression (4490/5000): loss=0.5507313474694475\n",
      "Logistic Regression (4491/5000): loss=0.5554294572956321\n",
      "Logistic Regression (4492/5000): loss=0.5501158601268179\n",
      "Logistic Regression (4493/5000): loss=0.5556921469544955\n",
      "Logistic Regression (4494/5000): loss=0.5497831186234565\n",
      "Logistic Regression (4495/5000): loss=0.5595534460391887\n",
      "Logistic Regression (4496/5000): loss=0.5584819686220497\n",
      "Logistic Regression (4497/5000): loss=0.5497937575953712\n",
      "Logistic Regression (4498/5000): loss=0.5507336307030405\n",
      "Logistic Regression (4499/5000): loss=0.5549008281342964\n",
      "Logistic Regression (4500/5000): loss=0.569439512936135\n",
      "Logistic Regression (4501/5000): loss=0.5501509064051722\n",
      "Logistic Regression (4502/5000): loss=0.5500502129399153\n",
      "Logistic Regression (4503/5000): loss=0.5609301223304646\n",
      "Logistic Regression (4504/5000): loss=0.5505411008554421\n",
      "Logistic Regression (4505/5000): loss=0.5506582196674406\n",
      "Logistic Regression (4506/5000): loss=0.5525456595044009\n",
      "Logistic Regression (4507/5000): loss=0.5606450263352156\n",
      "Logistic Regression (4508/5000): loss=0.5540863189679834\n",
      "Logistic Regression (4509/5000): loss=0.5499102401863156\n",
      "Logistic Regression (4510/5000): loss=0.5528227897112802\n",
      "Logistic Regression (4511/5000): loss=0.5506462642476164\n",
      "Logistic Regression (4512/5000): loss=0.5570310907493732\n",
      "Logistic Regression (4513/5000): loss=0.5595846119504165\n",
      "Logistic Regression (4514/5000): loss=0.5556872946927646\n",
      "Logistic Regression (4515/5000): loss=0.5524234896947677\n",
      "Logistic Regression (4516/5000): loss=0.5619463282734767\n",
      "Logistic Regression (4517/5000): loss=0.5511716153365731\n",
      "Logistic Regression (4518/5000): loss=0.5675512989276617\n",
      "Logistic Regression (4519/5000): loss=0.5592741181282126\n",
      "Logistic Regression (4520/5000): loss=0.5510532904555673\n",
      "Logistic Regression (4521/5000): loss=0.5601750104642355\n",
      "Logistic Regression (4522/5000): loss=0.550081859645231\n",
      "Logistic Regression (4523/5000): loss=0.549832738644499\n",
      "Logistic Regression (4524/5000): loss=0.5543213549713878\n",
      "Logistic Regression (4525/5000): loss=0.5537854454681304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (4526/5000): loss=0.5527680904040247\n",
      "Logistic Regression (4527/5000): loss=0.553110539012415\n",
      "Logistic Regression (4528/5000): loss=0.5692331637318412\n",
      "Logistic Regression (4529/5000): loss=0.5571678179335915\n",
      "Logistic Regression (4530/5000): loss=0.5496545727377565\n",
      "Logistic Regression (4531/5000): loss=0.5498468239494932\n",
      "Logistic Regression (4532/5000): loss=0.5555372286930977\n",
      "Logistic Regression (4533/5000): loss=0.5505571184416721\n",
      "Logistic Regression (4534/5000): loss=0.5513296818618588\n",
      "Logistic Regression (4535/5000): loss=0.5554948938556128\n",
      "Logistic Regression (4536/5000): loss=0.5555924296857894\n",
      "Logistic Regression (4537/5000): loss=0.5500736055542802\n",
      "Logistic Regression (4538/5000): loss=0.5552342696029697\n",
      "Logistic Regression (4539/5000): loss=0.5620511685870506\n",
      "Logistic Regression (4540/5000): loss=0.5527359568397134\n",
      "Logistic Regression (4541/5000): loss=0.5606066075404602\n",
      "Logistic Regression (4542/5000): loss=0.5499851705793769\n",
      "Logistic Regression (4543/5000): loss=0.549790291372795\n",
      "Logistic Regression (4544/5000): loss=0.5511709244220816\n",
      "Logistic Regression (4545/5000): loss=0.5496363211462116\n",
      "Logistic Regression (4546/5000): loss=0.5529909523196229\n",
      "Logistic Regression (4547/5000): loss=0.55181952901255\n",
      "Logistic Regression (4548/5000): loss=0.5518595447567333\n",
      "Logistic Regression (4549/5000): loss=0.5517623101560598\n",
      "Logistic Regression (4550/5000): loss=0.5496701706283237\n",
      "Logistic Regression (4551/5000): loss=0.5498643710441922\n",
      "Logistic Regression (4552/5000): loss=0.5502450481568776\n",
      "Logistic Regression (4553/5000): loss=0.5499023246394616\n",
      "Logistic Regression (4554/5000): loss=0.5648407310353579\n",
      "Logistic Regression (4555/5000): loss=0.5558180691331651\n",
      "Logistic Regression (4556/5000): loss=0.5568308272434959\n",
      "Logistic Regression (4557/5000): loss=0.5497897701776755\n",
      "Logistic Regression (4558/5000): loss=0.5495587243086398\n",
      "Logistic Regression (4559/5000): loss=0.570509050384361\n",
      "Logistic Regression (4560/5000): loss=0.5518664400544382\n",
      "Logistic Regression (4561/5000): loss=0.550010978279602\n",
      "Logistic Regression (4562/5000): loss=0.5497536874392024\n",
      "Logistic Regression (4563/5000): loss=0.5551931203808332\n",
      "Logistic Regression (4564/5000): loss=0.5496723834251364\n",
      "Logistic Regression (4565/5000): loss=0.554623067705109\n",
      "Logistic Regression (4566/5000): loss=0.5616229542915041\n",
      "Logistic Regression (4567/5000): loss=0.5567731567147413\n",
      "Logistic Regression (4568/5000): loss=0.5508796489157347\n",
      "Logistic Regression (4569/5000): loss=0.5529050385838997\n",
      "Logistic Regression (4570/5000): loss=0.5499393634052197\n",
      "Logistic Regression (4571/5000): loss=0.5495320581282745\n",
      "Logistic Regression (4572/5000): loss=0.5522977176479912\n",
      "Logistic Regression (4573/5000): loss=0.5530291673191362\n",
      "Logistic Regression (4574/5000): loss=0.5542978914952927\n",
      "Logistic Regression (4575/5000): loss=0.5501824940149042\n",
      "Logistic Regression (4576/5000): loss=0.5520780139685677\n",
      "Logistic Regression (4577/5000): loss=0.5499089564162338\n",
      "Logistic Regression (4578/5000): loss=0.5518889827510681\n",
      "Logistic Regression (4579/5000): loss=0.5503598243199821\n",
      "Logistic Regression (4580/5000): loss=0.554262648405238\n",
      "Logistic Regression (4581/5000): loss=0.5498043328805898\n",
      "Logistic Regression (4582/5000): loss=0.5501304334185443\n",
      "Logistic Regression (4583/5000): loss=0.5508352062554205\n",
      "Logistic Regression (4584/5000): loss=0.5550132209152296\n",
      "Logistic Regression (4585/5000): loss=0.5536277110765797\n",
      "Logistic Regression (4586/5000): loss=0.5540231781716588\n",
      "Logistic Regression (4587/5000): loss=0.5517804324162799\n",
      "Logistic Regression (4588/5000): loss=0.5503039940715366\n",
      "Logistic Regression (4589/5000): loss=0.552899596710412\n",
      "Logistic Regression (4590/5000): loss=0.5499986102753841\n",
      "Logistic Regression (4591/5000): loss=0.5522991333812595\n",
      "Logistic Regression (4592/5000): loss=0.5573273513411716\n",
      "Logistic Regression (4593/5000): loss=0.5540142336651815\n",
      "Logistic Regression (4594/5000): loss=0.5525064181775446\n",
      "Logistic Regression (4595/5000): loss=0.5508231468249696\n",
      "Logistic Regression (4596/5000): loss=0.5547912888512887\n",
      "Logistic Regression (4597/5000): loss=0.5518445319551156\n",
      "Logistic Regression (4598/5000): loss=0.5512886008078544\n",
      "Logistic Regression (4599/5000): loss=0.5496188987583099\n",
      "Logistic Regression (4600/5000): loss=0.5544502827424457\n",
      "Logistic Regression (4601/5000): loss=0.5556579317630395\n",
      "Logistic Regression (4602/5000): loss=0.5504726639757613\n",
      "Logistic Regression (4603/5000): loss=0.5494399472770511\n",
      "Logistic Regression (4604/5000): loss=0.5494376372041637\n",
      "Logistic Regression (4605/5000): loss=0.550108609244483\n",
      "Logistic Regression (4606/5000): loss=0.5505396220513713\n",
      "Logistic Regression (4607/5000): loss=0.5496748213738123\n",
      "Logistic Regression (4608/5000): loss=0.5538632440877226\n",
      "Logistic Regression (4609/5000): loss=0.549622349277858\n",
      "Logistic Regression (4610/5000): loss=0.5558961666970929\n",
      "Logistic Regression (4611/5000): loss=0.5581923065406804\n",
      "Logistic Regression (4612/5000): loss=0.5601768629452535\n",
      "Logistic Regression (4613/5000): loss=0.5553303919925622\n",
      "Logistic Regression (4614/5000): loss=0.5590276616883445\n",
      "Logistic Regression (4615/5000): loss=0.5508135845100166\n",
      "Logistic Regression (4616/5000): loss=0.5520747312442827\n",
      "Logistic Regression (4617/5000): loss=0.5543563402636232\n",
      "Logistic Regression (4618/5000): loss=0.5503761791299373\n",
      "Logistic Regression (4619/5000): loss=0.5580929846734822\n",
      "Logistic Regression (4620/5000): loss=0.5517782019143616\n",
      "Logistic Regression (4621/5000): loss=0.5497201351931411\n",
      "Logistic Regression (4622/5000): loss=0.5547357620237247\n",
      "Logistic Regression (4623/5000): loss=0.5503552350324663\n",
      "Logistic Regression (4624/5000): loss=0.5531258971404924\n",
      "Logistic Regression (4625/5000): loss=0.5493690431812615\n",
      "Logistic Regression (4626/5000): loss=0.5761921025581245\n",
      "Logistic Regression (4627/5000): loss=0.558429294616482\n",
      "Logistic Regression (4628/5000): loss=0.5497015484594183\n",
      "Logistic Regression (4629/5000): loss=0.5500618939377502\n",
      "Logistic Regression (4630/5000): loss=0.5496972245910944\n",
      "Logistic Regression (4631/5000): loss=0.5529024333837836\n",
      "Logistic Regression (4632/5000): loss=0.5559131415044549\n",
      "Logistic Regression (4633/5000): loss=0.5518668688969487\n",
      "Logistic Regression (4634/5000): loss=0.553779991829138\n",
      "Logistic Regression (4635/5000): loss=0.5504405517900565\n",
      "Logistic Regression (4636/5000): loss=0.550451870743756\n",
      "Logistic Regression (4637/5000): loss=0.5518532624749494\n",
      "Logistic Regression (4638/5000): loss=0.5504283814990484\n",
      "Logistic Regression (4639/5000): loss=0.5507341057189927\n",
      "Logistic Regression (4640/5000): loss=0.5580922453408426\n",
      "Logistic Regression (4641/5000): loss=0.5513013463363056\n",
      "Logistic Regression (4642/5000): loss=0.5493947467344741\n",
      "Logistic Regression (4643/5000): loss=0.549212050860133\n",
      "Logistic Regression (4644/5000): loss=0.5523888084855351\n",
      "Logistic Regression (4645/5000): loss=0.5492571744440407\n",
      "Logistic Regression (4646/5000): loss=0.5492144714247211\n",
      "Logistic Regression (4647/5000): loss=0.5496261552426907\n",
      "Logistic Regression (4648/5000): loss=0.5560863599950857\n",
      "Logistic Regression (4649/5000): loss=0.555737759369544\n",
      "Logistic Regression (4650/5000): loss=0.5491482192741807\n",
      "Logistic Regression (4651/5000): loss=0.550096508640981\n",
      "Logistic Regression (4652/5000): loss=0.5542216037715566\n",
      "Logistic Regression (4653/5000): loss=0.5494663444036112\n",
      "Logistic Regression (4654/5000): loss=0.5549086784292192\n",
      "Logistic Regression (4655/5000): loss=0.5547745013668661\n",
      "Logistic Regression (4656/5000): loss=0.5521207762208016\n",
      "Logistic Regression (4657/5000): loss=0.5511589824100697\n",
      "Logistic Regression (4658/5000): loss=0.5495555789703563\n",
      "Logistic Regression (4659/5000): loss=0.5529092565021635\n",
      "Logistic Regression (4660/5000): loss=0.5501084886240521\n",
      "Logistic Regression (4661/5000): loss=0.5504708623021473\n",
      "Logistic Regression (4662/5000): loss=0.5499118842296328\n",
      "Logistic Regression (4663/5000): loss=0.5497169362201442\n",
      "Logistic Regression (4664/5000): loss=0.552556519953626\n",
      "Logistic Regression (4665/5000): loss=0.5507638132927509\n",
      "Logistic Regression (4666/5000): loss=0.5491621170046367\n",
      "Logistic Regression (4667/5000): loss=0.5537865068743089\n",
      "Logistic Regression (4668/5000): loss=0.5495804674630488\n",
      "Logistic Regression (4669/5000): loss=0.5494711480846372\n",
      "Logistic Regression (4670/5000): loss=0.5583497310414308\n",
      "Logistic Regression (4671/5000): loss=0.5515048445705105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (4672/5000): loss=0.5493629477905045\n",
      "Logistic Regression (4673/5000): loss=0.5506168987802492\n",
      "Logistic Regression (4674/5000): loss=0.5723570228888732\n",
      "Logistic Regression (4675/5000): loss=0.5502081163530294\n",
      "Logistic Regression (4676/5000): loss=0.5586828028274647\n",
      "Logistic Regression (4677/5000): loss=0.5645961797732483\n",
      "Logistic Regression (4678/5000): loss=0.5529048363118997\n",
      "Logistic Regression (4679/5000): loss=0.5566253309870859\n",
      "Logistic Regression (4680/5000): loss=0.5572352184845742\n",
      "Logistic Regression (4681/5000): loss=0.549084511714951\n",
      "Logistic Regression (4682/5000): loss=0.5569981249807697\n",
      "Logistic Regression (4683/5000): loss=0.5542747559876225\n",
      "Logistic Regression (4684/5000): loss=0.5537403825066751\n",
      "Logistic Regression (4685/5000): loss=0.5561036852911785\n",
      "Logistic Regression (4686/5000): loss=0.5491479199791707\n",
      "Logistic Regression (4687/5000): loss=0.5496211378954301\n",
      "Logistic Regression (4688/5000): loss=0.5589554898071969\n",
      "Logistic Regression (4689/5000): loss=0.5545096224059092\n",
      "Logistic Regression (4690/5000): loss=0.5496233576397451\n",
      "Logistic Regression (4691/5000): loss=0.5537798080961149\n",
      "Logistic Regression (4692/5000): loss=0.5561505152920974\n",
      "Logistic Regression (4693/5000): loss=0.5491218425558999\n",
      "Logistic Regression (4694/5000): loss=0.5511088634952477\n",
      "Logistic Regression (4695/5000): loss=0.5616728860042313\n",
      "Logistic Regression (4696/5000): loss=0.5502288361140394\n",
      "Logistic Regression (4697/5000): loss=0.5491837309664683\n",
      "Logistic Regression (4698/5000): loss=0.5490455080768788\n",
      "Logistic Regression (4699/5000): loss=0.5502211856504073\n",
      "Logistic Regression (4700/5000): loss=0.5490744195744915\n",
      "Logistic Regression (4701/5000): loss=0.5528312925643469\n",
      "Logistic Regression (4702/5000): loss=0.5499834611966503\n",
      "Logistic Regression (4703/5000): loss=0.5515449106352956\n",
      "Logistic Regression (4704/5000): loss=0.5684448143089301\n",
      "Logistic Regression (4705/5000): loss=0.5500492227075603\n",
      "Logistic Regression (4706/5000): loss=0.5519785080905609\n",
      "Logistic Regression (4707/5000): loss=0.5492294406327852\n",
      "Logistic Regression (4708/5000): loss=0.5614649851460048\n",
      "Logistic Regression (4709/5000): loss=0.5513811996377752\n",
      "Logistic Regression (4710/5000): loss=0.5904795441049377\n",
      "Logistic Regression (4711/5000): loss=0.5537994765150822\n",
      "Logistic Regression (4712/5000): loss=0.5507061724829554\n",
      "Logistic Regression (4713/5000): loss=0.5499476781331154\n",
      "Logistic Regression (4714/5000): loss=0.5598081415031135\n",
      "Logistic Regression (4715/5000): loss=0.5495193661341419\n",
      "Logistic Regression (4716/5000): loss=0.5490352090090376\n",
      "Logistic Regression (4717/5000): loss=0.5501882851570978\n",
      "Logistic Regression (4718/5000): loss=0.5494119805696818\n",
      "Logistic Regression (4719/5000): loss=0.5515567544372315\n",
      "Logistic Regression (4720/5000): loss=0.5550349772827354\n",
      "Logistic Regression (4721/5000): loss=0.55005312668258\n",
      "Logistic Regression (4722/5000): loss=0.5493875449656471\n",
      "Logistic Regression (4723/5000): loss=0.5504431495199938\n",
      "Logistic Regression (4724/5000): loss=0.5609813437116731\n",
      "Logistic Regression (4725/5000): loss=0.5535810939364929\n",
      "Logistic Regression (4726/5000): loss=0.5494011545528928\n",
      "Logistic Regression (4727/5000): loss=0.5495430423538451\n",
      "Logistic Regression (4728/5000): loss=0.5693418127554499\n",
      "Logistic Regression (4729/5000): loss=0.5594220549944062\n",
      "Logistic Regression (4730/5000): loss=0.5672497176524431\n",
      "Logistic Regression (4731/5000): loss=0.5506343885736498\n",
      "Logistic Regression (4732/5000): loss=0.5500453792093618\n",
      "Logistic Regression (4733/5000): loss=0.5528015989615854\n",
      "Logistic Regression (4734/5000): loss=0.552377468850783\n",
      "Logistic Regression (4735/5000): loss=0.5502985870047526\n",
      "Logistic Regression (4736/5000): loss=0.5550479902088841\n",
      "Logistic Regression (4737/5000): loss=0.5742141594362081\n",
      "Logistic Regression (4738/5000): loss=0.5617655283936274\n",
      "Logistic Regression (4739/5000): loss=0.5491660002391631\n",
      "Logistic Regression (4740/5000): loss=0.5492637541690439\n",
      "Logistic Regression (4741/5000): loss=0.5525615282941913\n",
      "Logistic Regression (4742/5000): loss=0.5502781353747785\n",
      "Logistic Regression (4743/5000): loss=0.550595446296262\n",
      "Logistic Regression (4744/5000): loss=0.5560773588589376\n",
      "Logistic Regression (4745/5000): loss=0.5518577320239998\n",
      "Logistic Regression (4746/5000): loss=0.551855047017063\n",
      "Logistic Regression (4747/5000): loss=0.549554038389405\n",
      "Logistic Regression (4748/5000): loss=0.5493331731025124\n",
      "Logistic Regression (4749/5000): loss=0.5492607139608409\n",
      "Logistic Regression (4750/5000): loss=0.5496360711206557\n",
      "Logistic Regression (4751/5000): loss=0.5545211294819191\n",
      "Logistic Regression (4752/5000): loss=0.5525707368212401\n",
      "Logistic Regression (4753/5000): loss=0.5488960519972489\n",
      "Logistic Regression (4754/5000): loss=0.5506259573116352\n",
      "Logistic Regression (4755/5000): loss=0.5487399443124878\n",
      "Logistic Regression (4756/5000): loss=0.5619248885627259\n",
      "Logistic Regression (4757/5000): loss=0.5737113295809837\n",
      "Logistic Regression (4758/5000): loss=0.5546065665287454\n",
      "Logistic Regression (4759/5000): loss=0.5493570727861024\n",
      "Logistic Regression (4760/5000): loss=0.5577737693669764\n",
      "Logistic Regression (4761/5000): loss=0.5581082044293162\n",
      "Logistic Regression (4762/5000): loss=0.5492437461689444\n",
      "Logistic Regression (4763/5000): loss=0.5512206730392339\n",
      "Logistic Regression (4764/5000): loss=0.5512694236796443\n",
      "Logistic Regression (4765/5000): loss=0.5505360034341618\n",
      "Logistic Regression (4766/5000): loss=0.5522363470112515\n",
      "Logistic Regression (4767/5000): loss=0.5486908900297118\n",
      "Logistic Regression (4768/5000): loss=0.5532614254987472\n",
      "Logistic Regression (4769/5000): loss=0.5489644278891571\n",
      "Logistic Regression (4770/5000): loss=0.5556895807815433\n",
      "Logistic Regression (4771/5000): loss=0.5604310638350766\n",
      "Logistic Regression (4772/5000): loss=0.5493233685368359\n",
      "Logistic Regression (4773/5000): loss=0.5506681462871211\n",
      "Logistic Regression (4774/5000): loss=0.549289582567023\n",
      "Logistic Regression (4775/5000): loss=0.5491803709252816\n",
      "Logistic Regression (4776/5000): loss=0.549099443735956\n",
      "Logistic Regression (4777/5000): loss=0.5525286441736443\n",
      "Logistic Regression (4778/5000): loss=0.5505994312510293\n",
      "Logistic Regression (4779/5000): loss=0.5541698485609323\n",
      "Logistic Regression (4780/5000): loss=0.5515597773222207\n",
      "Logistic Regression (4781/5000): loss=0.548734827470053\n",
      "Logistic Regression (4782/5000): loss=0.5506302453641178\n",
      "Logistic Regression (4783/5000): loss=0.5492254209627012\n",
      "Logistic Regression (4784/5000): loss=0.5490542432330775\n",
      "Logistic Regression (4785/5000): loss=0.5489248523537561\n",
      "Logistic Regression (4786/5000): loss=0.5499900762597537\n",
      "Logistic Regression (4787/5000): loss=0.5498566117793348\n",
      "Logistic Regression (4788/5000): loss=0.5609201883746024\n",
      "Logistic Regression (4789/5000): loss=0.5533533223287322\n",
      "Logistic Regression (4790/5000): loss=0.5660342045923363\n",
      "Logistic Regression (4791/5000): loss=0.5513138221581418\n",
      "Logistic Regression (4792/5000): loss=0.5562055100216411\n",
      "Logistic Regression (4793/5000): loss=0.5502563385972635\n",
      "Logistic Regression (4794/5000): loss=0.5497285437280554\n",
      "Logistic Regression (4795/5000): loss=0.5541517489013853\n",
      "Logistic Regression (4796/5000): loss=0.5678062333353074\n",
      "Logistic Regression (4797/5000): loss=0.5504646491601273\n",
      "Logistic Regression (4798/5000): loss=0.5507471941062353\n",
      "Logistic Regression (4799/5000): loss=0.550637200352483\n",
      "Logistic Regression (4800/5000): loss=0.5513431936681835\n",
      "Logistic Regression (4801/5000): loss=0.5536634156838098\n",
      "Logistic Regression (4802/5000): loss=0.5491958458320373\n",
      "Logistic Regression (4803/5000): loss=0.5491025572909\n",
      "Logistic Regression (4804/5000): loss=0.5606032451019455\n",
      "Logistic Regression (4805/5000): loss=0.5561071040766039\n",
      "Logistic Regression (4806/5000): loss=0.5548621892923219\n",
      "Logistic Regression (4807/5000): loss=0.5498673858076107\n",
      "Logistic Regression (4808/5000): loss=0.5635485782293177\n",
      "Logistic Regression (4809/5000): loss=0.5486254419603528\n",
      "Logistic Regression (4810/5000): loss=0.5492392158389806\n",
      "Logistic Regression (4811/5000): loss=0.5538940300071025\n",
      "Logistic Regression (4812/5000): loss=0.551554211722649\n",
      "Logistic Regression (4813/5000): loss=0.5546220971771838\n",
      "Logistic Regression (4814/5000): loss=0.5489347046796977\n",
      "Logistic Regression (4815/5000): loss=0.5505201185910402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (4816/5000): loss=0.5485099084201795\n",
      "Logistic Regression (4817/5000): loss=0.5516188490764157\n",
      "Logistic Regression (4818/5000): loss=0.5493355453725859\n",
      "Logistic Regression (4819/5000): loss=0.550467589906189\n",
      "Logistic Regression (4820/5000): loss=0.5548865067670383\n",
      "Logistic Regression (4821/5000): loss=0.5515026401533867\n",
      "Logistic Regression (4822/5000): loss=0.5511698813594622\n",
      "Logistic Regression (4823/5000): loss=0.5514275863816921\n",
      "Logistic Regression (4824/5000): loss=0.5500432641622184\n",
      "Logistic Regression (4825/5000): loss=0.5487237015685901\n",
      "Logistic Regression (4826/5000): loss=0.5487242099658645\n",
      "Logistic Regression (4827/5000): loss=0.5582248291760529\n",
      "Logistic Regression (4828/5000): loss=0.5489022600030018\n",
      "Logistic Regression (4829/5000): loss=0.5512718841346302\n",
      "Logistic Regression (4830/5000): loss=0.5522761634648874\n",
      "Logistic Regression (4831/5000): loss=0.5507706921428366\n",
      "Logistic Regression (4832/5000): loss=0.5538702204324311\n",
      "Logistic Regression (4833/5000): loss=0.5506827476193684\n",
      "Logistic Regression (4834/5000): loss=0.5489877387875042\n",
      "Logistic Regression (4835/5000): loss=0.5495029918053259\n",
      "Logistic Regression (4836/5000): loss=0.5496353269554376\n",
      "Logistic Regression (4837/5000): loss=0.549461763903669\n",
      "Logistic Regression (4838/5000): loss=0.551312507228454\n",
      "Logistic Regression (4839/5000): loss=0.5577648893377277\n",
      "Logistic Regression (4840/5000): loss=0.5510775752606243\n",
      "Logistic Regression (4841/5000): loss=0.5512802057834474\n",
      "Logistic Regression (4842/5000): loss=0.5494446164480351\n",
      "Logistic Regression (4843/5000): loss=0.5511404813811434\n",
      "Logistic Regression (4844/5000): loss=0.5591530318956215\n",
      "Logistic Regression (4845/5000): loss=0.5497567694242763\n",
      "Logistic Regression (4846/5000): loss=0.5509708169795311\n",
      "Logistic Regression (4847/5000): loss=0.5487195703520867\n",
      "Logistic Regression (4848/5000): loss=0.5538957516306844\n",
      "Logistic Regression (4849/5000): loss=0.5495916596348854\n",
      "Logistic Regression (4850/5000): loss=0.5490107260360533\n",
      "Logistic Regression (4851/5000): loss=0.5573595203002968\n",
      "Logistic Regression (4852/5000): loss=0.5490212683582822\n",
      "Logistic Regression (4853/5000): loss=0.558132183694068\n",
      "Logistic Regression (4854/5000): loss=0.5517985675961528\n",
      "Logistic Regression (4855/5000): loss=0.5484161079127242\n",
      "Logistic Regression (4856/5000): loss=0.5509412809369951\n",
      "Logistic Regression (4857/5000): loss=0.5486668104634571\n",
      "Logistic Regression (4858/5000): loss=0.5485417673895798\n",
      "Logistic Regression (4859/5000): loss=0.5487668905736517\n",
      "Logistic Regression (4860/5000): loss=0.5505935373719534\n",
      "Logistic Regression (4861/5000): loss=0.5488534420098226\n",
      "Logistic Regression (4862/5000): loss=0.551497983710755\n",
      "Logistic Regression (4863/5000): loss=0.5588061021185093\n",
      "Logistic Regression (4864/5000): loss=0.5513925902836609\n",
      "Logistic Regression (4865/5000): loss=0.5793345997412117\n",
      "Logistic Regression (4866/5000): loss=0.5513130693604539\n",
      "Logistic Regression (4867/5000): loss=0.5494664259244442\n",
      "Logistic Regression (4868/5000): loss=0.5506316168191882\n",
      "Logistic Regression (4869/5000): loss=0.5558872283711993\n",
      "Logistic Regression (4870/5000): loss=0.5510501907166963\n",
      "Logistic Regression (4871/5000): loss=0.5533996558304152\n",
      "Logistic Regression (4872/5000): loss=0.5489186743165995\n",
      "Logistic Regression (4873/5000): loss=0.54987929884809\n",
      "Logistic Regression (4874/5000): loss=0.5501123723166941\n",
      "Logistic Regression (4875/5000): loss=0.5507265016258799\n",
      "Logistic Regression (4876/5000): loss=0.5553118816324916\n",
      "Logistic Regression (4877/5000): loss=0.5502274978833631\n",
      "Logistic Regression (4878/5000): loss=0.5509649443534238\n",
      "Logistic Regression (4879/5000): loss=0.5492834628176222\n",
      "Logistic Regression (4880/5000): loss=0.552107289732698\n",
      "Logistic Regression (4881/5000): loss=0.5515208411540642\n",
      "Logistic Regression (4882/5000): loss=0.5494776375191104\n",
      "Logistic Regression (4883/5000): loss=0.5497568602407499\n",
      "Logistic Regression (4884/5000): loss=0.5585500819178385\n",
      "Logistic Regression (4885/5000): loss=0.5509881285720641\n",
      "Logistic Regression (4886/5000): loss=0.5556309145067313\n",
      "Logistic Regression (4887/5000): loss=0.5505977365388132\n",
      "Logistic Regression (4888/5000): loss=0.5491981629804947\n",
      "Logistic Regression (4889/5000): loss=0.5496586876957759\n",
      "Logistic Regression (4890/5000): loss=0.5496929179727817\n",
      "Logistic Regression (4891/5000): loss=0.5489136869871886\n",
      "Logistic Regression (4892/5000): loss=0.5537014279927638\n",
      "Logistic Regression (4893/5000): loss=0.5493751589247537\n",
      "Logistic Regression (4894/5000): loss=0.5519142635357287\n",
      "Logistic Regression (4895/5000): loss=0.5513416069590509\n",
      "Logistic Regression (4896/5000): loss=0.5488669964978978\n",
      "Logistic Regression (4897/5000): loss=0.5489161340188952\n",
      "Logistic Regression (4898/5000): loss=0.5521660448688898\n",
      "Logistic Regression (4899/5000): loss=0.548936577378791\n",
      "Logistic Regression (4900/5000): loss=0.5488548880466406\n",
      "Logistic Regression (4901/5000): loss=0.5489164979034586\n",
      "Logistic Regression (4902/5000): loss=0.5491809503016878\n",
      "Logistic Regression (4903/5000): loss=0.5530320943383225\n",
      "Logistic Regression (4904/5000): loss=0.5483022492060426\n",
      "Logistic Regression (4905/5000): loss=0.552917728525927\n",
      "Logistic Regression (4906/5000): loss=0.5482813788548286\n",
      "Logistic Regression (4907/5000): loss=0.5548368848309342\n",
      "Logistic Regression (4908/5000): loss=0.5482033457449625\n",
      "Logistic Regression (4909/5000): loss=0.5504958037179748\n",
      "Logistic Regression (4910/5000): loss=0.5482487306234232\n",
      "Logistic Regression (4911/5000): loss=0.551346650984455\n",
      "Logistic Regression (4912/5000): loss=0.5497633693440701\n",
      "Logistic Regression (4913/5000): loss=0.5504525577254235\n",
      "Logistic Regression (4914/5000): loss=0.548895787205912\n",
      "Logistic Regression (4915/5000): loss=0.5513262330354718\n",
      "Logistic Regression (4916/5000): loss=0.549214203826249\n",
      "Logistic Regression (4917/5000): loss=0.5547278928620731\n",
      "Logistic Regression (4918/5000): loss=0.5589232319869448\n",
      "Logistic Regression (4919/5000): loss=0.5492273227854273\n",
      "Logistic Regression (4920/5000): loss=0.5556442091482966\n",
      "Logistic Regression (4921/5000): loss=0.5497225282526186\n",
      "Logistic Regression (4922/5000): loss=0.5489262393709584\n",
      "Logistic Regression (4923/5000): loss=0.5484250139420646\n",
      "Logistic Regression (4924/5000): loss=0.550314575582068\n",
      "Logistic Regression (4925/5000): loss=0.5503237733283015\n",
      "Logistic Regression (4926/5000): loss=0.5499929970445289\n",
      "Logistic Regression (4927/5000): loss=0.5490789671693188\n",
      "Logistic Regression (4928/5000): loss=0.548305265681532\n",
      "Logistic Regression (4929/5000): loss=0.5487270928350927\n",
      "Logistic Regression (4930/5000): loss=0.5482495025955715\n",
      "Logistic Regression (4931/5000): loss=0.5501031114012588\n",
      "Logistic Regression (4932/5000): loss=0.5481515942334428\n",
      "Logistic Regression (4933/5000): loss=0.5480900705889119\n",
      "Logistic Regression (4934/5000): loss=0.548115280463261\n",
      "Logistic Regression (4935/5000): loss=0.5482328839632169\n",
      "Logistic Regression (4936/5000): loss=0.5512663644918041\n",
      "Logistic Regression (4937/5000): loss=0.5481833557769361\n",
      "Logistic Regression (4938/5000): loss=0.5573271911972147\n",
      "Logistic Regression (4939/5000): loss=0.5480605903610368\n",
      "Logistic Regression (4940/5000): loss=0.5491074260166041\n",
      "Logistic Regression (4941/5000): loss=0.5487955440482105\n",
      "Logistic Regression (4942/5000): loss=0.5487838920506583\n",
      "Logistic Regression (4943/5000): loss=0.5482680806617652\n",
      "Logistic Regression (4944/5000): loss=0.549380081124709\n",
      "Logistic Regression (4945/5000): loss=0.5480393656267928\n",
      "Logistic Regression (4946/5000): loss=0.5481434249101829\n",
      "Logistic Regression (4947/5000): loss=0.562294026480736\n",
      "Logistic Regression (4948/5000): loss=0.5510664130652464\n",
      "Logistic Regression (4949/5000): loss=0.5516393282659219\n",
      "Logistic Regression (4950/5000): loss=0.5670457604834149\n",
      "Logistic Regression (4951/5000): loss=0.5487492502519141\n",
      "Logistic Regression (4952/5000): loss=0.549408888009904\n",
      "Logistic Regression (4953/5000): loss=0.5494989532884521\n",
      "Logistic Regression (4954/5000): loss=0.5482269571954224\n",
      "Logistic Regression (4955/5000): loss=0.5509010761388151\n",
      "Logistic Regression (4956/5000): loss=0.5487601221069481\n",
      "Logistic Regression (4957/5000): loss=0.5483996105576696\n",
      "Logistic Regression (4958/5000): loss=0.5482487847954679\n",
      "Logistic Regression (4959/5000): loss=0.549090397206649\n",
      "Logistic Regression (4960/5000): loss=0.5513993967268193\n",
      "Logistic Regression (4961/5000): loss=0.5613108298094737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (4962/5000): loss=0.5481353315327202\n",
      "Logistic Regression (4963/5000): loss=0.5484503295574309\n",
      "Logistic Regression (4964/5000): loss=0.5548326231726877\n",
      "Logistic Regression (4965/5000): loss=0.5494945264793896\n",
      "Logistic Regression (4966/5000): loss=0.5643172977848158\n",
      "Logistic Regression (4967/5000): loss=0.5483178760539762\n",
      "Logistic Regression (4968/5000): loss=0.5484028368489268\n",
      "Logistic Regression (4969/5000): loss=0.549125350290542\n",
      "Logistic Regression (4970/5000): loss=0.5537826341549295\n",
      "Logistic Regression (4971/5000): loss=0.5637057255099651\n",
      "Logistic Regression (4972/5000): loss=0.5537675560434356\n",
      "Logistic Regression (4973/5000): loss=0.5501433890882863\n",
      "Logistic Regression (4974/5000): loss=0.5514630001748972\n",
      "Logistic Regression (4975/5000): loss=0.5613261022998631\n",
      "Logistic Regression (4976/5000): loss=0.5484659064737453\n",
      "Logistic Regression (4977/5000): loss=0.5506927725603705\n",
      "Logistic Regression (4978/5000): loss=0.5494208292066723\n",
      "Logistic Regression (4979/5000): loss=0.548416566442411\n",
      "Logistic Regression (4980/5000): loss=0.551363663426627\n",
      "Logistic Regression (4981/5000): loss=0.5492633367893017\n",
      "Logistic Regression (4982/5000): loss=0.5617063059048539\n",
      "Logistic Regression (4983/5000): loss=0.5488603156048371\n",
      "Logistic Regression (4984/5000): loss=0.5487271555439653\n",
      "Logistic Regression (4985/5000): loss=0.5539483914362242\n",
      "Logistic Regression (4986/5000): loss=0.56026762689215\n",
      "Logistic Regression (4987/5000): loss=0.552930435174233\n",
      "Logistic Regression (4988/5000): loss=0.5486617722577862\n",
      "Logistic Regression (4989/5000): loss=0.5508482315472247\n",
      "Logistic Regression (4990/5000): loss=0.5480154131411039\n",
      "Logistic Regression (4991/5000): loss=0.5483274429921934\n",
      "Logistic Regression (4992/5000): loss=0.5484677354012795\n",
      "Logistic Regression (4993/5000): loss=0.5511747568407507\n",
      "Logistic Regression (4994/5000): loss=0.5484122223780611\n",
      "Logistic Regression (4995/5000): loss=0.5501242961714894\n",
      "Logistic Regression (4996/5000): loss=0.5496241410726883\n",
      "Logistic Regression (4997/5000): loss=0.5538500644205766\n",
      "Logistic Regression (4998/5000): loss=0.5638359957672755\n",
      "Logistic Regression (4999/5000): loss=0.5519994991674588\n",
      "Logistic Regression (5000/5000): loss=0.5571607067461251\n",
      "0.5571607067461251\n"
     ]
    }
   ],
   "source": [
    "from ML_methods import *\n",
    "\n",
    "initial_w = np.zeros(tX.shape[1])\n",
    "max_iters = 5000\n",
    "gamma = 1e-8\n",
    "\n",
    "y_tr_log_reg = y_tr>0\n",
    "\n",
    "\n",
    "w, mse = logistic_regression(y_tr_log_reg, x_tr, initial_w, max_iters, gamma, batch_size=64)\n",
    "\n",
    "\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.699"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, x_te)\n",
    "np.mean(y_te.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J34FUFP-mN8q"
   },
   "source": [
    "## Regularized Logistic Regression Regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yscGLE8NmN8r",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg Log Regression(1/5000): Loss=0.6281272880714616 l1=0.6281272362061958 l2=5.186526578202607e-08\n",
      "Reg Log Regression(2/5000): Loss=0.6294332782321886 l1=0.6294332432678932 l2=3.4964295495014173e-08\n",
      "Reg Log Regression(3/5000): Loss=0.6278865184718415 l1=0.6278864726851018 l2=4.5786739692091594e-08\n",
      "Reg Log Regression(4/5000): Loss=0.6515175678751645 l1=0.6515175492211825 l2=1.8653981869900034e-08\n",
      "Reg Log Regression(5/5000): Loss=0.6259400820805825 l1=0.6259400163345908 l2=6.574599164179404e-08\n",
      "Reg Log Regression(6/5000): Loss=0.6260253907263215 l1=0.626025310499011 l2=8.022731041173114e-08\n",
      "Reg Log Regression(7/5000): Loss=0.6527957370413755 l1=0.6527957108971947 l2=2.6144180795151187e-08\n",
      "Reg Log Regression(8/5000): Loss=0.6223232587236339 l1=0.6223231743777499 l2=8.434588403420909e-08\n",
      "Reg Log Regression(9/5000): Loss=0.621859415467937 l1=0.6218593442893074 l2=7.117862949776011e-08\n",
      "Reg Log Regression(10/5000): Loss=0.6281308077782765 l1=0.62813075332093 l2=5.445734640213624e-08\n",
      "Reg Log Regression(11/5000): Loss=0.6254507048961956 l1=0.6254506305694152 l2=7.432678044584586e-08\n",
      "Reg Log Regression(12/5000): Loss=0.6206364869028619 l1=0.6206364009421705 l2=8.596069136352246e-08\n",
      "Reg Log Regression(13/5000): Loss=0.6185563627963935 l1=0.6185562614018882 l2=1.013945052879776e-07\n",
      "Reg Log Regression(14/5000): Loss=0.6205379144990746 l1=0.6205378108610031 l2=1.0363807144692266e-07\n",
      "Reg Log Regression(15/5000): Loss=0.6201644291880021 l1=0.6201642802843947 l2=1.4890360732503817e-07\n",
      "Reg Log Regression(16/5000): Loss=0.6170451648115852 l1=0.6170450310608719 l2=1.337507132378729e-07\n",
      "Reg Log Regression(17/5000): Loss=0.6178063100598571 l1=0.6178061644955123 l2=1.455643447802018e-07\n",
      "Reg Log Regression(18/5000): Loss=0.6173032794625806 l1=0.6173031267977034 l2=1.5266487716065566e-07\n"
     ]
    }
   ],
   "source": [
    "from ML_methods import *\n",
    "\n",
    "initial_w = np.zeros(tX.shape[1])\n",
    "y_tr_reg_log_reg = y_tr>0\n",
    "\n",
    "max_iters = 5000\n",
    "gamma = 1e-8\n",
    "lambda_ = 1\n",
    "\n",
    "w, mse = reg_logistic_regression(y_tr_reg_log_reg, x_tr, lambda_, initial_w, max_iters, gamma, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69624"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new way to check accuracy\n",
    "y_pred = predict_labels(w, x_te)\n",
    "np.mean(y_te.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vD3AinsF47z0"
   },
   "source": [
    "### Plotting a correlation matrix to see which of our features are highly correlated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SGDAAuBPtv65"
   },
   "outputs": [],
   "source": [
    "feat_corr_matrix = np.corrcoef(tX.T)\n",
    "#print (feat_corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "eISotaXU0EdB",
    "outputId": "5a73efad-59cc-46a1-945d-649daf654caf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd578117be0>"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAloAAAH+CAYAAABEPqCPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERENrSAIgiAIghERDa0gCIIgCIIR8S8Hee51mLcWFAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 792x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(feat_corr_matrix, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(feat_corr_matrix, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D932-XqEmN7d"
   },
   "source": [
    "### 1.1 Training using Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "a-9UyqRfmN7f",
    "outputId": "813a1730-7b2b-4525-b39a-b7ad20004da2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  0.3394801404386872 \n",
      "Testing MSE: 0.3405695670310389\n"
     ]
    }
   ],
   "source": [
    "# First Model using least squares\n",
    "\n",
    "weights = least_squares(y_tr, x_tr)\n",
    "MSE_tr = compute_loss(y_tr, x_tr, weights)\n",
    "MSE_te = compute_loss(y_te, x_te, weights)\n",
    "\n",
    "print(\"Training MSE: \", MSE_tr, \"\\nTesting MSE:\", MSE_te)\n",
    "#0.3394801404386906  0.34056956713540926"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1S2eqB6ImN7j"
   },
   "source": [
    "### 1.2 Prediction using Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "e-S_4tmjmN7k",
    "outputId": "650d49f8-3abc-4803-ae06-6d440b10f2eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  0.74242\n"
     ]
    }
   ],
   "source": [
    "# predict on the test data slice\n",
    "y_pred = predict_labels(weights, x_te)\n",
    "# Check accuracy\n",
    "print(\"Prediction Accuracy: \", np.mean(y_te==y_pred)) # 0.34396"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0HNjp0nCmN7n"
   },
   "source": [
    "# 2. Second Model: Pre-processing removing all NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x3bdTD3MmN7o"
   },
   "outputs": [],
   "source": [
    "# removing all nan but there actually are none\n",
    "tX_2 = tX[~np.isnan(tX).any(axis=1)]\n",
    "\n",
    "# removing all rows with -999, there are 181886\n",
    "y = y[np.all(tX_2 != -999, axis=1)]\n",
    "tX_2 = tX_2[np.all(tX_2 != -999, axis=1)]\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX_2, y, ratio, seed)\n",
    "\n",
    "# then to standardize x_tr and x_te\n",
    "x_tr = standardize(x_tr)[0]\n",
    "x_te = standardize(x_te)[0]\n",
    "\n",
    "# next to add a column of ones\n",
    "y_tr, x_tr = build_model_data(x_tr, y_tr)\n",
    "y_te, x_te = build_model_data(x_te, y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMhTcYnMmN7s"
   },
   "source": [
    "### 2.1 Training using Least Squares removing NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "wyj2uTf0mN7t",
    "outputId": "a85eaed2-13fe-4409-9072-fe8744c23952"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  0.3686416204656051 \n",
      "Testing MSE: 0.541116014511367\n"
     ]
    }
   ],
   "source": [
    "weights = least_squares(y_tr, x_tr)\n",
    "MSE_tr = compute_loss(y_tr, x_tr, weights)\n",
    "MSE_te = compute_loss(y_te, x_te, weights)\n",
    "\n",
    "print(\"Training MSE: \", MSE_tr, \"\\nTesting MSE:\", MSE_te) \n",
    "#0.36864162046559024 0.5411866496785176"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ncYqU-AkmN7w"
   },
   "source": [
    "### 2.2 Prediction using Least Squares removing NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Zg_AVDfnmN7x",
    "outputId": "de7487ed-df47-4141-9b4a-119dcf597241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  0.6120531454158409\n"
     ]
    }
   ],
   "source": [
    "# predict on the test data slice\n",
    "y_pred = predict_labels(weights, x_te)\n",
    "# Check accuracy\n",
    "print(\"Prediction Accuracy: \", np.mean(y_te==y_pred)) # 0.6121265506863393"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2nwYZm7CmN70"
   },
   "source": [
    "# 3. Third Model: Better Pre-processing replacing NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KTro2jBfmN72"
   },
   "outputs": [],
   "source": [
    "# replace nans with mean\n",
    "\n",
    "tX_3 = replace_nan_with_mean(tX, -999)\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX_3, y, ratio, seed)\n",
    "\n",
    "# then to standardize x_tr and x_te\n",
    "x_tr = standardize(x_tr)[0]\n",
    "x_te = standardize(x_te)[0]\n",
    "\n",
    "# next to add a column of ones\n",
    "y_tr, x_tr = build_model_data(x_tr, y_tr)\n",
    "y_te, x_te = build_model_data(x_te, y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWrmk8hCmN75"
   },
   "source": [
    "### 3.1 Training using Least Squares replacing NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z1itjN60mN76",
    "outputId": "6d9610fe-5706-4035-a30c-e64955ebe0ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  0.4978447945914067 \n",
      "Testing MSE: 0.6275084689089252\n"
     ]
    }
   ],
   "source": [
    "weights = least_squares(y_tr, x_tr)\n",
    "MSE_tr = compute_loss(y_tr, x_tr, weights)\n",
    "MSE_te = compute_loss(y_te, x_te, weights)\n",
    "\n",
    "print(\"Training MSE: \", MSE_tr, \"\\nTesting MSE:\", MSE_te) \n",
    "#0.4978447945914067  0.6275084689089252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjB1EK3DmN79"
   },
   "source": [
    "### 3.2 Prediction using Least Squares replacing NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p4c-LqCEmN7-",
    "outputId": "a3f02f2f-3305-4a7a-9a8e-c0f18f28ab85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  0.5331424796300375\n"
     ]
    }
   ],
   "source": [
    "# predict on the test data slice\n",
    "y_pred = predict_labels(weights, x_te)\n",
    "# Check accuracy\n",
    "print(\"Prediction Accuracy: \", np.mean(y_te==y_pred)) # 0.6121265506863393"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12zEyThcmN8B"
   },
   "source": [
    "# 4. Fourth Model: Feature Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ttr_E4CBmN8C"
   },
   "source": [
    "### 4. 1 Feature Augmentation on Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "99j_wg3xmN8D",
    "outputId": "9ddd4e39-9e8d-4838-d9b8-b328eafb6812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1th experiment, degree=1, rmse=0.8239257849571571\n",
      "Prediction Accuracy:  0.74402\n",
      "\n",
      "Processing 2th experiment, degree=2, rmse=0.7990539549852868\n",
      "Prediction Accuracy:  0.76944\n",
      "\n",
      "Processing 3th experiment, degree=3, rmse=0.788424663107187\n",
      "Prediction Accuracy:  0.77844\n",
      "\n",
      "Processing 4th experiment, degree=4, rmse=0.7786936013526474\n",
      "Prediction Accuracy:  0.7884\n",
      "\n",
      "Processing 5th experiment, degree=5, rmse=0.7979556716445746\n",
      "Prediction Accuracy:  0.7775\n",
      "\n",
      "Processing 6th experiment, degree=6, rmse=0.7649938635511518\n",
      "Prediction Accuracy:  0.79966\n",
      "\n",
      "Processing 7th experiment, degree=7, rmse=2893718.602071626\n",
      "Prediction Accuracy:  0.47306\n",
      "\n",
      "Processing 8th experiment, degree=8, rmse=2266262.8281303146\n",
      "Prediction Accuracy:  0.5427\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratio = 0.8\n",
    "seed = 1\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX, y, ratio, seed)\n",
    "\n",
    "degrees = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "\n",
    "for ind, degree in enumerate(degrees):\n",
    "\n",
    "    polynome = build_poly(x_tr,degree)\n",
    "    weights = least_squares(y_tr, polynome)\n",
    "    MSE = compute_loss(y_tr, polynome, weights)\n",
    "    rmse = np.sqrt(2*MSE)\n",
    "    \n",
    "    polynome_te = build_poly(x_te,degree)\n",
    "    y_pred = predict_labels(weights, polynome_te)\n",
    "\n",
    "    print(\"Processing {i}th experiment, degree={d}, rmse={loss}\".format( \n",
    "        i=ind + 1, d=degree, loss=rmse))\n",
    "    print(\"Prediction Accuracy: \", str(np.mean(y_te == y_pred)) + \"\\n\")\n",
    "    \n",
    "# Degree of 4 seems optimal (6 is overfitting it seems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vf-HClpPmN8G"
   },
   "source": [
    "### 4.2 Feature Augementation removing NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Rz2uGzMmN8H",
    "outputId": "3617c3e3-5800-4aab-fb99-a7d64e9f8cc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1th experiment, degree=1, rmse=0.8595472059393301\n",
      "Prediction Accuracy:  0.7209865668354988\n",
      "\n",
      "Processing 2th experiment, degree=2, rmse=0.8180119618944975\n",
      "Prediction Accuracy:  0.7653233502165455\n",
      "\n",
      "Processing 3th experiment, degree=3, rmse=0.7950250798192138\n",
      "Prediction Accuracy:  0.7857300154151068\n",
      "\n",
      "Processing 4th experiment, degree=4, rmse=0.7824540912580449\n",
      "Prediction Accuracy:  0.7980621008588417\n",
      "\n",
      "Processing 5th experiment, degree=5, rmse=0.7779028561821488\n",
      "Prediction Accuracy:  0.7998238273508038\n",
      "\n",
      "Processing 6th experiment, degree=6, rmse=0.7726834497676207\n",
      "Prediction Accuracy:  0.8031270645232328\n",
      "\n",
      "Processing 7th experiment, degree=7, rmse=0.7557874987888077\n",
      "Prediction Accuracy:  0.8123027233355354\n",
      "\n",
      "Processing 8th experiment, degree=8, rmse=0.7371216575803359\n",
      "Prediction Accuracy:  0.8225060559348161\n",
      "\n",
      "Processing 9th experiment, degree=9, rmse=0.7285225907993939\n",
      "Prediction Accuracy:  0.827350803787712\n",
      "\n",
      "Processing 10th experiment, degree=10, rmse=0.7278725366831396\n",
      "Prediction Accuracy:  0.827350803787712\n",
      "\n",
      "Processing 11th experiment, degree=11, rmse=0.8283684072325078\n",
      "Prediction Accuracy:  0.7668648608970124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratio = 0.8\n",
    "seed = 1\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX_2, y, ratio, seed)\n",
    "\n",
    "degrees = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "\n",
    "\n",
    "for ind, degree in enumerate(degrees):\n",
    "\n",
    "    polynome = build_poly(x_tr,degree)\n",
    "    weights = least_squares(y_tr, polynome)\n",
    "    MSE = compute_loss(y_tr, polynome, weights)\n",
    "    rmse = np.sqrt(2*MSE)\n",
    "    \n",
    "    polynome_te = build_poly(x_te,degree)\n",
    "    y_pred = predict_labels(weights, polynome_te)\n",
    "\n",
    "    print(\"Processing {i}th experiment, degree={d}, rmse={loss}\".format( \n",
    "        i=ind + 1, d=degree, loss=rmse))\n",
    "    print(\"Prediction Accuracy: \", str(np.mean(y_te == y_pred)) + \"\\n\")\n",
    "    \n",
    "# Degree of 9 seems optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2aEvtq_JmN8J"
   },
   "source": [
    "###  Feature Augementation replacing NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AN12G_8umN8K",
    "outputId": "1899e416-c13d-486b-9241-3efdbdb03309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1th experiment, degree=1, rmse=0.9978758293721322\n",
      "Prediction Accuracy:  0.5319679953020627\n",
      "\n",
      "Processing 2th experiment, degree=2, rmse=0.9977151297869898\n",
      "Prediction Accuracy:  0.5308669162445864\n",
      "\n",
      "Processing 3th experiment, degree=3, rmse=0.9974064350165527\n",
      "Prediction Accuracy:  0.5316009689495705\n",
      "\n",
      "Processing 4th experiment, degree=4, rmse=0.9970844513748015\n",
      "Prediction Accuracy:  0.5286647581296338\n",
      "\n",
      "Processing 5th experiment, degree=5, rmse=0.996862156198522\n",
      "Prediction Accuracy:  0.5277104896131543\n",
      "\n",
      "Processing 6th experiment, degree=6, rmse=0.9966915013548663\n",
      "Prediction Accuracy:  0.5262423842031858\n",
      "\n",
      "Processing 7th experiment, degree=7, rmse=0.9964604356881909\n",
      "Prediction Accuracy:  0.52697643690817\n",
      "\n",
      "Processing 8th experiment, degree=8, rmse=0.9961572986651697\n",
      "Prediction Accuracy:  0.5287381634001321\n",
      "\n",
      "Processing 9th experiment, degree=9, rmse=0.9959695949502358\n",
      "Prediction Accuracy:  0.52697643690817\n",
      "\n",
      "Processing 10th experiment, degree=10, rmse=0.9955706395713501\n",
      "Prediction Accuracy:  0.5269030316376716\n",
      "\n",
      "Processing 11th experiment, degree=11, rmse=0.9960747334508928\n",
      "Prediction Accuracy:  0.5266094105556779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratio = 0.8\n",
    "seed = 1\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX_3, y, ratio, seed)\n",
    "\n",
    "degrees = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "\n",
    "\n",
    "for ind, degree in enumerate(degrees):\n",
    "\n",
    "    polynome = build_poly(x_tr,degree)\n",
    "    weights = least_squares(y_tr, polynome)\n",
    "    MSE = compute_loss(y_tr, polynome, weights)\n",
    "    rmse = np.sqrt(2*MSE)\n",
    "    \n",
    "    polynome_te = build_poly(x_te,degree)\n",
    "    y_pred = predict_labels(weights, polynome_te)\n",
    "\n",
    "    print(\"Processing {i}th experiment, degree={d}, rmse={loss}\".format( \n",
    "        i=ind + 1, d=degree, loss=rmse))\n",
    "    print(\"Prediction Accuracy: \", str(np.mean(y_te == y_pred)) + \"\\n\")\n",
    "    \n",
    "# Degree of 9 seems optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y6P6-wNMmN8M"
   },
   "outputs": [],
   "source": [
    " ## WORK IN PROGRESS ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qtBkIHnvmN8O"
   },
   "source": [
    "# 4. Model 1 with Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgwalcU7mN8P"
   },
   "outputs": [],
   "source": [
    "## we noticed that the first model had the best accuracy, without any pre-processing.\n",
    "## we then applied ridge regression to the first model.\n",
    "ratio = 0.8\n",
    "seed = 1\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX, y, ratio, seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XJsnYe6TmN8S"
   },
   "source": [
    "### 4.1 Ridge Regressiong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_bWVUHTmN8U",
    "outputId": "28fcf89e-8565-42c1-b7e1-7e7f58be9092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda=0; Loss=0.33942684955862673\n",
      "Prediction Accuracy:  0.74402\n",
      "\n",
      "Lambda=1e-15; Loss=0.33942684955862695\n",
      "Prediction Accuracy:  0.74402\n",
      "\n",
      "Lambda=1e-10; Loss=0.33942684959099034\n",
      "Prediction Accuracy:  0.74402\n",
      "\n",
      "Lambda=1e-05; Loss=0.3394308610386454\n",
      "Prediction Accuracy:  0.744\n",
      "\n",
      "Lambda=1; Loss=0.35329072520671573\n",
      "Prediction Accuracy:  0.73516\n",
      "\n",
      "Lambda=10.0; Loss=0.35648911776731346\n",
      "Prediction Accuracy:  0.734\n",
      "\n",
      "Lambda=1000; Loss=0.3820216654176733\n",
      "Prediction Accuracy:  0.69854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0, 1e-15, 1e-10, 1e-5, 1, 1e1, 1000]\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    \n",
    "    w, loss = ridge_regression(y_tr, x_tr, lambda_)\n",
    "    y_pred = predict_labels(w, x_te)\n",
    "    print(\"Lambda=\" + str(lambda_)+\"; Loss=\"+str(loss))\n",
    "\n",
    "    # predict on the test data slice\n",
    "    y_pred = predict_labels(w, x_te)\n",
    "    # Check accuracy\n",
    "    print(\"Prediction Accuracy: \", str(np.mean(y_te == y_pred)) + \"\\n\")\n",
    "    # Best for lambda = 0, Accuracy = 0.744"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6CsNEyoNmN8Z"
   },
   "source": [
    "### 4.2 Regularized Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-EcqTERUmN8a"
   },
   "outputs": [],
   "source": [
    "lambdas = [0, 1e-15, 1e-10, 1e-5, 1, 1e1, 1000]\n",
    "\n",
    "max_iter = 10\n",
    "gamma = 0.01\n",
    "lambda_ = 0.3\n",
    "threshold = 1e-8\n",
    "losses = []\n",
    "\n",
    "w = np.zeros((x_tr.shape[1], 1))\n",
    "\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_penalized_gradient(y_tr, x_tr, w, gamma, lambda_)\n",
    "    # log info\n",
    "    if iter % 100 == 0:\n",
    "        print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "    # converge criterion\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_penalized_gradient_descent\")\n",
    "print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gRuD4Eq1mN8f"
   },
   "source": [
    "### 2.2.1  GD for Second Model : train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mOSixt3LmN8f",
    "outputId": "054b0cb0-c80d-4d19-ffd3-5ddaeffa5a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/150 loss = 16.742723002367942\n",
      "Step 2/150 loss = 13.640113171583614\n",
      "Step 3/150 loss = 11.197208614873603\n",
      "Step 4/150 loss = 9.241386941981181\n",
      "Step 5/150 loss = 7.6637217309515515\n",
      "Step 6/150 loss = 6.386469086966029\n",
      "Step 7/150 loss = 5.350289705071333\n",
      "Step 8/150 loss = 4.5084324324530805\n",
      "Step 9/150 loss = 3.8235432932229134\n",
      "Step 10/150 loss = 3.2655939588419374\n",
      "Step 11/150 loss = 2.8103795766576725\n",
      "Step 12/150 loss = 2.4383641700028043\n",
      "Step 13/150 loss = 2.133768862969578\n",
      "Step 14/150 loss = 1.883843255860377\n",
      "Step 15/150 loss = 1.6782801536029566\n",
      "Step 16/150 loss = 1.5087443685174633\n",
      "Step 17/150 loss = 1.3684929531113643\n",
      "Step 18/150 loss = 1.2520689442005608\n",
      "Step 19/150 loss = 1.1550542964765451\n",
      "Step 20/150 loss = 1.0738705085735656\n",
      "Step 21/150 loss = 1.005617694447243\n",
      "Step 22/150 loss = 0.9479446554141303\n",
      "Step 23/150 loss = 0.8989439562644814\n",
      "Step 24/150 loss = 0.8570671735948378\n",
      "Step 25/150 loss = 0.8210564219510524\n",
      "Step 26/150 loss = 0.7898890181788069\n",
      "Step 27/150 loss = 0.7627327523082615\n",
      "Step 28/150 loss = 0.7389097230556666\n",
      "Step 29/150 loss = 0.7178670906709678\n",
      "Step 30/150 loss = 0.6991534179387848\n",
      "Step 31/150 loss = 0.6823995265631582\n",
      "Step 32/150 loss = 0.6673030029310052\n",
      "Step 33/150 loss = 0.6536156540088478\n",
      "Step 34/150 loss = 0.6411333486518163\n",
      "Step 35/150 loss = 0.6296877881482006\n",
      "Step 36/150 loss = 0.6191398374234844\n",
      "Step 37/150 loss = 0.6093741190414087\n",
      "Step 38/150 loss = 0.600294629233899\n",
      "Step 39/150 loss = 0.5918211812997786\n",
      "Step 40/150 loss = 0.5838865189560527\n",
      "Step 41/150 loss = 0.5764339723158559\n",
      "Step 42/150 loss = 0.5694155534830965\n",
      "Step 43/150 loss = 0.5627904084077481\n",
      "Step 44/150 loss = 0.5565235575348215\n",
      "Step 45/150 loss = 0.5505848706282405\n",
      "Step 46/150 loss = 0.544948231542285\n",
      "Step 47/150 loss = 0.5395908571195215\n",
      "Step 48/150 loss = 0.5344927411958731\n",
      "Step 49/150 loss = 0.5296362001981282\n",
      "Step 50/150 loss = 0.5250055012750214\n",
      "Step 51/150 loss = 0.520586557510647\n",
      "Step 52/150 loss = 0.516366677690452\n",
      "Step 53/150 loss = 0.512334360456416\n",
      "Step 54/150 loss = 0.508479124605126\n",
      "Step 55/150 loss = 0.5047913688359527\n",
      "Step 56/150 loss = 0.5012622555156313\n",
      "Step 57/150 loss = 0.4978836140462908\n",
      "Step 58/150 loss = 0.4946478602516449\n",
      "Step 59/150 loss = 0.49154792886734355\n",
      "Step 60/150 loss = 0.4885772167660517\n",
      "Step 61/150 loss = 0.4857295349897058\n",
      "Step 62/150 loss = 0.48299906802004466\n",
      "Step 63/150 loss = 0.48038033900968197\n",
      "Step 64/150 loss = 0.47786817993243186\n",
      "Step 65/150 loss = 0.47545770580367885\n",
      "Step 66/150 loss = 0.4731442922776409\n",
      "Step 67/150 loss = 0.4709235560552418\n",
      "Step 68/150 loss = 0.4687913376394558\n",
      "Step 69/150 loss = 0.4667436860588944\n",
      "Step 70/150 loss = 0.4647768452486903\n",
      "Step 71/150 loss = 0.4628872418333283\n",
      "Step 72/150 loss = 0.46107147410135857\n",
      "Step 73/150 loss = 0.4593263019988414\n",
      "Step 74/150 loss = 0.45764863799848043\n",
      "Step 75/150 loss = 0.4560355387259724\n",
      "Step 76/150 loss = 0.4544841972451777\n",
      "Step 77/150 loss = 0.4529919359201264\n",
      "Step 78/150 loss = 0.4515561997853129\n",
      "Step 79/150 loss = 0.45017455036673865\n",
      "Step 80/150 loss = 0.44884465990520145\n",
      "Step 81/150 loss = 0.44756430594075247\n",
      "Step 82/150 loss = 0.44633136622335856\n",
      "Step 83/150 loss = 0.4451438139198544\n",
      "Step 84/150 loss = 0.44399971309143355\n",
      "Step 85/150 loss = 0.4428972144193932\n",
      "Step 86/150 loss = 0.4418345511597172\n",
      "Step 87/150 loss = 0.44081003530948076\n",
      "Step 88/150 loss = 0.439822053970068\n",
      "Step 89/150 loss = 0.4388690658938778\n",
      "Step 90/150 loss = 0.4379495982026175\n",
      "Step 91/150 loss = 0.43706224326649035\n",
      "Step 92/150 loss = 0.4362056557346088\n",
      "Step 93/150 loss = 0.4353785497078481\n",
      "Step 94/150 loss = 0.43457969604611246\n",
      "Step 95/150 loss = 0.43380791980264066\n",
      "Step 96/150 loss = 0.43306209777855803\n",
      "Step 97/150 loss = 0.4323411561913749\n",
      "Step 98/150 loss = 0.4316440684515836\n",
      "Step 99/150 loss = 0.43096985304189783\n",
      "Step 100/150 loss = 0.4303175714940281\n",
      "Step 101/150 loss = 0.4296863264582086\n",
      "Step 102/150 loss = 0.42907525986097433\n",
      "Step 103/150 loss = 0.42848355114695047\n",
      "Step 104/150 loss = 0.4279104156006517\n",
      "Step 105/150 loss = 0.4273551027445123\n",
      "Step 106/150 loss = 0.4268168948095646\n",
      "Step 107/150 loss = 0.4262951052753771\n",
      "Step 108/150 loss = 0.42578907747603184\n",
      "Step 109/150 loss = 0.4252981832690858\n",
      "Step 110/150 loss = 0.424821821764614\n",
      "Step 111/150 loss = 0.4243594181115716\n",
      "Step 112/150 loss = 0.4239104223388498\n",
      "Step 113/150 loss = 0.4234743082485231\n",
      "Step 114/150 loss = 0.4230505723589071\n",
      "Step 115/150 loss = 0.422638732895159\n",
      "Step 116/150 loss = 0.42223832882525714\n",
      "Step 117/150 loss = 0.42184891893929843\n",
      "Step 118/150 loss = 0.42147008097015043\n",
      "Step 119/150 loss = 0.421101410753582\n",
      "Step 120/150 loss = 0.4207425214260851\n",
      "Step 121/150 loss = 0.42039304265868344\n",
      "Step 122/150 loss = 0.42005261992510057\n",
      "Step 123/150 loss = 0.4197209138027333\n",
      "Step 124/150 loss = 0.4193975993049524\n",
      "Step 125/150 loss = 0.4190823652433125\n",
      "Step 126/150 loss = 0.41877491361832464\n",
      "Step 127/150 loss = 0.41847495903750165\n",
      "Step 128/150 loss = 0.4181822281594493\n",
      "Step 129/150 loss = 0.4178964591628246\n",
      "Step 130/150 loss = 0.4176174012390471\n",
      "Step 131/150 loss = 0.4173448141076879\n",
      "Step 132/150 loss = 0.41707846755351874\n",
      "Step 133/150 loss = 0.4168181409842444\n",
      "Step 134/150 loss = 0.4165636230079889\n",
      "Step 135/150 loss = 0.4163147110296451\n",
      "Step 136/150 loss = 0.4160712108652412\n",
      "Step 137/150 loss = 0.41583293637351065\n",
      "Step 138/150 loss = 0.41559970910389793\n",
      "Step 139/150 loss = 0.4153713579602538\n",
      "Step 140/150 loss = 0.41514771887952084\n",
      "Step 141/150 loss = 0.4149286345247334\n",
      "Step 142/150 loss = 0.4147139539916889\n",
      "Step 143/150 loss = 0.41450353252867783\n",
      "Step 144/150 loss = 0.41429723126868334\n",
      "Step 145/150 loss = 0.41409491697349454\n",
      "Step 146/150 loss = 0.4138964617891952\n",
      "Step 147/150 loss = 0.4137017430125199\n",
      "Step 148/150 loss = 0.41351064286758865\n",
      "Step 149/150 loss = 0.4133230482925556\n",
      "Step 150/150 loss = 0.4131388507357264\n",
      "Gradient final loss for w* = 0.4131388507357264\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 150\n",
    "gamma = 0.08\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones(x_tr.shape[1])\n",
    "\n",
    "gradient_w, gradient_loss = least_squares_GD(y_tr, x_tr, w_initial, max_iters, gamma) \n",
    "\n",
    "print(\"Gradient final loss for w* = \" + str(gradient_loss)) # 0.4131388507357264"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1hypHlamN8h"
   },
   "source": [
    "### 2.2.2  GD for Second Model : predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "238Dy2vRmN8i",
    "outputId": "6e2e7345-1e13-4c76-ae27-1c07c0ca780b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6810115431906186\n",
      "0.687954195111209\n"
     ]
    }
   ],
   "source": [
    "# predict on the train data slice using the gradient_w\n",
    "y_pred = predict_labels(gradient_w, x_tr)\n",
    "# Check accuracy\n",
    "print(np.mean(y_tr == y_pred)) # 0.6810115431906186\n",
    "\n",
    "# predict on the test data slice\n",
    "y_pred = predict_labels(gradient_w, x_te)\n",
    "# Check accuracy\n",
    "print(np.mean(y_te == y_pred)) # 0.687954195111209"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oabfrVHbmN8k"
   },
   "source": [
    "### 2.3  Ridge regression for Second Model : train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MZHcfmv8mN8l"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T3RgwnxsmN8w"
   },
   "source": [
    "## 3. Third Model: replace all the nanValues with the column mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "feJkeLQqmN8x"
   },
   "outputs": [],
   "source": [
    "tX_3 = replace_nan_with_mean(tX, -999)\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX_3, y, ratio, seed)\n",
    "\n",
    "x_tr = standardize(x_tr)[0]\n",
    "x_te = standardize(x_te)[0]\n",
    "\n",
    "y_tr, x_tr = build_model_data(x_tr, y_tr)\n",
    "y_te, x_te = build_model_data(x_te, y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i50lIiN_mN8y"
   },
   "source": [
    "### 3.1.1 Third Model: Use least_squares : train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FRxuEt4SmN8z"
   },
   "outputs": [],
   "source": [
    "weights = least_squares(y_tr, x_tr)\n",
    "MSE_tr = compute_loss(y_tr, x_tr, weights)\n",
    "MSE_te = compute_loss(y_te, x_te, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UfFLCwe5mN81"
   },
   "source": [
    "### 3.1.2 Third Model: Use least_squares: prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UoeUF6FNmN81",
    "outputId": "b6c1839e-9f25-4953-a42e-38c47b8d5c33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5331424796300375"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on the test data slice\n",
    "y_pred = predict_labels(weights, x_te)\n",
    "# Check accuracy\n",
    "np.mean(y_te==y_pred) # 0.5331424796300375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U5vAj6-ymN83"
   },
   "source": [
    "### 3.2.1 Third Model: Use GD : Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1kc84Lv4mN83",
    "outputId": "28a5489b-11ce-4f37-d208-831056107268"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/150 loss = 17.3154463737639\n",
      "Step 2/150 loss = 12.85154559694335\n",
      "Step 3/150 loss = 10.15793488186781\n",
      "Step 4/150 loss = 8.393558092837578\n",
      "Step 5/150 loss = 7.146208061898221\n",
      "Step 6/150 loss = 6.207766930275749\n",
      "Step 7/150 loss = 5.468531107274753\n",
      "Step 8/150 loss = 4.867250480498854\n",
      "Step 9/150 loss = 4.3672931475690016\n",
      "Step 10/150 loss = 3.945107348603659\n",
      "Step 11/150 loss = 3.584507879895943\n",
      "Step 12/150 loss = 3.2737494035344157\n",
      "Step 13/150 loss = 3.003955009009131\n",
      "Step 14/150 loss = 2.7682204992546464\n",
      "Step 15/150 loss = 2.5610681557279027\n",
      "Step 16/150 loss = 2.3780904919748433\n",
      "Step 17/150 loss = 2.21570384800992\n",
      "Step 18/150 loss = 2.0709699189386193\n",
      "Step 19/150 loss = 1.9414621183893161\n",
      "Step 20/150 loss = 1.8251632119498742\n",
      "Step 21/150 loss = 1.7203857016225739\n",
      "Step 22/150 loss = 1.6257092655990009\n",
      "Step 23/150 loss = 1.5399312431392211\n",
      "Step 24/150 loss = 1.4620272297442516\n",
      "Step 25/150 loss = 1.391119576400008\n",
      "Step 26/150 loss = 1.3264521046700706\n",
      "Step 27/150 loss = 1.2673697309008831\n",
      "Step 28/150 loss = 1.2133019805895224\n",
      "Step 29/150 loss = 1.1637495944975655\n",
      "Step 30/150 loss = 1.1182735987878\n",
      "Step 31/150 loss = 1.0764863444154489\n",
      "Step 32/150 loss = 1.0380441249870351\n",
      "Step 33/150 loss = 1.0026410638424992\n",
      "Step 34/150 loss = 0.9700040251849756\n",
      "Step 35/150 loss = 0.9398883544980099\n",
      "Step 36/150 loss = 0.9120742932129489\n",
      "Step 37/150 loss = 0.8863639439261444\n",
      "Step 38/150 loss = 0.8625786872179533\n",
      "Step 39/150 loss = 0.8405569707031436\n",
      "Step 40/150 loss = 0.8201524064499298\n",
      "Step 41/150 loss = 0.8012321252086315\n",
      "Step 42/150 loss = 0.7836753456707534\n",
      "Step 43/150 loss = 0.7673721247687452\n",
      "Step 44/150 loss = 0.752222261245137\n",
      "Step 45/150 loss = 0.738134329696625\n",
      "Step 46/150 loss = 0.7250248262931946\n",
      "Step 47/150 loss = 0.7128174105883744\n",
      "Step 48/150 loss = 0.7014422304348106\n",
      "Step 49/150 loss = 0.6908353191260848\n",
      "Step 50/150 loss = 0.6809380556008503\n",
      "Step 51/150 loss = 0.6716966799477725\n",
      "Step 52/150 loss = 0.6630618576017079\n",
      "Step 53/150 loss = 0.6549882865723688\n",
      "Step 54/150 loss = 0.6474343428354737\n",
      "Step 55/150 loss = 0.6403617596741387\n",
      "Step 56/150 loss = 0.6337353373097031\n",
      "Step 57/150 loss = 0.6275226796259898\n",
      "Step 58/150 loss = 0.6216939551849388\n",
      "Step 59/150 loss = 0.6162216800671952\n",
      "Step 60/150 loss = 0.6110805203587445\n",
      "Step 61/150 loss = 0.6062471123522134\n",
      "Step 62/150 loss = 0.6016998987456245\n",
      "Step 63/150 loss = 0.5974189793075648\n",
      "Step 64/150 loss = 0.593385974640277\n",
      "Step 65/150 loss = 0.5895839018147041\n",
      "Step 66/150 loss = 0.5859970607769617\n",
      "Step 67/150 loss = 0.582610930536506\n",
      "Step 68/150 loss = 0.5794120742444594\n",
      "Step 69/150 loss = 0.5763880523578194\n",
      "Step 70/150 loss = 0.5735273431630584\n",
      "Step 71/150 loss = 0.5708192700021012\n",
      "Step 72/150 loss = 0.5682539346058826\n",
      "Step 73/150 loss = 0.5658221559964923\n",
      "Step 74/150 loss = 0.5635154144690775\n",
      "Step 75/150 loss = 0.5613258002098145\n",
      "Step 76/150 loss = 0.5592459661469539\n",
      "Step 77/150 loss = 0.5572690846686743\n",
      "Step 78/150 loss = 0.5553888078746554\n",
      "Step 79/150 loss = 0.5535992310582993\n",
      "Step 80/150 loss = 0.5518948591436936\n",
      "Step 81/150 loss = 0.5502705758260209\n",
      "Step 82/150 loss = 0.5487216151864425\n",
      "Step 83/150 loss = 0.547243535572723\n",
      "Step 84/150 loss = 0.5458321955552446\n",
      "Step 85/150 loss = 0.5444837317847548\n",
      "Step 86/150 loss = 0.5431945385933596\n",
      "Step 87/150 loss = 0.5419612491940675\n",
      "Step 88/150 loss = 0.5407807183467386\n",
      "Step 89/150 loss = 0.5396500063697021\n",
      "Step 90/150 loss = 0.538566364386705\n",
      "Step 91/150 loss = 0.5375272207083142\n",
      "Step 92/150 loss = 0.5365301682555131\n",
      "Step 93/150 loss = 0.535572952941096\n",
      "Step 94/150 loss = 0.5346534629316162\n",
      "Step 95/150 loss = 0.5337697187191817\n",
      "Step 96/150 loss = 0.5329198639383429\n",
      "Step 97/150 loss = 0.532102156868753\n",
      "Step 98/150 loss = 0.5313149625692426\n",
      "Step 99/150 loss = 0.5305567455934778\n",
      "Step 100/150 loss = 0.5298260632415076\n",
      "Step 101/150 loss = 0.5291215593052825\n",
      "Step 102/150 loss = 0.5284419582696784\n",
      "Step 103/150 loss = 0.5277860599337224\n",
      "Step 104/150 loss = 0.5271527344195904\n",
      "Step 105/150 loss = 0.526540917539597\n",
      "Step 106/150 loss = 0.5259496064938061\n",
      "Step 107/150 loss = 0.5253778558731028\n",
      "Step 108/150 loss = 0.5248247739445909\n",
      "Step 109/150 loss = 0.5242895191980314\n",
      "Step 110/150 loss = 0.5237712971337334\n",
      "Step 111/150 loss = 0.5232693572738711\n",
      "Step 112/150 loss = 0.5227829903806157\n",
      "Step 113/150 loss = 0.522311525865787\n",
      "Step 114/150 loss = 0.5218543293779249\n",
      "Step 115/150 loss = 0.5214108005537809\n",
      "Step 116/150 loss = 0.5209803709222421\n",
      "Step 117/150 loss = 0.5205625019496256\n",
      "Step 118/150 loss = 0.5201566832161326\n",
      "Step 119/150 loss = 0.519762430714036\n",
      "Step 120/150 loss = 0.5193792852588957\n",
      "Step 121/150 loss = 0.5190068110057512\n",
      "Step 122/150 loss = 0.5186445940628579\n",
      "Step 123/150 loss = 0.5182922411960877\n",
      "Step 124/150 loss = 0.5179493786176316\n",
      "Step 125/150 loss = 0.5176156508531166\n",
      "Step 126/150 loss = 0.5172907196816855\n",
      "Step 127/150 loss = 0.5169742631439946\n",
      "Step 128/150 loss = 0.5166659746134483\n",
      "Step 129/150 loss = 0.5163655619263404\n",
      "Step 130/150 loss = 0.5160727465668825\n",
      "Step 131/150 loss = 0.5157872629033923\n",
      "Step 132/150 loss = 0.515508857472185\n",
      "Step 133/150 loss = 0.5152372883059594\n",
      "Step 134/150 loss = 0.5149723243036971\n",
      "Step 135/150 loss = 0.5147137446393111\n",
      "Step 136/150 loss = 0.5144613382064693\n",
      "Step 137/150 loss = 0.5142149030972066\n",
      "Step 138/150 loss = 0.5139742461121015\n",
      "Step 139/150 loss = 0.5137391822999532\n",
      "Step 140/150 loss = 0.5135095345250347\n",
      "Step 141/150 loss = 0.5132851330601355\n",
      "Step 142/150 loss = 0.5130658152037264\n",
      "Step 143/150 loss = 0.512851424919696\n",
      "Step 144/150 loss = 0.5126418124982142\n",
      "Step 145/150 loss = 0.5124368342363763\n",
      "Step 146/150 loss = 0.512236352137372\n",
      "Step 147/150 loss = 0.5120402336270091\n",
      "Step 148/150 loss = 0.5118483512865011\n",
      "Step 149/150 loss = 0.511660582600499\n",
      "Step 150/150 loss = 0.5114768097194176\n",
      "Gradient final loss for w* = 0.5114768097194176\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 150\n",
    "gamma = 0.08\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones(x_tr.shape[1])\n",
    "\n",
    "gradient_w, gradient_loss = least_squares_GD(y_tr, x_tr, w_initial, max_iters, gamma) \n",
    "\n",
    "print(\"Gradient final loss for w* = \" + str(gradient_loss)) # 0.5114768097194176\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35mi7d6ZmN85"
   },
   "source": [
    "### 3.2.2 Third Model: Use GD : Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwUbgYFzmN86",
    "outputId": "c2e5cb4b-5ce3-4f08-82dc-1ed556d0e6cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5006239447992366"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on the test data slice\n",
    "y_pred = predict_labels(gradient_w, x_te)\n",
    "# Check accuracy\n",
    "np.mean(y_te==y_pred) # 0.5006239447992366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5N_zjXCpmN88"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yjZtzi6jmN8-"
   },
   "source": [
    "## 4. Fourth Model with Feature Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_no8zn8UmN8-",
    "outputId": "7916ac38-f78f-463c-8459-8b964c079746"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (54491,32) and (54491,) not aligned: 32 (dim 1) != 54491 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-a4f7a1f6e8d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpolynome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolynome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolynome\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-861c55fb9329>\u001b[0m in \u001b[0;36mleast_squares\u001b[0;34m(y, tx)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (54491,32) and (54491,) not aligned: 32 (dim 1) != 54491 (dim 0)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14a0607f0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "degrees = [1, 2, 3, 4]\n",
    "\n",
    "# define the structure of the figure\n",
    "num_row = 2\n",
    "num_col = 2\n",
    "f, axs = plt.subplots(num_row, num_col)\n",
    "    \n",
    "for ind, degree in enumerate(degrees):\n",
    "\n",
    "        polynome = build_poly(x_tr,degree)\n",
    "        w = least_squares(y_tr, polynome.T)\n",
    "        MSE = compute_loss(y_tr, polynome, w)\n",
    "\n",
    "        print(\"Processing {i}th experiment, degree={d}, mse={loss}\".format( \n",
    "            i=ind + 1, d=degree, loss=MSE))\n",
    "        plot_fitted_curve(y_tr, x_tr, weights, degree, axs[ind // num_col][ind % num_col])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2MuXRogAmN9B"
   },
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JOOmucUimN9B"
   },
   "outputs": [],
   "source": [
    "# We observed that model 2 with ridge_regression method and lambda_ = 1e-10 (TODO, we have to check them all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "agdRdr6zmN9D"
   },
   "outputs": [],
   "source": [
    "polynome = build_poly(x_tr,9)\n",
    "weights = least_squares(y_tr, polynome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JpTiNLMfmN9F"
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: unzip the file\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test = build_poly(tX_test,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fzpMrw0EmN9H"
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'out.csv'\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qa5LQkMImN9K"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "include_colab_link": true,
   "name": "project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%run implementations.ipynb\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. First model: Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# first to split the data into training and testing\n",
    "\n",
    "ratio = 0.8\n",
    "seed = 3\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX, y, ratio, seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Training using Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  0.3394801404386906 \n",
      "Testing MSE: 0.34056956713540926\n"
     ]
    }
   ],
   "source": [
    "# First Model using least squares\n",
    "\n",
    "weights = least_squares(y_tr, x_tr)\n",
    "MSE_tr = compute_loss(y_tr, x_tr, weights)\n",
    "MSE_te = compute_loss(y_te, x_te, weights)\n",
    "\n",
    "print(\"Training MSE: \", MSE_tr, \"\\nTesting MSE:\", MSE_te)\n",
    "#0.3394801404386906  0.34056956713540926"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prediction using Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  0.74242\n"
     ]
    }
   ],
   "source": [
    "# predict on the test data slice\n",
    "y_pred = predict_labels(weights, x_te)\n",
    "# Check accuracy\n",
    "print(\"Prediction Accuracy: \", np.mean(y_te==y_pred)) # 0.34396"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Second Model: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all nan but there actually are none\n",
    "tX_2 = tX[~np.isnan(tX).any(axis=1)]\n",
    "\n",
    "# removing all rows with -999, there are 181886\n",
    "y = y[np.all(tX_2 != -999, axis=1)]\n",
    "tX_2 = tX_2[np.all(tX_2 != -999, axis=1)]\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX_2, y, ratio, seed)\n",
    "\n",
    "# then to standardize x_tr and x_te\n",
    "x_tr = standardize(x_tr)[0]\n",
    "x_te = standardize(x_te)[0]\n",
    "\n",
    "# next to add a column of ones\n",
    "y_tr, x_tr = build_model_data(x_tr, y_tr)\n",
    "y_te, x_te = build_model_data(x_te, y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Training using Least Squares with pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  0.36864162046559024 \n",
      "Testing MSE: 0.5411866496785176\n"
     ]
    }
   ],
   "source": [
    "weights = least_squares(y_tr, x_tr)\n",
    "MSE_tr = compute_loss(y_tr, x_tr, weights)\n",
    "MSE_te = compute_loss(y_te, x_te, weights)\n",
    "\n",
    "print(\"Training MSE: \", MSE_tr, \"\\nTesting MSE:\", MSE_te) \n",
    "#0.36864162046559024 0.5411866496785176"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Prediction using Least Squares with pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  0.6121265506863393\n"
     ]
    }
   ],
   "source": [
    "# predict on the test data slice\n",
    "y_pred = predict_labels(weights, x_te)\n",
    "# Check accuracy\n",
    "print(\"Prediction Accuracy: \", np.mean(y_te==y_pred)) # 0.6121265506863393"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Third Model: Better Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace nans with mean\n",
    "\n",
    "tX_3 = replace_nan_with_mean(tX, -999)\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX_3, y, ratio, seed)\n",
    "\n",
    "# then to standardize x_tr and x_te\n",
    "x_tr = standardize(x_tr)[0]\n",
    "x_te = standardize(x_te)[0]\n",
    "\n",
    "# next to add a column of ones\n",
    "y_tr, x_tr = build_model_data(x_tr, y_tr)\n",
    "y_te, x_te = build_model_data(x_te, y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Training using Least Squares with better pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  0.4978447945914067 \n",
      "Testing MSE: 0.6275084689089252\n"
     ]
    }
   ],
   "source": [
    "weights = least_squares(y_tr, x_tr)\n",
    "MSE_tr = compute_loss(y_tr, x_tr, weights)\n",
    "MSE_te = compute_loss(y_te, x_te, weights)\n",
    "\n",
    "print(\"Training MSE: \", MSE_tr, \"\\nTesting MSE:\", MSE_te) \n",
    "#0.4978447945914067  0.6275084689089252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Prediction using Least Squares with better pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  0.5331424796300375\n"
     ]
    }
   ],
   "source": [
    "# predict on the test data slice\n",
    "y_pred = predict_labels(weights, x_te)\n",
    "# Check accuracy\n",
    "print(\"Prediction Accuracy: \", np.mean(y_te==y_pred)) # 0.6121265506863393"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model 1 with Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we noticed that the first model had the best accuracy, without any pre-processing.\n",
    "## we then applied ridge regression to the first model.\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX, y, ratio, seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Ridge Regressiong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda=0; Loss=0.3394801404386932\n",
      "Prediction Accuracy:  0.74242\n",
      "\n",
      "Lambda=1e-15; Loss=0.33948014043869323\n",
      "Prediction Accuracy:  0.74242\n",
      "\n",
      "Lambda=1e-10; Loss=0.33948014044178076\n",
      "Prediction Accuracy:  0.74242\n",
      "\n",
      "Lambda=1e-05; Loss=0.33948053067594797\n",
      "Prediction Accuracy:  0.74238\n",
      "\n",
      "Lambda=1; Loss=0.3533787279917536\n",
      "Prediction Accuracy:  0.73164\n",
      "\n",
      "Lambda=10.0; Loss=0.3565863372538669\n",
      "Prediction Accuracy:  0.72986\n",
      "\n",
      "Lambda=1000; Loss=0.3823587921558087\n",
      "Prediction Accuracy:  0.69884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0, 1e-15, 1e-10, 1e-5, 1, 1e1, 1000]\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    \n",
    "    w, loss = ridge_regression(y_tr, x_tr, lambda_)\n",
    "    y_pred = predict_labels(w, x_te)\n",
    "    print(\"Lambda=\" + str(lambda_)+\"; Loss=\"+str(loss))\n",
    "\n",
    "    # predict on the test data slice\n",
    "    y_pred = predict_labels(w, x_te)\n",
    "    # Check accuracy\n",
    "    print(\"Prediction Accuracy: \", str(np.mean(y_te == y_pred)) + \"\\n\")\n",
    "    # Best for lambda = 0, Accuracy = 0.742"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Regularized Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0, 1e-15, 1e-10, 1e-5, 1, 1e1, 1000]\n",
    "\n",
    "max_iter = 10\n",
    "gamma = 0.01\n",
    "lambda_ = 0.3\n",
    "threshold = 1e-8\n",
    "losses = []\n",
    "\n",
    "x_tr = x_tr.T\n",
    "y_tr = y_tr.T\n",
    "w = np.zeros((x_tr.shape[1], 1))\n",
    "\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_penalized_gradient(y_tr, x_tr, w, gamma, lambda_)\n",
    "    # log info\n",
    "    if iter % 100 == 0:\n",
    "        print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "    # converge criterion\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_penalized_gradient_descent\")\n",
    "print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1  GD for Second Model : train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/150 loss = 16.742723002367942\n",
      "Step 2/150 loss = 13.640113171583614\n",
      "Step 3/150 loss = 11.197208614873603\n",
      "Step 4/150 loss = 9.241386941981181\n",
      "Step 5/150 loss = 7.6637217309515515\n",
      "Step 6/150 loss = 6.386469086966029\n",
      "Step 7/150 loss = 5.350289705071333\n",
      "Step 8/150 loss = 4.5084324324530805\n",
      "Step 9/150 loss = 3.8235432932229134\n",
      "Step 10/150 loss = 3.2655939588419374\n",
      "Step 11/150 loss = 2.8103795766576725\n",
      "Step 12/150 loss = 2.4383641700028043\n",
      "Step 13/150 loss = 2.133768862969578\n",
      "Step 14/150 loss = 1.883843255860377\n",
      "Step 15/150 loss = 1.6782801536029566\n",
      "Step 16/150 loss = 1.5087443685174633\n",
      "Step 17/150 loss = 1.3684929531113643\n",
      "Step 18/150 loss = 1.2520689442005608\n",
      "Step 19/150 loss = 1.1550542964765451\n",
      "Step 20/150 loss = 1.0738705085735656\n",
      "Step 21/150 loss = 1.005617694447243\n",
      "Step 22/150 loss = 0.9479446554141303\n",
      "Step 23/150 loss = 0.8989439562644814\n",
      "Step 24/150 loss = 0.8570671735948378\n",
      "Step 25/150 loss = 0.8210564219510524\n",
      "Step 26/150 loss = 0.7898890181788069\n",
      "Step 27/150 loss = 0.7627327523082615\n",
      "Step 28/150 loss = 0.7389097230556666\n",
      "Step 29/150 loss = 0.7178670906709678\n",
      "Step 30/150 loss = 0.6991534179387848\n",
      "Step 31/150 loss = 0.6823995265631582\n",
      "Step 32/150 loss = 0.6673030029310052\n",
      "Step 33/150 loss = 0.6536156540088478\n",
      "Step 34/150 loss = 0.6411333486518163\n",
      "Step 35/150 loss = 0.6296877881482006\n",
      "Step 36/150 loss = 0.6191398374234844\n",
      "Step 37/150 loss = 0.6093741190414087\n",
      "Step 38/150 loss = 0.600294629233899\n",
      "Step 39/150 loss = 0.5918211812997786\n",
      "Step 40/150 loss = 0.5838865189560527\n",
      "Step 41/150 loss = 0.5764339723158559\n",
      "Step 42/150 loss = 0.5694155534830965\n",
      "Step 43/150 loss = 0.5627904084077481\n",
      "Step 44/150 loss = 0.5565235575348215\n",
      "Step 45/150 loss = 0.5505848706282405\n",
      "Step 46/150 loss = 0.544948231542285\n",
      "Step 47/150 loss = 0.5395908571195215\n",
      "Step 48/150 loss = 0.5344927411958731\n",
      "Step 49/150 loss = 0.5296362001981282\n",
      "Step 50/150 loss = 0.5250055012750214\n",
      "Step 51/150 loss = 0.520586557510647\n",
      "Step 52/150 loss = 0.516366677690452\n",
      "Step 53/150 loss = 0.512334360456416\n",
      "Step 54/150 loss = 0.508479124605126\n",
      "Step 55/150 loss = 0.5047913688359527\n",
      "Step 56/150 loss = 0.5012622555156313\n",
      "Step 57/150 loss = 0.4978836140462908\n",
      "Step 58/150 loss = 0.4946478602516449\n",
      "Step 59/150 loss = 0.49154792886734355\n",
      "Step 60/150 loss = 0.4885772167660517\n",
      "Step 61/150 loss = 0.4857295349897058\n",
      "Step 62/150 loss = 0.48299906802004466\n",
      "Step 63/150 loss = 0.48038033900968197\n",
      "Step 64/150 loss = 0.47786817993243186\n",
      "Step 65/150 loss = 0.47545770580367885\n",
      "Step 66/150 loss = 0.4731442922776409\n",
      "Step 67/150 loss = 0.4709235560552418\n",
      "Step 68/150 loss = 0.4687913376394558\n",
      "Step 69/150 loss = 0.4667436860588944\n",
      "Step 70/150 loss = 0.4647768452486903\n",
      "Step 71/150 loss = 0.4628872418333283\n",
      "Step 72/150 loss = 0.46107147410135857\n",
      "Step 73/150 loss = 0.4593263019988414\n",
      "Step 74/150 loss = 0.45764863799848043\n",
      "Step 75/150 loss = 0.4560355387259724\n",
      "Step 76/150 loss = 0.4544841972451777\n",
      "Step 77/150 loss = 0.4529919359201264\n",
      "Step 78/150 loss = 0.4515561997853129\n",
      "Step 79/150 loss = 0.45017455036673865\n",
      "Step 80/150 loss = 0.44884465990520145\n",
      "Step 81/150 loss = 0.44756430594075247\n",
      "Step 82/150 loss = 0.44633136622335856\n",
      "Step 83/150 loss = 0.4451438139198544\n",
      "Step 84/150 loss = 0.44399971309143355\n",
      "Step 85/150 loss = 0.4428972144193932\n",
      "Step 86/150 loss = 0.4418345511597172\n",
      "Step 87/150 loss = 0.44081003530948076\n",
      "Step 88/150 loss = 0.439822053970068\n",
      "Step 89/150 loss = 0.4388690658938778\n",
      "Step 90/150 loss = 0.4379495982026175\n",
      "Step 91/150 loss = 0.43706224326649035\n",
      "Step 92/150 loss = 0.4362056557346088\n",
      "Step 93/150 loss = 0.4353785497078481\n",
      "Step 94/150 loss = 0.43457969604611246\n",
      "Step 95/150 loss = 0.43380791980264066\n",
      "Step 96/150 loss = 0.43306209777855803\n",
      "Step 97/150 loss = 0.4323411561913749\n",
      "Step 98/150 loss = 0.4316440684515836\n",
      "Step 99/150 loss = 0.43096985304189783\n",
      "Step 100/150 loss = 0.4303175714940281\n",
      "Step 101/150 loss = 0.4296863264582086\n",
      "Step 102/150 loss = 0.42907525986097433\n",
      "Step 103/150 loss = 0.42848355114695047\n",
      "Step 104/150 loss = 0.4279104156006517\n",
      "Step 105/150 loss = 0.4273551027445123\n",
      "Step 106/150 loss = 0.4268168948095646\n",
      "Step 107/150 loss = 0.4262951052753771\n",
      "Step 108/150 loss = 0.42578907747603184\n",
      "Step 109/150 loss = 0.4252981832690858\n",
      "Step 110/150 loss = 0.424821821764614\n",
      "Step 111/150 loss = 0.4243594181115716\n",
      "Step 112/150 loss = 0.4239104223388498\n",
      "Step 113/150 loss = 0.4234743082485231\n",
      "Step 114/150 loss = 0.4230505723589071\n",
      "Step 115/150 loss = 0.422638732895159\n",
      "Step 116/150 loss = 0.42223832882525714\n",
      "Step 117/150 loss = 0.42184891893929843\n",
      "Step 118/150 loss = 0.42147008097015043\n",
      "Step 119/150 loss = 0.421101410753582\n",
      "Step 120/150 loss = 0.4207425214260851\n",
      "Step 121/150 loss = 0.42039304265868344\n",
      "Step 122/150 loss = 0.42005261992510057\n",
      "Step 123/150 loss = 0.4197209138027333\n",
      "Step 124/150 loss = 0.4193975993049524\n",
      "Step 125/150 loss = 0.4190823652433125\n",
      "Step 126/150 loss = 0.41877491361832464\n",
      "Step 127/150 loss = 0.41847495903750165\n",
      "Step 128/150 loss = 0.4181822281594493\n",
      "Step 129/150 loss = 0.4178964591628246\n",
      "Step 130/150 loss = 0.4176174012390471\n",
      "Step 131/150 loss = 0.4173448141076879\n",
      "Step 132/150 loss = 0.41707846755351874\n",
      "Step 133/150 loss = 0.4168181409842444\n",
      "Step 134/150 loss = 0.4165636230079889\n",
      "Step 135/150 loss = 0.4163147110296451\n",
      "Step 136/150 loss = 0.4160712108652412\n",
      "Step 137/150 loss = 0.41583293637351065\n",
      "Step 138/150 loss = 0.41559970910389793\n",
      "Step 139/150 loss = 0.4153713579602538\n",
      "Step 140/150 loss = 0.41514771887952084\n",
      "Step 141/150 loss = 0.4149286345247334\n",
      "Step 142/150 loss = 0.4147139539916889\n",
      "Step 143/150 loss = 0.41450353252867783\n",
      "Step 144/150 loss = 0.41429723126868334\n",
      "Step 145/150 loss = 0.41409491697349454\n",
      "Step 146/150 loss = 0.4138964617891952\n",
      "Step 147/150 loss = 0.4137017430125199\n",
      "Step 148/150 loss = 0.41351064286758865\n",
      "Step 149/150 loss = 0.4133230482925556\n",
      "Step 150/150 loss = 0.4131388507357264\n",
      "Gradient final loss for w* = 0.4131388507357264\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 150\n",
    "gamma = 0.08\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones(x_tr.shape[1])\n",
    "\n",
    "gradient_w, gradient_loss = least_squares_GD(y_tr, x_tr, w_initial, max_iters, gamma) \n",
    "\n",
    "print(\"Gradient final loss for w* = \" + str(gradient_loss)) # 0.4131388507357264"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2  GD for Second Model : predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6810115431906186\n",
      "0.687954195111209\n"
     ]
    }
   ],
   "source": [
    "# predict on the train data slice using the gradient_w\n",
    "y_pred = predict_labels(gradient_w, x_tr)\n",
    "# Check accuracy\n",
    "print(np.mean(y_tr == y_pred)) # 0.6810115431906186\n",
    "\n",
    "# predict on the test data slice\n",
    "y_pred = predict_labels(gradient_w, x_te)\n",
    "# Check accuracy\n",
    "print(np.mean(y_te == y_pred)) # 0.687954195111209"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3  Ridge regression for Second Model : train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Third Model: replace all the nanValues with the column mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_3 = replace_nan_with_mean(tX, -999)\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX_3, y, ratio, seed)\n",
    "\n",
    "x_tr = standardize(x_tr)[0]\n",
    "x_te = standardize(x_te)[0]\n",
    "\n",
    "y_tr, x_tr = build_model_data(x_tr, y_tr)\n",
    "y_te, x_te = build_model_data(x_te, y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Third Model: Use least_squares : train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = least_squares(y_tr, x_tr)\n",
    "MSE_tr = compute_loss(y_tr, x_tr, weights)\n",
    "MSE_te = compute_loss(y_te, x_te, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Third Model: Use least_squares: prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5331424796300375"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on the test data slice\n",
    "y_pred = predict_labels(weights, x_te)\n",
    "# Check accuracy\n",
    "np.mean(y_te==y_pred) # 0.5331424796300375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Third Model: Use GD : Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/150 loss = 17.3154463737639\n",
      "Step 2/150 loss = 12.85154559694335\n",
      "Step 3/150 loss = 10.15793488186781\n",
      "Step 4/150 loss = 8.393558092837578\n",
      "Step 5/150 loss = 7.146208061898221\n",
      "Step 6/150 loss = 6.207766930275749\n",
      "Step 7/150 loss = 5.468531107274753\n",
      "Step 8/150 loss = 4.867250480498854\n",
      "Step 9/150 loss = 4.3672931475690016\n",
      "Step 10/150 loss = 3.945107348603659\n",
      "Step 11/150 loss = 3.584507879895943\n",
      "Step 12/150 loss = 3.2737494035344157\n",
      "Step 13/150 loss = 3.003955009009131\n",
      "Step 14/150 loss = 2.7682204992546464\n",
      "Step 15/150 loss = 2.5610681557279027\n",
      "Step 16/150 loss = 2.3780904919748433\n",
      "Step 17/150 loss = 2.21570384800992\n",
      "Step 18/150 loss = 2.0709699189386193\n",
      "Step 19/150 loss = 1.9414621183893161\n",
      "Step 20/150 loss = 1.8251632119498742\n",
      "Step 21/150 loss = 1.7203857016225739\n",
      "Step 22/150 loss = 1.6257092655990009\n",
      "Step 23/150 loss = 1.5399312431392211\n",
      "Step 24/150 loss = 1.4620272297442516\n",
      "Step 25/150 loss = 1.391119576400008\n",
      "Step 26/150 loss = 1.3264521046700706\n",
      "Step 27/150 loss = 1.2673697309008831\n",
      "Step 28/150 loss = 1.2133019805895224\n",
      "Step 29/150 loss = 1.1637495944975655\n",
      "Step 30/150 loss = 1.1182735987878\n",
      "Step 31/150 loss = 1.0764863444154489\n",
      "Step 32/150 loss = 1.0380441249870351\n",
      "Step 33/150 loss = 1.0026410638424992\n",
      "Step 34/150 loss = 0.9700040251849756\n",
      "Step 35/150 loss = 0.9398883544980099\n",
      "Step 36/150 loss = 0.9120742932129489\n",
      "Step 37/150 loss = 0.8863639439261444\n",
      "Step 38/150 loss = 0.8625786872179533\n",
      "Step 39/150 loss = 0.8405569707031436\n",
      "Step 40/150 loss = 0.8201524064499298\n",
      "Step 41/150 loss = 0.8012321252086315\n",
      "Step 42/150 loss = 0.7836753456707534\n",
      "Step 43/150 loss = 0.7673721247687452\n",
      "Step 44/150 loss = 0.752222261245137\n",
      "Step 45/150 loss = 0.738134329696625\n",
      "Step 46/150 loss = 0.7250248262931946\n",
      "Step 47/150 loss = 0.7128174105883744\n",
      "Step 48/150 loss = 0.7014422304348106\n",
      "Step 49/150 loss = 0.6908353191260848\n",
      "Step 50/150 loss = 0.6809380556008503\n",
      "Step 51/150 loss = 0.6716966799477725\n",
      "Step 52/150 loss = 0.6630618576017079\n",
      "Step 53/150 loss = 0.6549882865723688\n",
      "Step 54/150 loss = 0.6474343428354737\n",
      "Step 55/150 loss = 0.6403617596741387\n",
      "Step 56/150 loss = 0.6337353373097031\n",
      "Step 57/150 loss = 0.6275226796259898\n",
      "Step 58/150 loss = 0.6216939551849388\n",
      "Step 59/150 loss = 0.6162216800671952\n",
      "Step 60/150 loss = 0.6110805203587445\n",
      "Step 61/150 loss = 0.6062471123522134\n",
      "Step 62/150 loss = 0.6016998987456245\n",
      "Step 63/150 loss = 0.5974189793075648\n",
      "Step 64/150 loss = 0.593385974640277\n",
      "Step 65/150 loss = 0.5895839018147041\n",
      "Step 66/150 loss = 0.5859970607769617\n",
      "Step 67/150 loss = 0.582610930536506\n",
      "Step 68/150 loss = 0.5794120742444594\n",
      "Step 69/150 loss = 0.5763880523578194\n",
      "Step 70/150 loss = 0.5735273431630584\n",
      "Step 71/150 loss = 0.5708192700021012\n",
      "Step 72/150 loss = 0.5682539346058826\n",
      "Step 73/150 loss = 0.5658221559964923\n",
      "Step 74/150 loss = 0.5635154144690775\n",
      "Step 75/150 loss = 0.5613258002098145\n",
      "Step 76/150 loss = 0.5592459661469539\n",
      "Step 77/150 loss = 0.5572690846686743\n",
      "Step 78/150 loss = 0.5553888078746554\n",
      "Step 79/150 loss = 0.5535992310582993\n",
      "Step 80/150 loss = 0.5518948591436936\n",
      "Step 81/150 loss = 0.5502705758260209\n",
      "Step 82/150 loss = 0.5487216151864425\n",
      "Step 83/150 loss = 0.547243535572723\n",
      "Step 84/150 loss = 0.5458321955552446\n",
      "Step 85/150 loss = 0.5444837317847548\n",
      "Step 86/150 loss = 0.5431945385933596\n",
      "Step 87/150 loss = 0.5419612491940675\n",
      "Step 88/150 loss = 0.5407807183467386\n",
      "Step 89/150 loss = 0.5396500063697021\n",
      "Step 90/150 loss = 0.538566364386705\n",
      "Step 91/150 loss = 0.5375272207083142\n",
      "Step 92/150 loss = 0.5365301682555131\n",
      "Step 93/150 loss = 0.535572952941096\n",
      "Step 94/150 loss = 0.5346534629316162\n",
      "Step 95/150 loss = 0.5337697187191817\n",
      "Step 96/150 loss = 0.5329198639383429\n",
      "Step 97/150 loss = 0.532102156868753\n",
      "Step 98/150 loss = 0.5313149625692426\n",
      "Step 99/150 loss = 0.5305567455934778\n",
      "Step 100/150 loss = 0.5298260632415076\n",
      "Step 101/150 loss = 0.5291215593052825\n",
      "Step 102/150 loss = 0.5284419582696784\n",
      "Step 103/150 loss = 0.5277860599337224\n",
      "Step 104/150 loss = 0.5271527344195904\n",
      "Step 105/150 loss = 0.526540917539597\n",
      "Step 106/150 loss = 0.5259496064938061\n",
      "Step 107/150 loss = 0.5253778558731028\n",
      "Step 108/150 loss = 0.5248247739445909\n",
      "Step 109/150 loss = 0.5242895191980314\n",
      "Step 110/150 loss = 0.5237712971337334\n",
      "Step 111/150 loss = 0.5232693572738711\n",
      "Step 112/150 loss = 0.5227829903806157\n",
      "Step 113/150 loss = 0.522311525865787\n",
      "Step 114/150 loss = 0.5218543293779249\n",
      "Step 115/150 loss = 0.5214108005537809\n",
      "Step 116/150 loss = 0.5209803709222421\n",
      "Step 117/150 loss = 0.5205625019496256\n",
      "Step 118/150 loss = 0.5201566832161326\n",
      "Step 119/150 loss = 0.519762430714036\n",
      "Step 120/150 loss = 0.5193792852588957\n",
      "Step 121/150 loss = 0.5190068110057512\n",
      "Step 122/150 loss = 0.5186445940628579\n",
      "Step 123/150 loss = 0.5182922411960877\n",
      "Step 124/150 loss = 0.5179493786176316\n",
      "Step 125/150 loss = 0.5176156508531166\n",
      "Step 126/150 loss = 0.5172907196816855\n",
      "Step 127/150 loss = 0.5169742631439946\n",
      "Step 128/150 loss = 0.5166659746134483\n",
      "Step 129/150 loss = 0.5163655619263404\n",
      "Step 130/150 loss = 0.5160727465668825\n",
      "Step 131/150 loss = 0.5157872629033923\n",
      "Step 132/150 loss = 0.515508857472185\n",
      "Step 133/150 loss = 0.5152372883059594\n",
      "Step 134/150 loss = 0.5149723243036971\n",
      "Step 135/150 loss = 0.5147137446393111\n",
      "Step 136/150 loss = 0.5144613382064693\n",
      "Step 137/150 loss = 0.5142149030972066\n",
      "Step 138/150 loss = 0.5139742461121015\n",
      "Step 139/150 loss = 0.5137391822999532\n",
      "Step 140/150 loss = 0.5135095345250347\n",
      "Step 141/150 loss = 0.5132851330601355\n",
      "Step 142/150 loss = 0.5130658152037264\n",
      "Step 143/150 loss = 0.512851424919696\n",
      "Step 144/150 loss = 0.5126418124982142\n",
      "Step 145/150 loss = 0.5124368342363763\n",
      "Step 146/150 loss = 0.512236352137372\n",
      "Step 147/150 loss = 0.5120402336270091\n",
      "Step 148/150 loss = 0.5118483512865011\n",
      "Step 149/150 loss = 0.511660582600499\n",
      "Step 150/150 loss = 0.5114768097194176\n",
      "Gradient final loss for w* = 0.5114768097194176\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 150\n",
    "gamma = 0.08\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones(x_tr.shape[1])\n",
    "\n",
    "gradient_w, gradient_loss = least_squares_GD(y_tr, x_tr, w_initial, max_iters, gamma) \n",
    "\n",
    "print(\"Gradient final loss for w* = \" + str(gradient_loss)) # 0.5114768097194176\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Third Model: Use GD : Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5006239447992366"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on the test data slice\n",
    "y_pred = predict_labels(gradient_w, x_te)\n",
    "# Check accuracy\n",
    "np.mean(y_te==y_pred) # 0.5006239447992366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fourth Model with Feature Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (54491,32) and (54491,) not aligned: 32 (dim 1) != 54491 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-a4f7a1f6e8d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpolynome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolynome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolynome\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-861c55fb9329>\u001b[0m in \u001b[0;36mleast_squares\u001b[0;34m(y, tx)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (54491,32) and (54491,) not aligned: 32 (dim 1) != 54491 (dim 0)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14a0607f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "degrees = [1, 2, 3, 4]\n",
    "\n",
    "# define the structure of the figure\n",
    "num_row = 2\n",
    "num_col = 2\n",
    "f, axs = plt.subplots(num_row, num_col)\n",
    "    \n",
    "for ind, degree in enumerate(degrees):\n",
    "\n",
    "        polynome = build_poly(x_tr,degree)\n",
    "        w = least_squares(y_tr, polynome.T)\n",
    "        MSE = compute_loss(y_tr, polynome, w)\n",
    "\n",
    "        print(\"Processing {i}th experiment, degree={d}, mse={loss}\".format( \n",
    "            i=ind + 1, d=degree, loss=MSE))\n",
    "        plot_fitted_curve(y_tr, x_tr, weights, degree, axs[ind // num_col][ind % num_col])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We observed that model 2 with ridge_regression method and lambda_ = 1e-10 (TODO, we have to check them all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, loss = ridge_regression(y, tX, 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: unzip the file\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'out.csv'\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

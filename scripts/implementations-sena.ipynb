{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "implementations.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nklingen/CS-433-Project-1/blob/master/scripts/implementations-sena.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S9i9NKottsGH",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SMH95nqZs22G",
        "colab": {}
      },
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NfUTXbA4Wiyw"
      },
      "source": [
        "# **1. Least Squares Gradient Descent**\n",
        "Linear regression using gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "44RSzhP1t8_v",
        "colab": {}
      },
      "source": [
        "def least_squares_GD(y, tx, initial_w, max_iters, gamma): \n",
        "        ws = [initial_w]\n",
        "        losses = []\n",
        "        w = initial_w\n",
        "    \n",
        "        for n_iter in range(max_iters):\n",
        "            # ***************************************************\n",
        "            # compute gradient and loss\n",
        "            gradient = compute_gradient(y, tx, w)\n",
        "            loss = compute_loss(y, tx, w)\n",
        "            # ***************************************************\n",
        "            # update w by gradient\n",
        "            w = w - gamma*gradient\n",
        "            # ***************************************************\n",
        "            # store w and loss\n",
        "            ws.append(w)\n",
        "            losses.append(loss)\n",
        "            \n",
        "        return ws[-1], losses[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zXLpV5RCWl2n"
      },
      "source": [
        "# **2. Least Squares Stochastic Gradient Descent**\n",
        "Linear regression using stochastic gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gqMQctqJueGt",
        "colab": {}
      },
      "source": [
        "def least_squares_SGD(y, tx, initial_w, max_iters, gamma): \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gTVYJq_nWoFk"
      },
      "source": [
        "# **3. Least Squares**\n",
        "Least squares regression using normal equations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W9PWr4-Tuh3v",
        "colab": {}
      },
      "source": [
        "def least_squares(y, tx):\n",
        "    w = np.linalg.solve(np.dot(tx.T,tx), np.dot(tx.T,y))\n",
        "    MSE = compute_loss(y, tx, w)\n",
        "    return w, MSE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QBflVp0FWpCf"
      },
      "source": [
        "# **4. Ridge Regression**\n",
        "Ridge regression using normal equations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rG2j5WYXukiZ",
        "colab": {}
      },
      "source": [
        "def ridge_regression(y, tx, lambda_):\n",
        "    a = (1/len(y))*(np.dot(tx.T,tx)) + 2*(lambda_*np.identity(tx.shape[1]))\n",
        "    b = (1/len(y))*np.dot(tx.T,y)\n",
        "    w = np.linalg.solve(a,b)\n",
        "    MSE = compute_loss(y, tx, w)\n",
        "    return w, MSE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UE6g9RoTWuOf"
      },
      "source": [
        "# **5. Logistic Regression**\n",
        "Logistic regression using gradient descent or SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B63ld585urMs",
        "colab": {}
      },
      "source": [
        "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9dO4-cnmWxpn"
      },
      "source": [
        "# **6. Regularized Logistic Regression**\n",
        "Regularized logistic regression using gradient descent\n",
        "or SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xEF8rZmeusk1",
        "colab": {}
      },
      "source": [
        "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qUjkIrVVW2RX"
      },
      "source": [
        "# **Helper Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nJW6HstRWfuq",
        "colab": {}
      },
      "source": [
        "def compute_loss(y, tx, w):\n",
        "    # Mean Squared Error\n",
        "    MSE = 1/(2*y.shape[0])*np.sum(np.square(y-np.dot(tx,w)))\n",
        "    return MSE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bc2EgK-TWfus",
        "colab": {}
      },
      "source": [
        "def compute_gradient(y, tx, w):\n",
        "    gradient = (-1/y.shape[0])*np.dot(tx.T,(y-np.dot(tx, w)))\n",
        "    return gradient"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB0HKmphl3F6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_poly(x, degree):\n",
        "    return np.array([x**j for j in range(degree+1)]).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf4wSwL7l3F9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_diff(x, y, degree, ratio, seed):\n",
        "        # split the data, and return train and test data\n",
        "        train_x, train_y, test_x, test_y = split_data(tX, y, ratio, seed)\n",
        "\n",
        "        # calcualte weight through least square\n",
        "        w, loss_star = least_squares(train_y,train_x)\n",
        "\n",
        "        # calculate RMSE for train and test data,\n",
        "        # and store them in rmse_tr and rmse_te respectively: TODO\n",
        "        mse_tr = compute_loss(train_y,train_x,w)\n",
        "        mse_te = compute_loss(test_y,test_x,w)\n",
        "\n",
        "        print(\"proportion={p}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
        "                  p=ratio, tr=mse_tr, te=mse_te))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXAaP9Xil3F_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
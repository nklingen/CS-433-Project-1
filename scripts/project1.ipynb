{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nklingen/CS-433-Project-1/blob/master/scripts/project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJwBaFjTmN7O"
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import datetime\n",
    "# %run implementations.ipynb\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCDzdiFmmN7U"
   },
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k4oJSK5smN7V"
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jg-e8g_YmN7Z"
   },
   "source": [
    "# 1. First model: Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yU8bkkXjmN7a"
   },
   "outputs": [],
   "source": [
    "# first to split the data into training and testing\n",
    "\n",
    "ratio = 0.8\n",
    "seed = 3\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX, y, ratio, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vD3AinsF47z0"
   },
   "source": [
    "### Plotting a correlation matrix to see which of our features are highly correlated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SGDAAuBPtv65"
   },
   "outputs": [],
   "source": [
    "feat_corr_matrix = np.corrcoef(tX.T)\n",
    "#print (feat_corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "eISotaXU0EdB",
    "outputId": "5a73efad-59cc-46a1-945d-649daf654caf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd578117be0>"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAloAAAH+CAYAAABEPqCPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERENrSAIgiAIghERDa0gCIIgCIIR8S8Hee51mLcWFAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 792x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(feat_corr_matrix, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(feat_corr_matrix, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D932-XqEmN7d"
   },
   "source": [
    "### 1.1 Training using Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "a-9UyqRfmN7f",
    "outputId": "813a1730-7b2b-4525-b39a-b7ad20004da2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  0.3394801404386872 \n",
      "Testing MSE: 0.3405695670310389\n"
     ]
    }
   ],
   "source": [
    "# First Model using least squares\n",
    "\n",
    "weights = least_squares(y_tr, x_tr)\n",
    "MSE_tr = compute_loss(y_tr, x_tr, weights)\n",
    "MSE_te = compute_loss(y_te, x_te, weights)\n",
    "\n",
    "print(\"Training MSE: \", MSE_tr, \"\\nTesting MSE:\", MSE_te)\n",
    "#0.3394801404386906  0.34056956713540926"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1S2eqB6ImN7j"
   },
   "source": [
    "### 1.2 Prediction using Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "e-S_4tmjmN7k",
    "outputId": "650d49f8-3abc-4803-ae06-6d440b10f2eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  0.74242\n"
     ]
    }
   ],
   "source": [
    "# predict on the test data slice\n",
    "y_pred = predict_labels(weights, x_te)\n",
    "# Check accuracy\n",
    "print(\"Prediction Accuracy: \", np.mean(y_te==y_pred)) # 0.34396"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0HNjp0nCmN7n"
   },
   "source": [
    "# 2. Second Model: Pre-processing removing all NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x3bdTD3MmN7o"
   },
   "outputs": [],
   "source": [
    "# removing all nan but there actually are none\n",
    "tX_2 = tX[~np.isnan(tX).any(axis=1)]\n",
    "\n",
    "# removing all rows with -999, there are 181886\n",
    "y = y[np.all(tX_2 != -999, axis=1)]\n",
    "tX_2 = tX_2[np.all(tX_2 != -999, axis=1)]\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX_2, y, ratio, seed)\n",
    "\n",
    "# then to standardize x_tr and x_te\n",
    "x_tr = standardize(x_tr)[0]\n",
    "x_te = standardize(x_te)[0]\n",
    "\n",
    "# next to add a column of ones\n",
    "y_tr, x_tr = build_model_data(x_tr, y_tr)\n",
    "y_te, x_te = build_model_data(x_te, y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMhTcYnMmN7s"
   },
   "source": [
    "### 2.1 Training using Least Squares removing NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "wyj2uTf0mN7t",
    "outputId": "a85eaed2-13fe-4409-9072-fe8744c23952"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  0.3686416204656051 \n",
      "Testing MSE: 0.541116014511367\n"
     ]
    }
   ],
   "source": [
    "weights = least_squares(y_tr, x_tr)\n",
    "MSE_tr = compute_loss(y_tr, x_tr, weights)\n",
    "MSE_te = compute_loss(y_te, x_te, weights)\n",
    "\n",
    "print(\"Training MSE: \", MSE_tr, \"\\nTesting MSE:\", MSE_te) \n",
    "#0.36864162046559024 0.5411866496785176"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ncYqU-AkmN7w"
   },
   "source": [
    "### 2.2 Prediction using Least Squares removing NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Zg_AVDfnmN7x",
    "outputId": "de7487ed-df47-4141-9b4a-119dcf597241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  0.6120531454158409\n"
     ]
    }
   ],
   "source": [
    "# predict on the test data slice\n",
    "y_pred = predict_labels(weights, x_te)\n",
    "# Check accuracy\n",
    "print(\"Prediction Accuracy: \", np.mean(y_te==y_pred)) # 0.6121265506863393"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2nwYZm7CmN70"
   },
   "source": [
    "# 3. Third Model: Better Pre-processing replacing NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KTro2jBfmN72"
   },
   "outputs": [],
   "source": [
    "# replace nans with mean\n",
    "\n",
    "tX_3 = replace_nan_with_mean(tX, -999)\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX_3, y, ratio, seed)\n",
    "\n",
    "# then to standardize x_tr and x_te\n",
    "x_tr = standardize(x_tr)[0]\n",
    "x_te = standardize(x_te)[0]\n",
    "\n",
    "# next to add a column of ones\n",
    "y_tr, x_tr = build_model_data(x_tr, y_tr)\n",
    "y_te, x_te = build_model_data(x_te, y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWrmk8hCmN75"
   },
   "source": [
    "### 3.1 Training using Least Squares replacing NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z1itjN60mN76",
    "outputId": "6d9610fe-5706-4035-a30c-e64955ebe0ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  0.4978447945914067 \n",
      "Testing MSE: 0.6275084689089252\n"
     ]
    }
   ],
   "source": [
    "weights = least_squares(y_tr, x_tr)\n",
    "MSE_tr = compute_loss(y_tr, x_tr, weights)\n",
    "MSE_te = compute_loss(y_te, x_te, weights)\n",
    "\n",
    "print(\"Training MSE: \", MSE_tr, \"\\nTesting MSE:\", MSE_te) \n",
    "#0.4978447945914067  0.6275084689089252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjB1EK3DmN79"
   },
   "source": [
    "### 3.2 Prediction using Least Squares replacing NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p4c-LqCEmN7-",
    "outputId": "a3f02f2f-3305-4a7a-9a8e-c0f18f28ab85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  0.5331424796300375\n"
     ]
    }
   ],
   "source": [
    "# predict on the test data slice\n",
    "y_pred = predict_labels(weights, x_te)\n",
    "# Check accuracy\n",
    "print(\"Prediction Accuracy: \", np.mean(y_te==y_pred)) # 0.6121265506863393"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12zEyThcmN8B"
   },
   "source": [
    "# 4. Fourth Model: Feature Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ttr_E4CBmN8C"
   },
   "source": [
    "### 4. 1 Feature Augmentation on Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "99j_wg3xmN8D",
    "outputId": "9ddd4e39-9e8d-4838-d9b8-b328eafb6812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1th experiment, degree=1, rmse=0.8239257849571571\n",
      "Prediction Accuracy:  0.74402\n",
      "\n",
      "Processing 2th experiment, degree=2, rmse=0.7990539549852868\n",
      "Prediction Accuracy:  0.76944\n",
      "\n",
      "Processing 3th experiment, degree=3, rmse=0.788424663107187\n",
      "Prediction Accuracy:  0.77844\n",
      "\n",
      "Processing 4th experiment, degree=4, rmse=0.7786936013526474\n",
      "Prediction Accuracy:  0.7884\n",
      "\n",
      "Processing 5th experiment, degree=5, rmse=0.7979556716445746\n",
      "Prediction Accuracy:  0.7775\n",
      "\n",
      "Processing 6th experiment, degree=6, rmse=0.7649938635511518\n",
      "Prediction Accuracy:  0.79966\n",
      "\n",
      "Processing 7th experiment, degree=7, rmse=2893718.602071626\n",
      "Prediction Accuracy:  0.47306\n",
      "\n",
      "Processing 8th experiment, degree=8, rmse=2266262.8281303146\n",
      "Prediction Accuracy:  0.5427\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratio = 0.8\n",
    "seed = 1\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX, y, ratio, seed)\n",
    "\n",
    "degrees = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "\n",
    "for ind, degree in enumerate(degrees):\n",
    "\n",
    "    polynome = build_poly(x_tr,degree)\n",
    "    weights = least_squares(y_tr, polynome)\n",
    "    MSE = compute_loss(y_tr, polynome, weights)\n",
    "    rmse = np.sqrt(2*MSE)\n",
    "    \n",
    "    polynome_te = build_poly(x_te,degree)\n",
    "    y_pred = predict_labels(weights, polynome_te)\n",
    "\n",
    "    print(\"Processing {i}th experiment, degree={d}, rmse={loss}\".format( \n",
    "        i=ind + 1, d=degree, loss=rmse))\n",
    "    print(\"Prediction Accuracy: \", str(np.mean(y_te == y_pred)) + \"\\n\")\n",
    "    \n",
    "# Degree of 4 seems optimal (6 is overfitting it seems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vf-HClpPmN8G"
   },
   "source": [
    "### 4.2 Feature Augementation removing NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Rz2uGzMmN8H",
    "outputId": "3617c3e3-5800-4aab-fb99-a7d64e9f8cc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1th experiment, degree=1, rmse=0.8595472059393301\n",
      "Prediction Accuracy:  0.7209865668354988\n",
      "\n",
      "Processing 2th experiment, degree=2, rmse=0.8180119618944975\n",
      "Prediction Accuracy:  0.7653233502165455\n",
      "\n",
      "Processing 3th experiment, degree=3, rmse=0.7950250798192138\n",
      "Prediction Accuracy:  0.7857300154151068\n",
      "\n",
      "Processing 4th experiment, degree=4, rmse=0.7824540912580449\n",
      "Prediction Accuracy:  0.7980621008588417\n",
      "\n",
      "Processing 5th experiment, degree=5, rmse=0.7779028561821488\n",
      "Prediction Accuracy:  0.7998238273508038\n",
      "\n",
      "Processing 6th experiment, degree=6, rmse=0.7726834497676207\n",
      "Prediction Accuracy:  0.8031270645232328\n",
      "\n",
      "Processing 7th experiment, degree=7, rmse=0.7557874987888077\n",
      "Prediction Accuracy:  0.8123027233355354\n",
      "\n",
      "Processing 8th experiment, degree=8, rmse=0.7371216575803359\n",
      "Prediction Accuracy:  0.8225060559348161\n",
      "\n",
      "Processing 9th experiment, degree=9, rmse=0.7285225907993939\n",
      "Prediction Accuracy:  0.827350803787712\n",
      "\n",
      "Processing 10th experiment, degree=10, rmse=0.7278725366831396\n",
      "Prediction Accuracy:  0.827350803787712\n",
      "\n",
      "Processing 11th experiment, degree=11, rmse=0.8283684072325078\n",
      "Prediction Accuracy:  0.7668648608970124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratio = 0.8\n",
    "seed = 1\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX_2, y, ratio, seed)\n",
    "\n",
    "degrees = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "\n",
    "\n",
    "for ind, degree in enumerate(degrees):\n",
    "\n",
    "    polynome = build_poly(x_tr,degree)\n",
    "    weights = least_squares(y_tr, polynome)\n",
    "    MSE = compute_loss(y_tr, polynome, weights)\n",
    "    rmse = np.sqrt(2*MSE)\n",
    "    \n",
    "    polynome_te = build_poly(x_te,degree)\n",
    "    y_pred = predict_labels(weights, polynome_te)\n",
    "\n",
    "    print(\"Processing {i}th experiment, degree={d}, rmse={loss}\".format( \n",
    "        i=ind + 1, d=degree, loss=rmse))\n",
    "    print(\"Prediction Accuracy: \", str(np.mean(y_te == y_pred)) + \"\\n\")\n",
    "    \n",
    "# Degree of 9 seems optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2aEvtq_JmN8J"
   },
   "source": [
    "###  Feature Augementation replacing NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AN12G_8umN8K",
    "outputId": "1899e416-c13d-486b-9241-3efdbdb03309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1th experiment, degree=1, rmse=0.9978758293721322\n",
      "Prediction Accuracy:  0.5319679953020627\n",
      "\n",
      "Processing 2th experiment, degree=2, rmse=0.9977151297869898\n",
      "Prediction Accuracy:  0.5308669162445864\n",
      "\n",
      "Processing 3th experiment, degree=3, rmse=0.9974064350165527\n",
      "Prediction Accuracy:  0.5316009689495705\n",
      "\n",
      "Processing 4th experiment, degree=4, rmse=0.9970844513748015\n",
      "Prediction Accuracy:  0.5286647581296338\n",
      "\n",
      "Processing 5th experiment, degree=5, rmse=0.996862156198522\n",
      "Prediction Accuracy:  0.5277104896131543\n",
      "\n",
      "Processing 6th experiment, degree=6, rmse=0.9966915013548663\n",
      "Prediction Accuracy:  0.5262423842031858\n",
      "\n",
      "Processing 7th experiment, degree=7, rmse=0.9964604356881909\n",
      "Prediction Accuracy:  0.52697643690817\n",
      "\n",
      "Processing 8th experiment, degree=8, rmse=0.9961572986651697\n",
      "Prediction Accuracy:  0.5287381634001321\n",
      "\n",
      "Processing 9th experiment, degree=9, rmse=0.9959695949502358\n",
      "Prediction Accuracy:  0.52697643690817\n",
      "\n",
      "Processing 10th experiment, degree=10, rmse=0.9955706395713501\n",
      "Prediction Accuracy:  0.5269030316376716\n",
      "\n",
      "Processing 11th experiment, degree=11, rmse=0.9960747334508928\n",
      "Prediction Accuracy:  0.5266094105556779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratio = 0.8\n",
    "seed = 1\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX_3, y, ratio, seed)\n",
    "\n",
    "degrees = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "\n",
    "\n",
    "for ind, degree in enumerate(degrees):\n",
    "\n",
    "    polynome = build_poly(x_tr,degree)\n",
    "    weights = least_squares(y_tr, polynome)\n",
    "    MSE = compute_loss(y_tr, polynome, weights)\n",
    "    rmse = np.sqrt(2*MSE)\n",
    "    \n",
    "    polynome_te = build_poly(x_te,degree)\n",
    "    y_pred = predict_labels(weights, polynome_te)\n",
    "\n",
    "    print(\"Processing {i}th experiment, degree={d}, rmse={loss}\".format( \n",
    "        i=ind + 1, d=degree, loss=rmse))\n",
    "    print(\"Prediction Accuracy: \", str(np.mean(y_te == y_pred)) + \"\\n\")\n",
    "    \n",
    "# Degree of 9 seems optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y6P6-wNMmN8M"
   },
   "outputs": [],
   "source": [
    " ## WORK IN PROGRESS ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qtBkIHnvmN8O"
   },
   "source": [
    "# 4. Model 1 with Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgwalcU7mN8P"
   },
   "outputs": [],
   "source": [
    "## we noticed that the first model had the best accuracy, without any pre-processing.\n",
    "## we then applied ridge regression to the first model.\n",
    "ratio = 0.8\n",
    "seed = 1\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX, y, ratio, seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XJsnYe6TmN8S"
   },
   "source": [
    "### 4.1 Ridge Regressiong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_bWVUHTmN8U",
    "outputId": "28fcf89e-8565-42c1-b7e1-7e7f58be9092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda=0; Loss=0.33942684955862673\n",
      "Prediction Accuracy:  0.74402\n",
      "\n",
      "Lambda=1e-15; Loss=0.33942684955862695\n",
      "Prediction Accuracy:  0.74402\n",
      "\n",
      "Lambda=1e-10; Loss=0.33942684959099034\n",
      "Prediction Accuracy:  0.74402\n",
      "\n",
      "Lambda=1e-05; Loss=0.3394308610386454\n",
      "Prediction Accuracy:  0.744\n",
      "\n",
      "Lambda=1; Loss=0.35329072520671573\n",
      "Prediction Accuracy:  0.73516\n",
      "\n",
      "Lambda=10.0; Loss=0.35648911776731346\n",
      "Prediction Accuracy:  0.734\n",
      "\n",
      "Lambda=1000; Loss=0.3820216654176733\n",
      "Prediction Accuracy:  0.69854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0, 1e-15, 1e-10, 1e-5, 1, 1e1, 1000]\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    \n",
    "    w, loss = ridge_regression(y_tr, x_tr, lambda_)\n",
    "    y_pred = predict_labels(w, x_te)\n",
    "    print(\"Lambda=\" + str(lambda_)+\"; Loss=\"+str(loss))\n",
    "\n",
    "    # predict on the test data slice\n",
    "    y_pred = predict_labels(w, x_te)\n",
    "    # Check accuracy\n",
    "    print(\"Prediction Accuracy: \", str(np.mean(y_te == y_pred)) + \"\\n\")\n",
    "    # Best for lambda = 0, Accuracy = 0.744"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6CsNEyoNmN8Z"
   },
   "source": [
    "### 4.2 Regularized Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-EcqTERUmN8a"
   },
   "outputs": [],
   "source": [
    "lambdas = [0, 1e-15, 1e-10, 1e-5, 1, 1e1, 1000]\n",
    "\n",
    "max_iter = 10\n",
    "gamma = 0.01\n",
    "lambda_ = 0.3\n",
    "threshold = 1e-8\n",
    "losses = []\n",
    "\n",
    "w = np.zeros((x_tr.shape[1], 1))\n",
    "\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_penalized_gradient(y_tr, x_tr, w, gamma, lambda_)\n",
    "    # log info\n",
    "    if iter % 100 == 0:\n",
    "        print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "    # converge criterion\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "# visualization\n",
    "visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_penalized_gradient_descent\")\n",
    "print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gRuD4Eq1mN8f"
   },
   "source": [
    "### 2.2.1  GD for Second Model : train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mOSixt3LmN8f",
    "outputId": "054b0cb0-c80d-4d19-ffd3-5ddaeffa5a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/150 loss = 16.742723002367942\n",
      "Step 2/150 loss = 13.640113171583614\n",
      "Step 3/150 loss = 11.197208614873603\n",
      "Step 4/150 loss = 9.241386941981181\n",
      "Step 5/150 loss = 7.6637217309515515\n",
      "Step 6/150 loss = 6.386469086966029\n",
      "Step 7/150 loss = 5.350289705071333\n",
      "Step 8/150 loss = 4.5084324324530805\n",
      "Step 9/150 loss = 3.8235432932229134\n",
      "Step 10/150 loss = 3.2655939588419374\n",
      "Step 11/150 loss = 2.8103795766576725\n",
      "Step 12/150 loss = 2.4383641700028043\n",
      "Step 13/150 loss = 2.133768862969578\n",
      "Step 14/150 loss = 1.883843255860377\n",
      "Step 15/150 loss = 1.6782801536029566\n",
      "Step 16/150 loss = 1.5087443685174633\n",
      "Step 17/150 loss = 1.3684929531113643\n",
      "Step 18/150 loss = 1.2520689442005608\n",
      "Step 19/150 loss = 1.1550542964765451\n",
      "Step 20/150 loss = 1.0738705085735656\n",
      "Step 21/150 loss = 1.005617694447243\n",
      "Step 22/150 loss = 0.9479446554141303\n",
      "Step 23/150 loss = 0.8989439562644814\n",
      "Step 24/150 loss = 0.8570671735948378\n",
      "Step 25/150 loss = 0.8210564219510524\n",
      "Step 26/150 loss = 0.7898890181788069\n",
      "Step 27/150 loss = 0.7627327523082615\n",
      "Step 28/150 loss = 0.7389097230556666\n",
      "Step 29/150 loss = 0.7178670906709678\n",
      "Step 30/150 loss = 0.6991534179387848\n",
      "Step 31/150 loss = 0.6823995265631582\n",
      "Step 32/150 loss = 0.6673030029310052\n",
      "Step 33/150 loss = 0.6536156540088478\n",
      "Step 34/150 loss = 0.6411333486518163\n",
      "Step 35/150 loss = 0.6296877881482006\n",
      "Step 36/150 loss = 0.6191398374234844\n",
      "Step 37/150 loss = 0.6093741190414087\n",
      "Step 38/150 loss = 0.600294629233899\n",
      "Step 39/150 loss = 0.5918211812997786\n",
      "Step 40/150 loss = 0.5838865189560527\n",
      "Step 41/150 loss = 0.5764339723158559\n",
      "Step 42/150 loss = 0.5694155534830965\n",
      "Step 43/150 loss = 0.5627904084077481\n",
      "Step 44/150 loss = 0.5565235575348215\n",
      "Step 45/150 loss = 0.5505848706282405\n",
      "Step 46/150 loss = 0.544948231542285\n",
      "Step 47/150 loss = 0.5395908571195215\n",
      "Step 48/150 loss = 0.5344927411958731\n",
      "Step 49/150 loss = 0.5296362001981282\n",
      "Step 50/150 loss = 0.5250055012750214\n",
      "Step 51/150 loss = 0.520586557510647\n",
      "Step 52/150 loss = 0.516366677690452\n",
      "Step 53/150 loss = 0.512334360456416\n",
      "Step 54/150 loss = 0.508479124605126\n",
      "Step 55/150 loss = 0.5047913688359527\n",
      "Step 56/150 loss = 0.5012622555156313\n",
      "Step 57/150 loss = 0.4978836140462908\n",
      "Step 58/150 loss = 0.4946478602516449\n",
      "Step 59/150 loss = 0.49154792886734355\n",
      "Step 60/150 loss = 0.4885772167660517\n",
      "Step 61/150 loss = 0.4857295349897058\n",
      "Step 62/150 loss = 0.48299906802004466\n",
      "Step 63/150 loss = 0.48038033900968197\n",
      "Step 64/150 loss = 0.47786817993243186\n",
      "Step 65/150 loss = 0.47545770580367885\n",
      "Step 66/150 loss = 0.4731442922776409\n",
      "Step 67/150 loss = 0.4709235560552418\n",
      "Step 68/150 loss = 0.4687913376394558\n",
      "Step 69/150 loss = 0.4667436860588944\n",
      "Step 70/150 loss = 0.4647768452486903\n",
      "Step 71/150 loss = 0.4628872418333283\n",
      "Step 72/150 loss = 0.46107147410135857\n",
      "Step 73/150 loss = 0.4593263019988414\n",
      "Step 74/150 loss = 0.45764863799848043\n",
      "Step 75/150 loss = 0.4560355387259724\n",
      "Step 76/150 loss = 0.4544841972451777\n",
      "Step 77/150 loss = 0.4529919359201264\n",
      "Step 78/150 loss = 0.4515561997853129\n",
      "Step 79/150 loss = 0.45017455036673865\n",
      "Step 80/150 loss = 0.44884465990520145\n",
      "Step 81/150 loss = 0.44756430594075247\n",
      "Step 82/150 loss = 0.44633136622335856\n",
      "Step 83/150 loss = 0.4451438139198544\n",
      "Step 84/150 loss = 0.44399971309143355\n",
      "Step 85/150 loss = 0.4428972144193932\n",
      "Step 86/150 loss = 0.4418345511597172\n",
      "Step 87/150 loss = 0.44081003530948076\n",
      "Step 88/150 loss = 0.439822053970068\n",
      "Step 89/150 loss = 0.4388690658938778\n",
      "Step 90/150 loss = 0.4379495982026175\n",
      "Step 91/150 loss = 0.43706224326649035\n",
      "Step 92/150 loss = 0.4362056557346088\n",
      "Step 93/150 loss = 0.4353785497078481\n",
      "Step 94/150 loss = 0.43457969604611246\n",
      "Step 95/150 loss = 0.43380791980264066\n",
      "Step 96/150 loss = 0.43306209777855803\n",
      "Step 97/150 loss = 0.4323411561913749\n",
      "Step 98/150 loss = 0.4316440684515836\n",
      "Step 99/150 loss = 0.43096985304189783\n",
      "Step 100/150 loss = 0.4303175714940281\n",
      "Step 101/150 loss = 0.4296863264582086\n",
      "Step 102/150 loss = 0.42907525986097433\n",
      "Step 103/150 loss = 0.42848355114695047\n",
      "Step 104/150 loss = 0.4279104156006517\n",
      "Step 105/150 loss = 0.4273551027445123\n",
      "Step 106/150 loss = 0.4268168948095646\n",
      "Step 107/150 loss = 0.4262951052753771\n",
      "Step 108/150 loss = 0.42578907747603184\n",
      "Step 109/150 loss = 0.4252981832690858\n",
      "Step 110/150 loss = 0.424821821764614\n",
      "Step 111/150 loss = 0.4243594181115716\n",
      "Step 112/150 loss = 0.4239104223388498\n",
      "Step 113/150 loss = 0.4234743082485231\n",
      "Step 114/150 loss = 0.4230505723589071\n",
      "Step 115/150 loss = 0.422638732895159\n",
      "Step 116/150 loss = 0.42223832882525714\n",
      "Step 117/150 loss = 0.42184891893929843\n",
      "Step 118/150 loss = 0.42147008097015043\n",
      "Step 119/150 loss = 0.421101410753582\n",
      "Step 120/150 loss = 0.4207425214260851\n",
      "Step 121/150 loss = 0.42039304265868344\n",
      "Step 122/150 loss = 0.42005261992510057\n",
      "Step 123/150 loss = 0.4197209138027333\n",
      "Step 124/150 loss = 0.4193975993049524\n",
      "Step 125/150 loss = 0.4190823652433125\n",
      "Step 126/150 loss = 0.41877491361832464\n",
      "Step 127/150 loss = 0.41847495903750165\n",
      "Step 128/150 loss = 0.4181822281594493\n",
      "Step 129/150 loss = 0.4178964591628246\n",
      "Step 130/150 loss = 0.4176174012390471\n",
      "Step 131/150 loss = 0.4173448141076879\n",
      "Step 132/150 loss = 0.41707846755351874\n",
      "Step 133/150 loss = 0.4168181409842444\n",
      "Step 134/150 loss = 0.4165636230079889\n",
      "Step 135/150 loss = 0.4163147110296451\n",
      "Step 136/150 loss = 0.4160712108652412\n",
      "Step 137/150 loss = 0.41583293637351065\n",
      "Step 138/150 loss = 0.41559970910389793\n",
      "Step 139/150 loss = 0.4153713579602538\n",
      "Step 140/150 loss = 0.41514771887952084\n",
      "Step 141/150 loss = 0.4149286345247334\n",
      "Step 142/150 loss = 0.4147139539916889\n",
      "Step 143/150 loss = 0.41450353252867783\n",
      "Step 144/150 loss = 0.41429723126868334\n",
      "Step 145/150 loss = 0.41409491697349454\n",
      "Step 146/150 loss = 0.4138964617891952\n",
      "Step 147/150 loss = 0.4137017430125199\n",
      "Step 148/150 loss = 0.41351064286758865\n",
      "Step 149/150 loss = 0.4133230482925556\n",
      "Step 150/150 loss = 0.4131388507357264\n",
      "Gradient final loss for w* = 0.4131388507357264\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 150\n",
    "gamma = 0.08\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones(x_tr.shape[1])\n",
    "\n",
    "gradient_w, gradient_loss = least_squares_GD(y_tr, x_tr, w_initial, max_iters, gamma) \n",
    "\n",
    "print(\"Gradient final loss for w* = \" + str(gradient_loss)) # 0.4131388507357264"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1hypHlamN8h"
   },
   "source": [
    "### 2.2.2  GD for Second Model : predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "238Dy2vRmN8i",
    "outputId": "6e2e7345-1e13-4c76-ae27-1c07c0ca780b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6810115431906186\n",
      "0.687954195111209\n"
     ]
    }
   ],
   "source": [
    "# predict on the train data slice using the gradient_w\n",
    "y_pred = predict_labels(gradient_w, x_tr)\n",
    "# Check accuracy\n",
    "print(np.mean(y_tr == y_pred)) # 0.6810115431906186\n",
    "\n",
    "# predict on the test data slice\n",
    "y_pred = predict_labels(gradient_w, x_te)\n",
    "# Check accuracy\n",
    "print(np.mean(y_te == y_pred)) # 0.687954195111209"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oabfrVHbmN8k"
   },
   "source": [
    "### 2.3  Ridge regression for Second Model : train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MZHcfmv8mN8l"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UqlLtJHJmN8m"
   },
   "source": [
    "#  Logistic regression for Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yXwARnNRmN8n",
    "outputId": "21bddea8-c088-46f2-b286-a4c0f0b8825e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (1/2000): loss=0.6917259022280502\n",
      "Logistic Regression (2/2000): loss=0.6872342727298547\n",
      "Logistic Regression (3/2000): loss=0.6832335819969858\n",
      "Logistic Regression (4/2000): loss=0.6781414732496351\n",
      "Logistic Regression (5/2000): loss=0.6739916255158099\n",
      "Logistic Regression (6/2000): loss=0.6708053172464308\n",
      "Logistic Regression (7/2000): loss=0.6686612226517009\n",
      "Logistic Regression (8/2000): loss=0.6643994087613666\n",
      "Logistic Regression (9/2000): loss=0.6606623426823868\n",
      "Logistic Regression (10/2000): loss=0.6586863979844305\n",
      "Logistic Regression (11/2000): loss=0.6561880771900984\n",
      "Logistic Regression (12/2000): loss=0.6537547500238531\n",
      "Logistic Regression (13/2000): loss=0.6504786643993647\n",
      "Logistic Regression (14/2000): loss=0.6484865383981637\n",
      "Logistic Regression (15/2000): loss=0.6477972721571535\n",
      "Logistic Regression (16/2000): loss=0.6470696055431121\n",
      "Logistic Regression (17/2000): loss=0.6451588772521825\n",
      "Logistic Regression (18/2000): loss=0.6450234922638917\n",
      "Logistic Regression (19/2000): loss=0.6435498239795913\n",
      "Logistic Regression (20/2000): loss=0.641849586526123\n",
      "Logistic Regression (21/2000): loss=0.6410598995979556\n",
      "Logistic Regression (22/2000): loss=0.6393287530310958\n",
      "Logistic Regression (23/2000): loss=0.6383969896166123\n",
      "Logistic Regression (24/2000): loss=0.6378158774148653\n",
      "Logistic Regression (25/2000): loss=0.6365374171678846\n",
      "Logistic Regression (26/2000): loss=0.6352827524587534\n",
      "Logistic Regression (27/2000): loss=0.635251032191965\n",
      "Logistic Regression (28/2000): loss=0.6347740098500757\n",
      "Logistic Regression (29/2000): loss=0.6341544142942885\n",
      "Logistic Regression (30/2000): loss=0.6344652060878613\n",
      "Logistic Regression (31/2000): loss=0.6344368074042693\n",
      "Logistic Regression (32/2000): loss=0.6338485065613197\n",
      "Logistic Regression (33/2000): loss=0.6339492724122039\n",
      "Logistic Regression (34/2000): loss=0.6336576430734279\n",
      "Logistic Regression (35/2000): loss=0.6332268401512545\n",
      "Logistic Regression (36/2000): loss=0.632281340345305\n",
      "Logistic Regression (37/2000): loss=0.633256470721797\n",
      "Logistic Regression (38/2000): loss=0.6335928594709513\n",
      "Logistic Regression (39/2000): loss=0.6329142380463493\n",
      "Logistic Regression (40/2000): loss=0.6324213376404071\n",
      "Logistic Regression (41/2000): loss=0.6324295256533127\n",
      "Logistic Regression (42/2000): loss=0.632342175608425\n",
      "Logistic Regression (43/2000): loss=0.6324917558205374\n",
      "Logistic Regression (44/2000): loss=0.631724448277694\n",
      "Logistic Regression (45/2000): loss=0.6317133208847774\n",
      "Logistic Regression (46/2000): loss=0.631058345688564\n",
      "Logistic Regression (47/2000): loss=0.6304427309206935\n",
      "Logistic Regression (48/2000): loss=0.6299469392948245\n",
      "Logistic Regression (49/2000): loss=0.6298625373363174\n",
      "Logistic Regression (50/2000): loss=0.629597446762441\n",
      "Logistic Regression (51/2000): loss=0.6293018310192432\n",
      "Logistic Regression (52/2000): loss=0.6292994622841899\n",
      "Logistic Regression (53/2000): loss=0.6288090796717796\n",
      "Logistic Regression (54/2000): loss=0.6285149481452675\n",
      "Logistic Regression (55/2000): loss=0.628352191027333\n",
      "Logistic Regression (56/2000): loss=0.6280045935240585\n",
      "Logistic Regression (57/2000): loss=0.6277951782367917\n",
      "Logistic Regression (58/2000): loss=0.6277146135072634\n",
      "Logistic Regression (59/2000): loss=0.6278521802476086\n",
      "Logistic Regression (60/2000): loss=0.6279965748709158\n",
      "Logistic Regression (61/2000): loss=0.6280177384708142\n",
      "Logistic Regression (62/2000): loss=0.6278847234135199\n",
      "Logistic Regression (63/2000): loss=0.6277289667521118\n",
      "Logistic Regression (64/2000): loss=0.6279348565872658\n",
      "Logistic Regression (65/2000): loss=0.6276535690541295\n",
      "Logistic Regression (66/2000): loss=0.6274463615593129\n",
      "Logistic Regression (67/2000): loss=0.6273354342737438\n",
      "Logistic Regression (68/2000): loss=0.6273968608351811\n",
      "Logistic Regression (69/2000): loss=0.6272820566118055\n",
      "Logistic Regression (70/2000): loss=0.627162052671867\n",
      "Logistic Regression (71/2000): loss=0.627108037900097\n",
      "Logistic Regression (72/2000): loss=0.6269497002169753\n",
      "Logistic Regression (73/2000): loss=0.626829535911798\n",
      "Logistic Regression (74/2000): loss=0.6267642058810813\n",
      "Logistic Regression (75/2000): loss=0.6267309963768868\n",
      "Logistic Regression (76/2000): loss=0.6267071031945628\n",
      "Logistic Regression (77/2000): loss=0.626610026232771\n",
      "Logistic Regression (78/2000): loss=0.6265220092772068\n",
      "Logistic Regression (79/2000): loss=0.6264661196494737\n",
      "Logistic Regression (80/2000): loss=0.6264265201389463\n",
      "Logistic Regression (81/2000): loss=0.626386074127672\n",
      "Logistic Regression (82/2000): loss=0.6262928270252127\n",
      "Logistic Regression (83/2000): loss=0.6262892299157924\n",
      "Logistic Regression (84/2000): loss=0.6262699535858656\n",
      "Logistic Regression (85/2000): loss=0.6262590529003387\n",
      "Logistic Regression (86/2000): loss=0.6261576724000598\n",
      "Logistic Regression (87/2000): loss=0.6260665773108027\n",
      "Logistic Regression (88/2000): loss=0.626024890818881\n",
      "Logistic Regression (89/2000): loss=0.626010265640388\n",
      "Logistic Regression (90/2000): loss=0.6259189451929568\n",
      "Logistic Regression (91/2000): loss=0.6258287851173017\n",
      "Logistic Regression (92/2000): loss=0.6257844403939646\n",
      "Logistic Regression (93/2000): loss=0.6257530660374799\n",
      "Logistic Regression (94/2000): loss=0.6256954870655084\n",
      "Logistic Regression (95/2000): loss=0.6256282175140396\n",
      "Logistic Regression (96/2000): loss=0.6255737640083319\n",
      "Logistic Regression (97/2000): loss=0.6255585713004974\n",
      "Logistic Regression (98/2000): loss=0.625510072847426\n",
      "Logistic Regression (99/2000): loss=0.6254402195531176\n",
      "Logistic Regression (100/2000): loss=0.6254150094695614\n",
      "Logistic Regression (101/2000): loss=0.6253838391382687\n",
      "Logistic Regression (102/2000): loss=0.6252908352280785\n",
      "Logistic Regression (103/2000): loss=0.6252249597178509\n",
      "Logistic Regression (104/2000): loss=0.625185262678643\n",
      "Logistic Regression (105/2000): loss=0.6251197015286927\n",
      "Logistic Regression (106/2000): loss=0.6250620334177769\n",
      "Logistic Regression (107/2000): loss=0.6250485142366629\n",
      "Logistic Regression (108/2000): loss=0.6251096047390509\n",
      "Logistic Regression (109/2000): loss=0.6250610092143252\n",
      "Logistic Regression (110/2000): loss=0.6250757205894774\n",
      "Logistic Regression (111/2000): loss=0.6250337532648091\n",
      "Logistic Regression (112/2000): loss=0.6249807627485886\n",
      "Logistic Regression (113/2000): loss=0.624897749485546\n",
      "Logistic Regression (114/2000): loss=0.6248374550088462\n",
      "Logistic Regression (115/2000): loss=0.6247926247074845\n",
      "Logistic Regression (116/2000): loss=0.624790226382999\n",
      "Logistic Regression (117/2000): loss=0.6247739756962003\n",
      "Logistic Regression (118/2000): loss=0.6247005458743214\n",
      "Logistic Regression (119/2000): loss=0.6246726644326238\n",
      "Logistic Regression (120/2000): loss=0.6246258027296276\n",
      "Logistic Regression (121/2000): loss=0.6246213082265091\n",
      "Logistic Regression (122/2000): loss=0.6245474258753816\n",
      "Logistic Regression (123/2000): loss=0.6245300187833759\n",
      "Logistic Regression (124/2000): loss=0.6244660638232055\n",
      "Logistic Regression (125/2000): loss=0.6244222941998921\n",
      "Logistic Regression (126/2000): loss=0.624347044245514\n",
      "Logistic Regression (127/2000): loss=0.6242741699173303\n",
      "Logistic Regression (128/2000): loss=0.6242331673566723\n",
      "Logistic Regression (129/2000): loss=0.6242573780522361\n",
      "Logistic Regression (130/2000): loss=0.6242102413405981\n",
      "Logistic Regression (131/2000): loss=0.624095883409953\n",
      "Logistic Regression (132/2000): loss=0.6240558037833744\n",
      "Logistic Regression (133/2000): loss=0.6240055436715618\n",
      "Logistic Regression (134/2000): loss=0.6240260219573633\n",
      "Logistic Regression (135/2000): loss=0.6239487694136495\n",
      "Logistic Regression (136/2000): loss=0.6239084977980501\n",
      "Logistic Regression (137/2000): loss=0.6238833386089323\n",
      "Logistic Regression (138/2000): loss=0.6237441211879942\n",
      "Logistic Regression (139/2000): loss=0.6237294342808462\n",
      "Logistic Regression (140/2000): loss=0.6237254425906383\n",
      "Logistic Regression (141/2000): loss=0.6236388852029342\n",
      "Logistic Regression (142/2000): loss=0.6236060640936504\n",
      "Logistic Regression (143/2000): loss=0.6235557397768937\n",
      "Logistic Regression (144/2000): loss=0.6235057052493088\n",
      "Logistic Regression (145/2000): loss=0.6234774074252408\n",
      "Logistic Regression (146/2000): loss=0.6235244773546711\n",
      "Logistic Regression (147/2000): loss=0.6234060885859289\n",
      "Logistic Regression (148/2000): loss=0.6234018768872202\n",
      "Logistic Regression (149/2000): loss=0.623396696581475\n",
      "Logistic Regression (150/2000): loss=0.6233755412879725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (151/2000): loss=0.6232972102728058\n",
      "Logistic Regression (152/2000): loss=0.6232125143311411\n",
      "Logistic Regression (153/2000): loss=0.6232549370005677\n",
      "Logistic Regression (154/2000): loss=0.6232487075199068\n",
      "Logistic Regression (155/2000): loss=0.623193444790672\n",
      "Logistic Regression (156/2000): loss=0.6231297699930868\n",
      "Logistic Regression (157/2000): loss=0.6231263679983194\n",
      "Logistic Regression (158/2000): loss=0.6230655578658937\n",
      "Logistic Regression (159/2000): loss=0.6230355580893265\n",
      "Logistic Regression (160/2000): loss=0.6230608519207745\n",
      "Logistic Regression (161/2000): loss=0.6231010016073661\n",
      "Logistic Regression (162/2000): loss=0.6230657005711558\n",
      "Logistic Regression (163/2000): loss=0.6229625311494973\n",
      "Logistic Regression (164/2000): loss=0.622887473321195\n",
      "Logistic Regression (165/2000): loss=0.6228223252610894\n",
      "Logistic Regression (166/2000): loss=0.6228304100628129\n",
      "Logistic Regression (167/2000): loss=0.622784898740348\n",
      "Logistic Regression (168/2000): loss=0.6228290762797205\n",
      "Logistic Regression (169/2000): loss=0.622732711713641\n",
      "Logistic Regression (170/2000): loss=0.6227210023325653\n",
      "Logistic Regression (171/2000): loss=0.6226659543718799\n",
      "Logistic Regression (172/2000): loss=0.6226693347095148\n",
      "Logistic Regression (173/2000): loss=0.6226505336989404\n",
      "Logistic Regression (174/2000): loss=0.6226371033627178\n",
      "Logistic Regression (175/2000): loss=0.6226415259031735\n",
      "Logistic Regression (176/2000): loss=0.6226266961004313\n",
      "Logistic Regression (177/2000): loss=0.6225251789696533\n",
      "Logistic Regression (178/2000): loss=0.622518681483201\n",
      "Logistic Regression (179/2000): loss=0.6224321408017175\n",
      "Logistic Regression (180/2000): loss=0.6223662225290825\n",
      "Logistic Regression (181/2000): loss=0.6223409185770605\n",
      "Logistic Regression (182/2000): loss=0.6222980339854315\n",
      "Logistic Regression (183/2000): loss=0.6222443748062152\n",
      "Logistic Regression (184/2000): loss=0.6221963560169523\n",
      "Logistic Regression (185/2000): loss=0.6221514026834112\n",
      "Logistic Regression (186/2000): loss=0.622097561233371\n",
      "Logistic Regression (187/2000): loss=0.6221206297734563\n",
      "Logistic Regression (188/2000): loss=0.6220475091386926\n",
      "Logistic Regression (189/2000): loss=0.6220295444789474\n",
      "Logistic Regression (190/2000): loss=0.6219672837509722\n",
      "Logistic Regression (191/2000): loss=0.6219503725516055\n",
      "Logistic Regression (192/2000): loss=0.6219061992598557\n",
      "Logistic Regression (193/2000): loss=0.6218817473397243\n",
      "Logistic Regression (194/2000): loss=0.6218458273068087\n",
      "Logistic Regression (195/2000): loss=0.6218647964840633\n",
      "Logistic Regression (196/2000): loss=0.6217945165676217\n",
      "Logistic Regression (197/2000): loss=0.6217586568055039\n",
      "Logistic Regression (198/2000): loss=0.6217247233324771\n",
      "Logistic Regression (199/2000): loss=0.6216787247523082\n",
      "Logistic Regression (200/2000): loss=0.6216685062596033\n",
      "Logistic Regression (201/2000): loss=0.6216074073785759\n",
      "Logistic Regression (202/2000): loss=0.6215789901073361\n",
      "Logistic Regression (203/2000): loss=0.6215725095093476\n",
      "Logistic Regression (204/2000): loss=0.6214743577258216\n",
      "Logistic Regression (205/2000): loss=0.6214590235639387\n",
      "Logistic Regression (206/2000): loss=0.6215485809024471\n",
      "Logistic Regression (207/2000): loss=0.621438005750096\n",
      "Logistic Regression (208/2000): loss=0.6214155294183453\n",
      "Logistic Regression (209/2000): loss=0.6213767190090076\n",
      "Logistic Regression (210/2000): loss=0.6212486292011037\n",
      "Logistic Regression (211/2000): loss=0.6211911749102692\n",
      "Logistic Regression (212/2000): loss=0.6211453896885439\n",
      "Logistic Regression (213/2000): loss=0.621109366780084\n",
      "Logistic Regression (214/2000): loss=0.6210861644714916\n",
      "Logistic Regression (215/2000): loss=0.6210733702809929\n",
      "Logistic Regression (216/2000): loss=0.6210745482347036\n",
      "Logistic Regression (217/2000): loss=0.6210238064286964\n",
      "Logistic Regression (218/2000): loss=0.6209514968489865\n",
      "Logistic Regression (219/2000): loss=0.6209088242734029\n",
      "Logistic Regression (220/2000): loss=0.6209086741524386\n",
      "Logistic Regression (221/2000): loss=0.620854099506319\n",
      "Logistic Regression (222/2000): loss=0.6208107060170246\n",
      "Logistic Regression (223/2000): loss=0.6208179597016786\n",
      "Logistic Regression (224/2000): loss=0.6207745025172922\n",
      "Logistic Regression (225/2000): loss=0.6206958993877145\n",
      "Logistic Regression (226/2000): loss=0.6206916737074847\n",
      "Logistic Regression (227/2000): loss=0.6206657808503164\n",
      "Logistic Regression (228/2000): loss=0.6206009748758715\n",
      "Logistic Regression (229/2000): loss=0.6205979539604624\n",
      "Logistic Regression (230/2000): loss=0.6205758286359171\n",
      "Logistic Regression (231/2000): loss=0.6205315370411678\n",
      "Logistic Regression (232/2000): loss=0.6205557221627123\n",
      "Logistic Regression (233/2000): loss=0.6205293643534464\n",
      "Logistic Regression (234/2000): loss=0.6205150026492556\n",
      "Logistic Regression (235/2000): loss=0.6204944830405718\n",
      "Logistic Regression (236/2000): loss=0.6204983571745238\n",
      "Logistic Regression (237/2000): loss=0.6204371887598715\n",
      "Logistic Regression (238/2000): loss=0.6204301500975483\n",
      "Logistic Regression (239/2000): loss=0.6203765707744089\n",
      "Logistic Regression (240/2000): loss=0.6203173939131968\n",
      "Logistic Regression (241/2000): loss=0.620292691622693\n",
      "Logistic Regression (242/2000): loss=0.620237812359171\n",
      "Logistic Regression (243/2000): loss=0.6201987452197952\n",
      "Logistic Regression (244/2000): loss=0.6201420172124725\n",
      "Logistic Regression (245/2000): loss=0.6201283090126303\n",
      "Logistic Regression (246/2000): loss=0.6201207151464603\n",
      "Logistic Regression (247/2000): loss=0.6200708216070331\n",
      "Logistic Regression (248/2000): loss=0.6200657577494875\n",
      "Logistic Regression (249/2000): loss=0.6199961139038705\n",
      "Logistic Regression (250/2000): loss=0.6199627558189009\n",
      "Logistic Regression (251/2000): loss=0.6199608014987874\n",
      "Logistic Regression (252/2000): loss=0.6198756747942371\n",
      "Logistic Regression (253/2000): loss=0.6198486580153713\n",
      "Logistic Regression (254/2000): loss=0.6198072659382834\n",
      "Logistic Regression (255/2000): loss=0.6197845338719546\n",
      "Logistic Regression (256/2000): loss=0.6197530024704724\n",
      "Logistic Regression (257/2000): loss=0.6197275152823499\n",
      "Logistic Regression (258/2000): loss=0.6196878179735226\n",
      "Logistic Regression (259/2000): loss=0.6196658648729688\n",
      "Logistic Regression (260/2000): loss=0.6196113810860308\n",
      "Logistic Regression (261/2000): loss=0.6196080368874395\n",
      "Logistic Regression (262/2000): loss=0.6195656215423101\n",
      "Logistic Regression (263/2000): loss=0.6195145360643196\n",
      "Logistic Regression (264/2000): loss=0.6194904056624406\n",
      "Logistic Regression (265/2000): loss=0.6194663152178349\n",
      "Logistic Regression (266/2000): loss=0.6194713794192876\n",
      "Logistic Regression (267/2000): loss=0.6194443127187006\n",
      "Logistic Regression (268/2000): loss=0.6194325894651602\n",
      "Logistic Regression (269/2000): loss=0.6193187515346714\n",
      "Logistic Regression (270/2000): loss=0.619309209381464\n",
      "Logistic Regression (271/2000): loss=0.6193273791851246\n",
      "Logistic Regression (272/2000): loss=0.619247158277232\n",
      "Logistic Regression (273/2000): loss=0.6192009406100774\n",
      "Logistic Regression (274/2000): loss=0.6191804496552393\n",
      "Logistic Regression (275/2000): loss=0.6191782794936189\n",
      "Logistic Regression (276/2000): loss=0.6191320196638798\n",
      "Logistic Regression (277/2000): loss=0.6191365163536031\n",
      "Logistic Regression (278/2000): loss=0.6190528119709154\n",
      "Logistic Regression (279/2000): loss=0.61905567014373\n",
      "Logistic Regression (280/2000): loss=0.6190249018984011\n",
      "Logistic Regression (281/2000): loss=0.6189881274036411\n",
      "Logistic Regression (282/2000): loss=0.6189655393405442\n",
      "Logistic Regression (283/2000): loss=0.6189259392286712\n",
      "Logistic Regression (284/2000): loss=0.6188749434924081\n",
      "Logistic Regression (285/2000): loss=0.6188666765863109\n",
      "Logistic Regression (286/2000): loss=0.6188861088972948\n",
      "Logistic Regression (287/2000): loss=0.6188289911533018\n",
      "Logistic Regression (288/2000): loss=0.6187328503718477\n",
      "Logistic Regression (289/2000): loss=0.6187111344472471\n",
      "Logistic Regression (290/2000): loss=0.6186653982317537\n",
      "Logistic Regression (291/2000): loss=0.6186995734225316\n",
      "Logistic Regression (292/2000): loss=0.6187054554709218\n",
      "Logistic Regression (293/2000): loss=0.6186437402622549\n",
      "Logistic Regression (294/2000): loss=0.6185829368396992\n",
      "Logistic Regression (295/2000): loss=0.6185763342805308\n",
      "Logistic Regression (296/2000): loss=0.6185508462911763\n",
      "Logistic Regression (297/2000): loss=0.6185345926992253\n",
      "Logistic Regression (298/2000): loss=0.618537230294864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (299/2000): loss=0.6184743669918089\n",
      "Logistic Regression (300/2000): loss=0.6184901810821215\n",
      "Logistic Regression (301/2000): loss=0.6185203495005206\n",
      "Logistic Regression (302/2000): loss=0.6184761372103409\n",
      "Logistic Regression (303/2000): loss=0.618403007651704\n",
      "Logistic Regression (304/2000): loss=0.6183137164493663\n",
      "Logistic Regression (305/2000): loss=0.6182721554699527\n",
      "Logistic Regression (306/2000): loss=0.6182367575913537\n",
      "Logistic Regression (307/2000): loss=0.618214557257355\n",
      "Logistic Regression (308/2000): loss=0.6181805237285155\n",
      "Logistic Regression (309/2000): loss=0.6181598482026964\n",
      "Logistic Regression (310/2000): loss=0.6181141532273603\n",
      "Logistic Regression (311/2000): loss=0.6180757394922274\n",
      "Logistic Regression (312/2000): loss=0.6180385397714204\n",
      "Logistic Regression (313/2000): loss=0.6180102545472889\n",
      "Logistic Regression (314/2000): loss=0.6179677226067923\n",
      "Logistic Regression (315/2000): loss=0.6179495121423415\n",
      "Logistic Regression (316/2000): loss=0.6179276392100264\n",
      "Logistic Regression (317/2000): loss=0.6178991215233497\n",
      "Logistic Regression (318/2000): loss=0.6178637938003092\n",
      "Logistic Regression (319/2000): loss=0.6178217978657063\n",
      "Logistic Regression (320/2000): loss=0.6178121625566416\n",
      "Logistic Regression (321/2000): loss=0.6177814154017557\n",
      "Logistic Regression (322/2000): loss=0.6177644422487264\n",
      "Logistic Regression (323/2000): loss=0.6177364316536091\n",
      "Logistic Regression (324/2000): loss=0.6177879046543494\n",
      "Logistic Regression (325/2000): loss=0.6177793485879365\n",
      "Logistic Regression (326/2000): loss=0.61776677046131\n",
      "Logistic Regression (327/2000): loss=0.6177668862684141\n",
      "Logistic Regression (328/2000): loss=0.617772255571177\n",
      "Logistic Regression (329/2000): loss=0.6178411060938013\n",
      "Logistic Regression (330/2000): loss=0.6177111246699613\n",
      "Logistic Regression (331/2000): loss=0.617720484681158\n",
      "Logistic Regression (332/2000): loss=0.6177787224940233\n",
      "Logistic Regression (333/2000): loss=0.6176857628428648\n",
      "Logistic Regression (334/2000): loss=0.6176690838106207\n",
      "Logistic Regression (335/2000): loss=0.6177091303319163\n",
      "Logistic Regression (336/2000): loss=0.6175788365422067\n",
      "Logistic Regression (337/2000): loss=0.617475712983194\n",
      "Logistic Regression (338/2000): loss=0.6174641324667244\n",
      "Logistic Regression (339/2000): loss=0.6174099517944999\n",
      "Logistic Regression (340/2000): loss=0.6174594429567464\n",
      "Logistic Regression (341/2000): loss=0.6174232761450679\n",
      "Logistic Regression (342/2000): loss=0.6173162044649247\n",
      "Logistic Regression (343/2000): loss=0.6173050129079889\n",
      "Logistic Regression (344/2000): loss=0.6172410540677189\n",
      "Logistic Regression (345/2000): loss=0.6172574522021586\n",
      "Logistic Regression (346/2000): loss=0.6171925831400447\n",
      "Logistic Regression (347/2000): loss=0.6171133696606694\n",
      "Logistic Regression (348/2000): loss=0.6171766202548872\n",
      "Logistic Regression (349/2000): loss=0.6172165065697337\n",
      "Logistic Regression (350/2000): loss=0.6173301403522127\n",
      "Logistic Regression (351/2000): loss=0.6173881774815021\n",
      "Logistic Regression (352/2000): loss=0.6173449594260666\n",
      "Logistic Regression (353/2000): loss=0.61720280313923\n",
      "Logistic Regression (354/2000): loss=0.6172329452718329\n",
      "Logistic Regression (355/2000): loss=0.6170952840406555\n",
      "Logistic Regression (356/2000): loss=0.6170144345247051\n",
      "Logistic Regression (357/2000): loss=0.6170713168942158\n",
      "Logistic Regression (358/2000): loss=0.6170206723772894\n",
      "Logistic Regression (359/2000): loss=0.6169195198915465\n",
      "Logistic Regression (360/2000): loss=0.6168114579690692\n",
      "Logistic Regression (361/2000): loss=0.6168489748085481\n",
      "Logistic Regression (362/2000): loss=0.6167834270472016\n",
      "Logistic Regression (363/2000): loss=0.6167465562975601\n",
      "Logistic Regression (364/2000): loss=0.6168182902602399\n",
      "Logistic Regression (365/2000): loss=0.6167505310012288\n",
      "Logistic Regression (366/2000): loss=0.6167802061938849\n",
      "Logistic Regression (367/2000): loss=0.6166967476898464\n",
      "Logistic Regression (368/2000): loss=0.6166315469259767\n",
      "Logistic Regression (369/2000): loss=0.6166601307380766\n",
      "Logistic Regression (370/2000): loss=0.6166130526239981\n",
      "Logistic Regression (371/2000): loss=0.6164932589472834\n",
      "Logistic Regression (372/2000): loss=0.6164773397580992\n",
      "Logistic Regression (373/2000): loss=0.6164969976129816\n",
      "Logistic Regression (374/2000): loss=0.6165664873756268\n",
      "Logistic Regression (375/2000): loss=0.6166430917742839\n",
      "Logistic Regression (376/2000): loss=0.6167097442912479\n",
      "Logistic Regression (377/2000): loss=0.6167096286549248\n",
      "Logistic Regression (378/2000): loss=0.6166703116534525\n",
      "Logistic Regression (379/2000): loss=0.6166220300476143\n",
      "Logistic Regression (380/2000): loss=0.6166313329873976\n",
      "Logistic Regression (381/2000): loss=0.6167382214565222\n",
      "Logistic Regression (382/2000): loss=0.6168184319985085\n",
      "Logistic Regression (383/2000): loss=0.6167594712338125\n",
      "Logistic Regression (384/2000): loss=0.6165604197762192\n",
      "Logistic Regression (385/2000): loss=0.6163881911981756\n",
      "Logistic Regression (386/2000): loss=0.6162783592834619\n",
      "Logistic Regression (387/2000): loss=0.6162679072736881\n",
      "Logistic Regression (388/2000): loss=0.6162799401610578\n",
      "Logistic Regression (389/2000): loss=0.6162415519233395\n",
      "Logistic Regression (390/2000): loss=0.6163651949551561\n",
      "Logistic Regression (391/2000): loss=0.6164244142966221\n",
      "Logistic Regression (392/2000): loss=0.6163830828877429\n",
      "Logistic Regression (393/2000): loss=0.6163285136688492\n",
      "Logistic Regression (394/2000): loss=0.6162039276643473\n",
      "Logistic Regression (395/2000): loss=0.6160234333501603\n",
      "Logistic Regression (396/2000): loss=0.6160020421440286\n",
      "Logistic Regression (397/2000): loss=0.6158519217486508\n",
      "Logistic Regression (398/2000): loss=0.6158005929785668\n",
      "Logistic Regression (399/2000): loss=0.6157357957347186\n",
      "Logistic Regression (400/2000): loss=0.6156855429736584\n",
      "Logistic Regression (401/2000): loss=0.6156256908314532\n",
      "Logistic Regression (402/2000): loss=0.6155595850642263\n",
      "Logistic Regression (403/2000): loss=0.6155146317520993\n",
      "Logistic Regression (404/2000): loss=0.615506696069563\n",
      "Logistic Regression (405/2000): loss=0.6154917809451051\n",
      "Logistic Regression (406/2000): loss=0.6154752141891244\n",
      "Logistic Regression (407/2000): loss=0.6154527165561351\n",
      "Logistic Regression (408/2000): loss=0.6154267177042546\n",
      "Logistic Regression (409/2000): loss=0.6154126669105728\n",
      "Logistic Regression (410/2000): loss=0.6153847869994731\n",
      "Logistic Regression (411/2000): loss=0.6153705042732317\n",
      "Logistic Regression (412/2000): loss=0.6153586381285955\n",
      "Logistic Regression (413/2000): loss=0.6153760497860087\n",
      "Logistic Regression (414/2000): loss=0.6153838137367771\n",
      "Logistic Regression (415/2000): loss=0.6153550576872414\n",
      "Logistic Regression (416/2000): loss=0.615371619475851\n",
      "Logistic Regression (417/2000): loss=0.6152670600152579\n",
      "Logistic Regression (418/2000): loss=0.6152424472209113\n",
      "Logistic Regression (419/2000): loss=0.6152289885489372\n",
      "Logistic Regression (420/2000): loss=0.6152115831604449\n",
      "Logistic Regression (421/2000): loss=0.6151943187692412\n",
      "Logistic Regression (422/2000): loss=0.6152249572834364\n",
      "Logistic Regression (423/2000): loss=0.6152303826612646\n",
      "Logistic Regression (424/2000): loss=0.615165128097592\n",
      "Logistic Regression (425/2000): loss=0.6151948470585803\n",
      "Logistic Regression (426/2000): loss=0.6151460837064106\n",
      "Logistic Regression (427/2000): loss=0.615174088304402\n",
      "Logistic Regression (428/2000): loss=0.6151054654662892\n",
      "Logistic Regression (429/2000): loss=0.6150742622763864\n",
      "Logistic Regression (430/2000): loss=0.6150147648256911\n",
      "Logistic Regression (431/2000): loss=0.6149634153393962\n",
      "Logistic Regression (432/2000): loss=0.6149466596080472\n",
      "Logistic Regression (433/2000): loss=0.6151758410582452\n",
      "Logistic Regression (434/2000): loss=0.6150472275126433\n",
      "Logistic Regression (435/2000): loss=0.6150265146783966\n",
      "Logistic Regression (436/2000): loss=0.6150072835832603\n",
      "Logistic Regression (437/2000): loss=0.6151748709704757\n",
      "Logistic Regression (438/2000): loss=0.6149498630482918\n",
      "Logistic Regression (439/2000): loss=0.614884979341494\n",
      "Logistic Regression (440/2000): loss=0.6148927578492508\n",
      "Logistic Regression (441/2000): loss=0.6147251814444531\n",
      "Logistic Regression (442/2000): loss=0.6147199523006543\n",
      "Logistic Regression (443/2000): loss=0.6147039013597815\n",
      "Logistic Regression (444/2000): loss=0.6147273642143608\n",
      "Logistic Regression (445/2000): loss=0.6147145994230645\n",
      "Logistic Regression (446/2000): loss=0.6147633541624405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (447/2000): loss=0.61475210176265\n",
      "Logistic Regression (448/2000): loss=0.6147380915639816\n",
      "Logistic Regression (449/2000): loss=0.6147093790801206\n",
      "Logistic Regression (450/2000): loss=0.6146912037949773\n",
      "Logistic Regression (451/2000): loss=0.6147180240591001\n",
      "Logistic Regression (452/2000): loss=0.6146801498399005\n",
      "Logistic Regression (453/2000): loss=0.6147163319370132\n",
      "Logistic Regression (454/2000): loss=0.6146213771759332\n",
      "Logistic Regression (455/2000): loss=0.6145099520343331\n",
      "Logistic Regression (456/2000): loss=0.6145613152495836\n",
      "Logistic Regression (457/2000): loss=0.6144124221311972\n",
      "Logistic Regression (458/2000): loss=0.6144830649330116\n",
      "Logistic Regression (459/2000): loss=0.6144745261795181\n",
      "Logistic Regression (460/2000): loss=0.6143664673776664\n",
      "Logistic Regression (461/2000): loss=0.6143406022184564\n",
      "Logistic Regression (462/2000): loss=0.6143358992411956\n",
      "Logistic Regression (463/2000): loss=0.6142967892618163\n",
      "Logistic Regression (464/2000): loss=0.6142910418102565\n",
      "Logistic Regression (465/2000): loss=0.6142686556237266\n",
      "Logistic Regression (466/2000): loss=0.6142698908538171\n",
      "Logistic Regression (467/2000): loss=0.6142911810540744\n",
      "Logistic Regression (468/2000): loss=0.614292229746983\n",
      "Logistic Regression (469/2000): loss=0.6144102244550933\n",
      "Logistic Regression (470/2000): loss=0.6144280414084865\n",
      "Logistic Regression (471/2000): loss=0.6142954696380994\n",
      "Logistic Regression (472/2000): loss=0.6141655173572272\n",
      "Logistic Regression (473/2000): loss=0.6140919234077118\n",
      "Logistic Regression (474/2000): loss=0.6141839124709674\n",
      "Logistic Regression (475/2000): loss=0.6143550209976849\n",
      "Logistic Regression (476/2000): loss=0.6141910575810894\n",
      "Logistic Regression (477/2000): loss=0.6141093010662907\n",
      "Logistic Regression (478/2000): loss=0.6141924573740095\n",
      "Logistic Regression (479/2000): loss=0.6142514374366755\n",
      "Logistic Regression (480/2000): loss=0.6142453278690593\n",
      "Logistic Regression (481/2000): loss=0.6143468510839335\n",
      "Logistic Regression (482/2000): loss=0.6144937191843245\n",
      "Logistic Regression (483/2000): loss=0.6146464214461366\n",
      "Logistic Regression (484/2000): loss=0.6143934516401386\n",
      "Logistic Regression (485/2000): loss=0.6143288454436996\n",
      "Logistic Regression (486/2000): loss=0.6142681692330951\n",
      "Logistic Regression (487/2000): loss=0.6144101528039818\n",
      "Logistic Regression (488/2000): loss=0.6145909513257573\n",
      "Logistic Regression (489/2000): loss=0.6146145262817352\n",
      "Logistic Regression (490/2000): loss=0.6145136918679659\n",
      "Logistic Regression (491/2000): loss=0.6141827795685445\n",
      "Logistic Regression (492/2000): loss=0.613891354355956\n",
      "Logistic Regression (493/2000): loss=0.6137204133801477\n",
      "Logistic Regression (494/2000): loss=0.6137876503945886\n",
      "Logistic Regression (495/2000): loss=0.6137292466181966\n",
      "Logistic Regression (496/2000): loss=0.6136793328465922\n",
      "Logistic Regression (497/2000): loss=0.6136536811183887\n",
      "Logistic Regression (498/2000): loss=0.6137221253856352\n",
      "Logistic Regression (499/2000): loss=0.6137174227700551\n",
      "Logistic Regression (500/2000): loss=0.613834073320566\n",
      "Logistic Regression (501/2000): loss=0.613865328613975\n",
      "Logistic Regression (502/2000): loss=0.6138154594932222\n",
      "Logistic Regression (503/2000): loss=0.6136554617642267\n",
      "Logistic Regression (504/2000): loss=0.6138138920637198\n",
      "Logistic Regression (505/2000): loss=0.6136720290495407\n",
      "Logistic Regression (506/2000): loss=0.6136538239876641\n",
      "Logistic Regression (507/2000): loss=0.6135701963900234\n",
      "Logistic Regression (508/2000): loss=0.6134720486305393\n",
      "Logistic Regression (509/2000): loss=0.6133440096030965\n",
      "Logistic Regression (510/2000): loss=0.6133424735241734\n",
      "Logistic Regression (511/2000): loss=0.6132962029497411\n",
      "Logistic Regression (512/2000): loss=0.6132346301147514\n",
      "Logistic Regression (513/2000): loss=0.6132343933099077\n",
      "Logistic Regression (514/2000): loss=0.6131951640213219\n",
      "Logistic Regression (515/2000): loss=0.6131595352241579\n",
      "Logistic Regression (516/2000): loss=0.6132105554330405\n",
      "Logistic Regression (517/2000): loss=0.6131428621005052\n",
      "Logistic Regression (518/2000): loss=0.61312102587252\n",
      "Logistic Regression (519/2000): loss=0.6130856509635185\n",
      "Logistic Regression (520/2000): loss=0.6130738083622869\n",
      "Logistic Regression (521/2000): loss=0.6130422394973296\n",
      "Logistic Regression (522/2000): loss=0.6130158747611107\n",
      "Logistic Regression (523/2000): loss=0.6130051275923122\n",
      "Logistic Regression (524/2000): loss=0.6130070765171474\n",
      "Logistic Regression (525/2000): loss=0.6129690314778725\n",
      "Logistic Regression (526/2000): loss=0.6129476687529596\n",
      "Logistic Regression (527/2000): loss=0.6129031471233954\n",
      "Logistic Regression (528/2000): loss=0.6129607921498424\n",
      "Logistic Regression (529/2000): loss=0.6128819690908098\n",
      "Logistic Regression (530/2000): loss=0.6128604303505997\n",
      "Logistic Regression (531/2000): loss=0.6128437025326322\n",
      "Logistic Regression (532/2000): loss=0.6128131964974618\n",
      "Logistic Regression (533/2000): loss=0.6127976299005867\n",
      "Logistic Regression (534/2000): loss=0.6128033812576588\n",
      "Logistic Regression (535/2000): loss=0.612764101276503\n",
      "Logistic Regression (536/2000): loss=0.6127628199472384\n",
      "Logistic Regression (537/2000): loss=0.6127517248118048\n",
      "Logistic Regression (538/2000): loss=0.6127819181099955\n",
      "Logistic Regression (539/2000): loss=0.6127569175126594\n",
      "Logistic Regression (540/2000): loss=0.6127426936042203\n",
      "Logistic Regression (541/2000): loss=0.6128200617586999\n",
      "Logistic Regression (542/2000): loss=0.6128942727706518\n",
      "Logistic Regression (543/2000): loss=0.6129544876719281\n",
      "Logistic Regression (544/2000): loss=0.6128043896217519\n",
      "Logistic Regression (545/2000): loss=0.6127516321454475\n",
      "Logistic Regression (546/2000): loss=0.6127239989583254\n",
      "Logistic Regression (547/2000): loss=0.6128337749426274\n",
      "Logistic Regression (548/2000): loss=0.612753439237388\n",
      "Logistic Regression (549/2000): loss=0.6128882599433176\n",
      "Logistic Regression (550/2000): loss=0.6128554471087517\n",
      "Logistic Regression (551/2000): loss=0.6128086193479878\n",
      "Logistic Regression (552/2000): loss=0.6129022991597372\n",
      "Logistic Regression (553/2000): loss=0.6126774757432782\n",
      "Logistic Regression (554/2000): loss=0.6126943776802258\n",
      "Logistic Regression (555/2000): loss=0.612589429595289\n",
      "Logistic Regression (556/2000): loss=0.6124501148451041\n",
      "Logistic Regression (557/2000): loss=0.612396849731957\n",
      "Logistic Regression (558/2000): loss=0.612477005959958\n",
      "Logistic Regression (559/2000): loss=0.6124204152335809\n",
      "Logistic Regression (560/2000): loss=0.6123325003928359\n",
      "Logistic Regression (561/2000): loss=0.6123164535704927\n",
      "Logistic Regression (562/2000): loss=0.6122865745751515\n",
      "Logistic Regression (563/2000): loss=0.6123171143121239\n",
      "Logistic Regression (564/2000): loss=0.6123067869579009\n",
      "Logistic Regression (565/2000): loss=0.6122743597384582\n",
      "Logistic Regression (566/2000): loss=0.6123243275190967\n",
      "Logistic Regression (567/2000): loss=0.6122560557764362\n",
      "Logistic Regression (568/2000): loss=0.6121645240115714\n",
      "Logistic Regression (569/2000): loss=0.6122263777348907\n",
      "Logistic Regression (570/2000): loss=0.6121763525240478\n",
      "Logistic Regression (571/2000): loss=0.6122556039754445\n",
      "Logistic Regression (572/2000): loss=0.612184101782909\n",
      "Logistic Regression (573/2000): loss=0.6121230191695065\n",
      "Logistic Regression (574/2000): loss=0.6121230402414196\n",
      "Logistic Regression (575/2000): loss=0.6120780833807543\n",
      "Logistic Regression (576/2000): loss=0.612041552038036\n",
      "Logistic Regression (577/2000): loss=0.6120149109155789\n",
      "Logistic Regression (578/2000): loss=0.6119688903415688\n",
      "Logistic Regression (579/2000): loss=0.6119850444415419\n",
      "Logistic Regression (580/2000): loss=0.6119922712710579\n",
      "Logistic Regression (581/2000): loss=0.611881237848924\n",
      "Logistic Regression (582/2000): loss=0.611829804569519\n",
      "Logistic Regression (583/2000): loss=0.6118258517651405\n",
      "Logistic Regression (584/2000): loss=0.6118215710411058\n",
      "Logistic Regression (585/2000): loss=0.6117890226403498\n",
      "Logistic Regression (586/2000): loss=0.6117946328964131\n",
      "Logistic Regression (587/2000): loss=0.6118604516860665\n",
      "Logistic Regression (588/2000): loss=0.6119347023541095\n",
      "Logistic Regression (589/2000): loss=0.6118529096868132\n",
      "Logistic Regression (590/2000): loss=0.6117858194450604\n",
      "Logistic Regression (591/2000): loss=0.6117467275148732\n",
      "Logistic Regression (592/2000): loss=0.6117257754521157\n",
      "Logistic Regression (593/2000): loss=0.611625615565461\n",
      "Logistic Regression (594/2000): loss=0.611639769729353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (595/2000): loss=0.6116619163937091\n",
      "Logistic Regression (596/2000): loss=0.611669748245937\n",
      "Logistic Regression (597/2000): loss=0.6115550308110358\n",
      "Logistic Regression (598/2000): loss=0.6115049631898876\n",
      "Logistic Regression (599/2000): loss=0.611487979904978\n",
      "Logistic Regression (600/2000): loss=0.611555318031261\n",
      "Logistic Regression (601/2000): loss=0.6115295406253662\n",
      "Logistic Regression (602/2000): loss=0.6115400126303335\n",
      "Logistic Regression (603/2000): loss=0.6115495423836459\n",
      "Logistic Regression (604/2000): loss=0.6115404714142227\n",
      "Logistic Regression (605/2000): loss=0.6115064029012397\n",
      "Logistic Regression (606/2000): loss=0.6114375065965723\n",
      "Logistic Regression (607/2000): loss=0.6114772946728553\n",
      "Logistic Regression (608/2000): loss=0.6114225570023026\n",
      "Logistic Regression (609/2000): loss=0.6114951957121577\n",
      "Logistic Regression (610/2000): loss=0.6113643970013247\n",
      "Logistic Regression (611/2000): loss=0.6113010834825954\n",
      "Logistic Regression (612/2000): loss=0.6112879595341477\n",
      "Logistic Regression (613/2000): loss=0.6112737358524535\n",
      "Logistic Regression (614/2000): loss=0.6112543266936421\n",
      "Logistic Regression (615/2000): loss=0.6112400452616389\n",
      "Logistic Regression (616/2000): loss=0.6112322260544851\n",
      "Logistic Regression (617/2000): loss=0.6112219672105068\n",
      "Logistic Regression (618/2000): loss=0.6113072413108267\n",
      "Logistic Regression (619/2000): loss=0.6112820967762651\n",
      "Logistic Regression (620/2000): loss=0.6113048506233627\n",
      "Logistic Regression (621/2000): loss=0.6113695053927547\n",
      "Logistic Regression (622/2000): loss=0.6114749810744413\n",
      "Logistic Regression (623/2000): loss=0.6114325766562739\n",
      "Logistic Regression (624/2000): loss=0.6113637988035923\n",
      "Logistic Regression (625/2000): loss=0.6113745933565059\n",
      "Logistic Regression (626/2000): loss=0.6112458410384554\n",
      "Logistic Regression (627/2000): loss=0.611177788671247\n",
      "Logistic Regression (628/2000): loss=0.6111368398274175\n",
      "Logistic Regression (629/2000): loss=0.6112322510256992\n",
      "Logistic Regression (630/2000): loss=0.6111832451438423\n",
      "Logistic Regression (631/2000): loss=0.6111921687842142\n",
      "Logistic Regression (632/2000): loss=0.6112310486819031\n",
      "Logistic Regression (633/2000): loss=0.6111513412414692\n",
      "Logistic Regression (634/2000): loss=0.6111336821430476\n",
      "Logistic Regression (635/2000): loss=0.6111511184344919\n",
      "Logistic Regression (636/2000): loss=0.6111221135311471\n",
      "Logistic Regression (637/2000): loss=0.6110673375653386\n",
      "Logistic Regression (638/2000): loss=0.6110663081569827\n",
      "Logistic Regression (639/2000): loss=0.611017106713977\n",
      "Logistic Regression (640/2000): loss=0.6109716835961917\n",
      "Logistic Regression (641/2000): loss=0.6109670507886947\n",
      "Logistic Regression (642/2000): loss=0.6109818601598305\n",
      "Logistic Regression (643/2000): loss=0.6109717785921597\n",
      "Logistic Regression (644/2000): loss=0.6109412230056295\n",
      "Logistic Regression (645/2000): loss=0.6109400891783601\n",
      "Logistic Regression (646/2000): loss=0.6110280373353381\n",
      "Logistic Regression (647/2000): loss=0.6111619280104116\n",
      "Logistic Regression (648/2000): loss=0.6109879392043307\n",
      "Logistic Regression (649/2000): loss=0.6109139148846288\n",
      "Logistic Regression (650/2000): loss=0.6109326458334493\n",
      "Logistic Regression (651/2000): loss=0.610854558783173\n",
      "Logistic Regression (652/2000): loss=0.6109141867299244\n",
      "Logistic Regression (653/2000): loss=0.6109049312259595\n",
      "Logistic Regression (654/2000): loss=0.6108856589538374\n",
      "Logistic Regression (655/2000): loss=0.6108332943365148\n",
      "Logistic Regression (656/2000): loss=0.6108704779673851\n",
      "Logistic Regression (657/2000): loss=0.6108764760451271\n",
      "Logistic Regression (658/2000): loss=0.6108908353666518\n",
      "Logistic Regression (659/2000): loss=0.6108143819053709\n",
      "Logistic Regression (660/2000): loss=0.6107729496669642\n",
      "Logistic Regression (661/2000): loss=0.6107909682779821\n",
      "Logistic Regression (662/2000): loss=0.6108247527388692\n",
      "Logistic Regression (663/2000): loss=0.6106645346747406\n",
      "Logistic Regression (664/2000): loss=0.6106096867867397\n",
      "Logistic Regression (665/2000): loss=0.6105775078421979\n",
      "Logistic Regression (666/2000): loss=0.6105566919854162\n",
      "Logistic Regression (667/2000): loss=0.610545469523097\n",
      "Logistic Regression (668/2000): loss=0.6105013089971142\n",
      "Logistic Regression (669/2000): loss=0.6104756287424072\n",
      "Logistic Regression (670/2000): loss=0.610496807102248\n",
      "Logistic Regression (671/2000): loss=0.6104776015666992\n",
      "Logistic Regression (672/2000): loss=0.6104380468778932\n",
      "Logistic Regression (673/2000): loss=0.6104523085580529\n",
      "Logistic Regression (674/2000): loss=0.6104328023467549\n",
      "Logistic Regression (675/2000): loss=0.6104179264343332\n",
      "Logistic Regression (676/2000): loss=0.6104685317746238\n",
      "Logistic Regression (677/2000): loss=0.610408978990504\n",
      "Logistic Regression (678/2000): loss=0.61037672900114\n",
      "Logistic Regression (679/2000): loss=0.6103354747061458\n",
      "Logistic Regression (680/2000): loss=0.610320660588403\n",
      "Logistic Regression (681/2000): loss=0.6102918055861462\n",
      "Logistic Regression (682/2000): loss=0.6102927743341012\n",
      "Logistic Regression (683/2000): loss=0.6103366347852877\n",
      "Logistic Regression (684/2000): loss=0.6103077314020113\n",
      "Logistic Regression (685/2000): loss=0.6102682982058335\n",
      "Logistic Regression (686/2000): loss=0.6102902036964463\n",
      "Logistic Regression (687/2000): loss=0.6103225458081735\n",
      "Logistic Regression (688/2000): loss=0.6103876663218795\n",
      "Logistic Regression (689/2000): loss=0.6103154247860942\n",
      "Logistic Regression (690/2000): loss=0.6102041917014879\n",
      "Logistic Regression (691/2000): loss=0.6101861917510901\n",
      "Logistic Regression (692/2000): loss=0.6101703867183819\n",
      "Logistic Regression (693/2000): loss=0.610147822687248\n",
      "Logistic Regression (694/2000): loss=0.6101587800637314\n",
      "Logistic Regression (695/2000): loss=0.6100814169022674\n",
      "Logistic Regression (696/2000): loss=0.6100813315787398\n",
      "Logistic Regression (697/2000): loss=0.61010394107261\n",
      "Logistic Regression (698/2000): loss=0.6101052624972199\n",
      "Logistic Regression (699/2000): loss=0.6100752533280013\n",
      "Logistic Regression (700/2000): loss=0.6100272242995757\n",
      "Logistic Regression (701/2000): loss=0.6099621197044656\n",
      "Logistic Regression (702/2000): loss=0.6099363168364607\n",
      "Logistic Regression (703/2000): loss=0.6099058084338743\n",
      "Logistic Regression (704/2000): loss=0.6099189379589078\n",
      "Logistic Regression (705/2000): loss=0.609892561825398\n",
      "Logistic Regression (706/2000): loss=0.609898258265255\n",
      "Logistic Regression (707/2000): loss=0.6098564209312727\n",
      "Logistic Regression (708/2000): loss=0.6098262529092643\n",
      "Logistic Regression (709/2000): loss=0.6097964664322313\n",
      "Logistic Regression (710/2000): loss=0.6097672469389235\n",
      "Logistic Regression (711/2000): loss=0.6097633806816187\n",
      "Logistic Regression (712/2000): loss=0.6097450942369562\n",
      "Logistic Regression (713/2000): loss=0.6097376712575239\n",
      "Logistic Regression (714/2000): loss=0.6097297309479789\n",
      "Logistic Regression (715/2000): loss=0.6097168952730776\n",
      "Logistic Regression (716/2000): loss=0.6096948523454696\n",
      "Logistic Regression (717/2000): loss=0.6096701306024033\n",
      "Logistic Regression (718/2000): loss=0.6096609864488399\n",
      "Logistic Regression (719/2000): loss=0.6096380719907877\n",
      "Logistic Regression (720/2000): loss=0.6096024445855024\n",
      "Logistic Regression (721/2000): loss=0.6095926181404743\n",
      "Logistic Regression (722/2000): loss=0.6095715057370923\n",
      "Logistic Regression (723/2000): loss=0.609559943704471\n",
      "Logistic Regression (724/2000): loss=0.6095527065392555\n",
      "Logistic Regression (725/2000): loss=0.6095421134490808\n",
      "Logistic Regression (726/2000): loss=0.6095253844075282\n",
      "Logistic Regression (727/2000): loss=0.6095033513347106\n",
      "Logistic Regression (728/2000): loss=0.6094823973001826\n",
      "Logistic Regression (729/2000): loss=0.6094853002555648\n",
      "Logistic Regression (730/2000): loss=0.6094515921530097\n",
      "Logistic Regression (731/2000): loss=0.6094357025145117\n",
      "Logistic Regression (732/2000): loss=0.6094331785667604\n",
      "Logistic Regression (733/2000): loss=0.6094170840770826\n",
      "Logistic Regression (734/2000): loss=0.6094043965393161\n",
      "Logistic Regression (735/2000): loss=0.6093899473474929\n",
      "Logistic Regression (736/2000): loss=0.6093924808613691\n",
      "Logistic Regression (737/2000): loss=0.6094249964166898\n",
      "Logistic Regression (738/2000): loss=0.6094939732436778\n",
      "Logistic Regression (739/2000): loss=0.6095077069806141\n",
      "Logistic Regression (740/2000): loss=0.6095239769483543\n",
      "Logistic Regression (741/2000): loss=0.6095408755298547\n",
      "Logistic Regression (742/2000): loss=0.6094145772480329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (743/2000): loss=0.6092976845329594\n",
      "Logistic Regression (744/2000): loss=0.6092584727892306\n",
      "Logistic Regression (745/2000): loss=0.6092864946860548\n",
      "Logistic Regression (746/2000): loss=0.6092286980864919\n",
      "Logistic Regression (747/2000): loss=0.609231335908273\n",
      "Logistic Regression (748/2000): loss=0.6091987598000712\n",
      "Logistic Regression (749/2000): loss=0.6091629402094592\n",
      "Logistic Regression (750/2000): loss=0.6091723481822391\n",
      "Logistic Regression (751/2000): loss=0.6091338823370349\n",
      "Logistic Regression (752/2000): loss=0.6091507448001169\n",
      "Logistic Regression (753/2000): loss=0.6090722676896759\n",
      "Logistic Regression (754/2000): loss=0.6090486585391299\n",
      "Logistic Regression (755/2000): loss=0.6091158430182694\n",
      "Logistic Regression (756/2000): loss=0.6091473355293966\n",
      "Logistic Regression (757/2000): loss=0.6091339146202964\n",
      "Logistic Regression (758/2000): loss=0.6090651473032392\n",
      "Logistic Regression (759/2000): loss=0.609056723530417\n",
      "Logistic Regression (760/2000): loss=0.6089964760602438\n",
      "Logistic Regression (761/2000): loss=0.6089960635701547\n",
      "Logistic Regression (762/2000): loss=0.6089611722865844\n",
      "Logistic Regression (763/2000): loss=0.6089920905533465\n",
      "Logistic Regression (764/2000): loss=0.6089562695291413\n",
      "Logistic Regression (765/2000): loss=0.6089879299938272\n",
      "Logistic Regression (766/2000): loss=0.6089192984472918\n",
      "Logistic Regression (767/2000): loss=0.6088789113755846\n",
      "Logistic Regression (768/2000): loss=0.6088577733025058\n",
      "Logistic Regression (769/2000): loss=0.6088562429517921\n",
      "Logistic Regression (770/2000): loss=0.6088316460992258\n",
      "Logistic Regression (771/2000): loss=0.6088231193711428\n",
      "Logistic Regression (772/2000): loss=0.6087788359036777\n",
      "Logistic Regression (773/2000): loss=0.6087852978789055\n",
      "Logistic Regression (774/2000): loss=0.6087549709230662\n",
      "Logistic Regression (775/2000): loss=0.6087387534878284\n",
      "Logistic Regression (776/2000): loss=0.6087337308579724\n",
      "Logistic Regression (777/2000): loss=0.6087182021900583\n",
      "Logistic Regression (778/2000): loss=0.6087332105685219\n",
      "Logistic Regression (779/2000): loss=0.6087537477333411\n",
      "Logistic Regression (780/2000): loss=0.6087748535189635\n",
      "Logistic Regression (781/2000): loss=0.6088479190359262\n",
      "Logistic Regression (782/2000): loss=0.6089065261886436\n",
      "Logistic Regression (783/2000): loss=0.6088743459549761\n",
      "Logistic Regression (784/2000): loss=0.6087386580416766\n",
      "Logistic Regression (785/2000): loss=0.608742655957856\n",
      "Logistic Regression (786/2000): loss=0.608748980934248\n",
      "Logistic Regression (787/2000): loss=0.608682634318023\n",
      "Logistic Regression (788/2000): loss=0.6086843483715818\n",
      "Logistic Regression (789/2000): loss=0.6088210881750088\n",
      "Logistic Regression (790/2000): loss=0.608820828768452\n",
      "Logistic Regression (791/2000): loss=0.608778982979232\n",
      "Logistic Regression (792/2000): loss=0.6087577793211735\n",
      "Logistic Regression (793/2000): loss=0.6086780461167357\n",
      "Logistic Regression (794/2000): loss=0.6086781030929125\n",
      "Logistic Regression (795/2000): loss=0.6086448250112334\n",
      "Logistic Regression (796/2000): loss=0.6085682688706148\n",
      "Logistic Regression (797/2000): loss=0.6085970079272225\n",
      "Logistic Regression (798/2000): loss=0.6086533675470673\n",
      "Logistic Regression (799/2000): loss=0.6086265696425336\n",
      "Logistic Regression (800/2000): loss=0.6085558148043562\n",
      "Logistic Regression (801/2000): loss=0.6086412830617137\n",
      "Logistic Regression (802/2000): loss=0.6087601227311259\n",
      "Logistic Regression (803/2000): loss=0.6087404164507448\n",
      "Logistic Regression (804/2000): loss=0.6087313792239886\n",
      "Logistic Regression (805/2000): loss=0.6087169251626244\n",
      "Logistic Regression (806/2000): loss=0.6089013420115017\n",
      "Logistic Regression (807/2000): loss=0.6088036969016487\n",
      "Logistic Regression (808/2000): loss=0.6088058709119383\n",
      "Logistic Regression (809/2000): loss=0.6087546423735183\n",
      "Logistic Regression (810/2000): loss=0.608579594355166\n",
      "Logistic Regression (811/2000): loss=0.6086490438891942\n",
      "Logistic Regression (812/2000): loss=0.6087198283623525\n",
      "Logistic Regression (813/2000): loss=0.6086247234166443\n",
      "Logistic Regression (814/2000): loss=0.6086976080296185\n",
      "Logistic Regression (815/2000): loss=0.6084663683862929\n",
      "Logistic Regression (816/2000): loss=0.6084528893782628\n",
      "Logistic Regression (817/2000): loss=0.6085339285064095\n",
      "Logistic Regression (818/2000): loss=0.6084003576642799\n",
      "Logistic Regression (819/2000): loss=0.6084024100278955\n",
      "Logistic Regression (820/2000): loss=0.6083657701609688\n",
      "Logistic Regression (821/2000): loss=0.6081715227271303\n",
      "Logistic Regression (822/2000): loss=0.6081880899106284\n",
      "Logistic Regression (823/2000): loss=0.6082382317987427\n",
      "Logistic Regression (824/2000): loss=0.6080985233790999\n",
      "Logistic Regression (825/2000): loss=0.6080596614445561\n",
      "Logistic Regression (826/2000): loss=0.6080919126051647\n",
      "Logistic Regression (827/2000): loss=0.6080159410114117\n",
      "Logistic Regression (828/2000): loss=0.608004033503021\n",
      "Logistic Regression (829/2000): loss=0.6079856254299804\n",
      "Logistic Regression (830/2000): loss=0.6079644185525132\n",
      "Logistic Regression (831/2000): loss=0.6079376300521918\n",
      "Logistic Regression (832/2000): loss=0.6079189631925322\n",
      "Logistic Regression (833/2000): loss=0.6079019651034193\n",
      "Logistic Regression (834/2000): loss=0.607873723351293\n",
      "Logistic Regression (835/2000): loss=0.6078643708956669\n",
      "Logistic Regression (836/2000): loss=0.6079000094041661\n",
      "Logistic Regression (837/2000): loss=0.6079162134969067\n",
      "Logistic Regression (838/2000): loss=0.6079278423122486\n",
      "Logistic Regression (839/2000): loss=0.6079346921622394\n",
      "Logistic Regression (840/2000): loss=0.6079707078077111\n",
      "Logistic Regression (841/2000): loss=0.6080993008080784\n",
      "Logistic Regression (842/2000): loss=0.6080399134303691\n",
      "Logistic Regression (843/2000): loss=0.6078577768132811\n",
      "Logistic Regression (844/2000): loss=0.6078191584045393\n",
      "Logistic Regression (845/2000): loss=0.6078693151050547\n",
      "Logistic Regression (846/2000): loss=0.6078410984089514\n",
      "Logistic Regression (847/2000): loss=0.6077632738389972\n",
      "Logistic Regression (848/2000): loss=0.607763751928444\n",
      "Logistic Regression (849/2000): loss=0.6077608046280049\n",
      "Logistic Regression (850/2000): loss=0.607688506477388\n",
      "Logistic Regression (851/2000): loss=0.6076641600227064\n",
      "Logistic Regression (852/2000): loss=0.6076349181844185\n",
      "Logistic Regression (853/2000): loss=0.6076236201871519\n",
      "Logistic Regression (854/2000): loss=0.6076170201445663\n",
      "Logistic Regression (855/2000): loss=0.6076101457667796\n",
      "Logistic Regression (856/2000): loss=0.6075907344676329\n",
      "Logistic Regression (857/2000): loss=0.6075692561362362\n",
      "Logistic Regression (858/2000): loss=0.6075537634991748\n",
      "Logistic Regression (859/2000): loss=0.6075598096353129\n",
      "Logistic Regression (860/2000): loss=0.6075438769523486\n",
      "Logistic Regression (861/2000): loss=0.6074987363862097\n",
      "Logistic Regression (862/2000): loss=0.6074767217710175\n",
      "Logistic Regression (863/2000): loss=0.6074611204372686\n",
      "Logistic Regression (864/2000): loss=0.6074870314722317\n",
      "Logistic Regression (865/2000): loss=0.607498246335174\n",
      "Logistic Regression (866/2000): loss=0.6074604248426811\n",
      "Logistic Regression (867/2000): loss=0.6074857552053841\n",
      "Logistic Regression (868/2000): loss=0.607477831307135\n",
      "Logistic Regression (869/2000): loss=0.6074938731610342\n",
      "Logistic Regression (870/2000): loss=0.6075486021219321\n",
      "Logistic Regression (871/2000): loss=0.6075364787849785\n",
      "Logistic Regression (872/2000): loss=0.6074602502414043\n",
      "Logistic Regression (873/2000): loss=0.6075831214254218\n",
      "Logistic Regression (874/2000): loss=0.6075751094399292\n",
      "Logistic Regression (875/2000): loss=0.6077593282040694\n",
      "Logistic Regression (876/2000): loss=0.6076870877954371\n",
      "Logistic Regression (877/2000): loss=0.6076905291982473\n",
      "Logistic Regression (878/2000): loss=0.6075353611035625\n",
      "Logistic Regression (879/2000): loss=0.6073902499415106\n",
      "Logistic Regression (880/2000): loss=0.6075407228727858\n",
      "Logistic Regression (881/2000): loss=0.6075638780505873\n",
      "Logistic Regression (882/2000): loss=0.6074823156684533\n",
      "Logistic Regression (883/2000): loss=0.607655235072895\n",
      "Logistic Regression (884/2000): loss=0.6077661707182707\n",
      "Logistic Regression (885/2000): loss=0.6075063363826385\n",
      "Logistic Regression (886/2000): loss=0.6074770579159061\n",
      "Logistic Regression (887/2000): loss=0.6074072104534378\n",
      "Logistic Regression (888/2000): loss=0.6075303481694906\n",
      "Logistic Regression (889/2000): loss=0.6074284837451492\n",
      "Logistic Regression (890/2000): loss=0.607285388199325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (891/2000): loss=0.6072591855034266\n",
      "Logistic Regression (892/2000): loss=0.6074047225657494\n",
      "Logistic Regression (893/2000): loss=0.6076407500838996\n",
      "Logistic Regression (894/2000): loss=0.6075542474386643\n",
      "Logistic Regression (895/2000): loss=0.6076195470110614\n",
      "Logistic Regression (896/2000): loss=0.6075200015146174\n",
      "Logistic Regression (897/2000): loss=0.6075559178831241\n",
      "Logistic Regression (898/2000): loss=0.6074703395215243\n",
      "Logistic Regression (899/2000): loss=0.6074304873676819\n",
      "Logistic Regression (900/2000): loss=0.6072997794236046\n",
      "Logistic Regression (901/2000): loss=0.607110464624352\n",
      "Logistic Regression (902/2000): loss=0.6071135917078396\n",
      "Logistic Regression (903/2000): loss=0.607135461943159\n",
      "Logistic Regression (904/2000): loss=0.6070937102910885\n",
      "Logistic Regression (905/2000): loss=0.6071480939447352\n",
      "Logistic Regression (906/2000): loss=0.607086933293259\n",
      "Logistic Regression (907/2000): loss=0.6070603620383667\n",
      "Logistic Regression (908/2000): loss=0.6071171913375935\n",
      "Logistic Regression (909/2000): loss=0.6069727307724437\n",
      "Logistic Regression (910/2000): loss=0.6069566032410227\n",
      "Logistic Regression (911/2000): loss=0.6069544496651643\n",
      "Logistic Regression (912/2000): loss=0.6070309177430984\n",
      "Logistic Regression (913/2000): loss=0.6069531426219568\n",
      "Logistic Regression (914/2000): loss=0.6068714521131805\n",
      "Logistic Regression (915/2000): loss=0.6068289275673466\n",
      "Logistic Regression (916/2000): loss=0.606804371309753\n",
      "Logistic Regression (917/2000): loss=0.6067893948817276\n",
      "Logistic Regression (918/2000): loss=0.6068455930330175\n",
      "Logistic Regression (919/2000): loss=0.6068271985097083\n",
      "Logistic Regression (920/2000): loss=0.6067512425405155\n",
      "Logistic Regression (921/2000): loss=0.6067506648175092\n",
      "Logistic Regression (922/2000): loss=0.6066919966542157\n",
      "Logistic Regression (923/2000): loss=0.6066601289991926\n",
      "Logistic Regression (924/2000): loss=0.6066470402735189\n",
      "Logistic Regression (925/2000): loss=0.6067069609119787\n",
      "Logistic Regression (926/2000): loss=0.6066978165853607\n",
      "Logistic Regression (927/2000): loss=0.606730320658947\n",
      "Logistic Regression (928/2000): loss=0.6066386060959762\n",
      "Logistic Regression (929/2000): loss=0.6065964199533702\n",
      "Logistic Regression (930/2000): loss=0.6066421460768493\n",
      "Logistic Regression (931/2000): loss=0.6066302572199564\n",
      "Logistic Regression (932/2000): loss=0.6066136077104758\n",
      "Logistic Regression (933/2000): loss=0.606654960411885\n",
      "Logistic Regression (934/2000): loss=0.606557949777032\n",
      "Logistic Regression (935/2000): loss=0.6065441661022476\n",
      "Logistic Regression (936/2000): loss=0.6065474404596877\n",
      "Logistic Regression (937/2000): loss=0.6064997895570469\n",
      "Logistic Regression (938/2000): loss=0.6064981334525246\n",
      "Logistic Regression (939/2000): loss=0.6064745669488018\n",
      "Logistic Regression (940/2000): loss=0.6064641133365591\n",
      "Logistic Regression (941/2000): loss=0.6064427076595946\n",
      "Logistic Regression (942/2000): loss=0.6064440202023987\n",
      "Logistic Regression (943/2000): loss=0.6064564913292176\n",
      "Logistic Regression (944/2000): loss=0.606457117987898\n",
      "Logistic Regression (945/2000): loss=0.6064251488351708\n",
      "Logistic Regression (946/2000): loss=0.6064219580139207\n",
      "Logistic Regression (947/2000): loss=0.6063981052047853\n",
      "Logistic Regression (948/2000): loss=0.6063895808623604\n",
      "Logistic Regression (949/2000): loss=0.6063834683139999\n",
      "Logistic Regression (950/2000): loss=0.60635961504799\n",
      "Logistic Regression (951/2000): loss=0.6063462154597203\n",
      "Logistic Regression (952/2000): loss=0.6063290502059872\n",
      "Logistic Regression (953/2000): loss=0.6063418532642925\n",
      "Logistic Regression (954/2000): loss=0.6063222774580385\n",
      "Logistic Regression (955/2000): loss=0.6063078967039828\n",
      "Logistic Regression (956/2000): loss=0.6062955936195548\n",
      "Logistic Regression (957/2000): loss=0.606273624281851\n",
      "Logistic Regression (958/2000): loss=0.606267049209909\n",
      "Logistic Regression (959/2000): loss=0.6062680883625329\n",
      "Logistic Regression (960/2000): loss=0.6063007571483247\n",
      "Logistic Regression (961/2000): loss=0.6064024080465293\n",
      "Logistic Regression (962/2000): loss=0.6062844889994596\n",
      "Logistic Regression (963/2000): loss=0.6062743928424896\n",
      "Logistic Regression (964/2000): loss=0.6062567377585267\n",
      "Logistic Regression (965/2000): loss=0.6062927393845458\n",
      "Logistic Regression (966/2000): loss=0.6063449085237312\n",
      "Logistic Regression (967/2000): loss=0.6063193144331348\n",
      "Logistic Regression (968/2000): loss=0.6063388401265065\n",
      "Logistic Regression (969/2000): loss=0.6065200836528282\n",
      "Logistic Regression (970/2000): loss=0.6064307044575457\n",
      "Logistic Regression (971/2000): loss=0.606379157419461\n",
      "Logistic Regression (972/2000): loss=0.6063846059531411\n",
      "Logistic Regression (973/2000): loss=0.6065204738231126\n",
      "Logistic Regression (974/2000): loss=0.6064181723784685\n",
      "Logistic Regression (975/2000): loss=0.6065027034691821\n",
      "Logistic Regression (976/2000): loss=0.6063994182698161\n",
      "Logistic Regression (977/2000): loss=0.6062978303628633\n",
      "Logistic Regression (978/2000): loss=0.6063719534735231\n",
      "Logistic Regression (979/2000): loss=0.6064694383976297\n",
      "Logistic Regression (980/2000): loss=0.6064442922479518\n",
      "Logistic Regression (981/2000): loss=0.6065406492974373\n",
      "Logistic Regression (982/2000): loss=0.606555379811744\n",
      "Logistic Regression (983/2000): loss=0.6063917957269385\n",
      "Logistic Regression (984/2000): loss=0.6063084526487944\n",
      "Logistic Regression (985/2000): loss=0.6062780555301424\n",
      "Logistic Regression (986/2000): loss=0.6062019695588028\n",
      "Logistic Regression (987/2000): loss=0.6061097737272453\n",
      "Logistic Regression (988/2000): loss=0.6060655924414335\n",
      "Logistic Regression (989/2000): loss=0.606093964700477\n",
      "Logistic Regression (990/2000): loss=0.6061233028184984\n",
      "Logistic Regression (991/2000): loss=0.606071997170798\n",
      "Logistic Regression (992/2000): loss=0.6060485994729921\n",
      "Logistic Regression (993/2000): loss=0.6061278340783063\n",
      "Logistic Regression (994/2000): loss=0.6060983107891725\n",
      "Logistic Regression (995/2000): loss=0.6060571443559294\n",
      "Logistic Regression (996/2000): loss=0.6061925674595297\n",
      "Logistic Regression (997/2000): loss=0.6061031944455144\n",
      "Logistic Regression (998/2000): loss=0.6060446831928004\n",
      "Logistic Regression (999/2000): loss=0.6060559122088345\n",
      "Logistic Regression (1000/2000): loss=0.6059831239500043\n",
      "Logistic Regression (1001/2000): loss=0.6060778384681317\n",
      "Logistic Regression (1002/2000): loss=0.6061006237125333\n",
      "Logistic Regression (1003/2000): loss=0.6059866147920702\n",
      "Logistic Regression (1004/2000): loss=0.6059463690052527\n",
      "Logistic Regression (1005/2000): loss=0.6058070387004826\n",
      "Logistic Regression (1006/2000): loss=0.6057839146122225\n",
      "Logistic Regression (1007/2000): loss=0.6058334607467262\n",
      "Logistic Regression (1008/2000): loss=0.6058270920737726\n",
      "Logistic Regression (1009/2000): loss=0.6057710328070607\n",
      "Logistic Regression (1010/2000): loss=0.6058133000257481\n",
      "Logistic Regression (1011/2000): loss=0.6058632061145272\n",
      "Logistic Regression (1012/2000): loss=0.6058285184579002\n",
      "Logistic Regression (1013/2000): loss=0.6058080693004088\n",
      "Logistic Regression (1014/2000): loss=0.6058242371328619\n",
      "Logistic Regression (1015/2000): loss=0.6058140753518065\n",
      "Logistic Regression (1016/2000): loss=0.6057514820364585\n",
      "Logistic Regression (1017/2000): loss=0.6057172313582395\n",
      "Logistic Regression (1018/2000): loss=0.605759072763534\n",
      "Logistic Regression (1019/2000): loss=0.6056708332260389\n",
      "Logistic Regression (1020/2000): loss=0.605789252144565\n",
      "Logistic Regression (1021/2000): loss=0.6056784811446196\n",
      "Logistic Regression (1022/2000): loss=0.6056605501703428\n",
      "Logistic Regression (1023/2000): loss=0.605768178794935\n",
      "Logistic Regression (1024/2000): loss=0.6058408334348013\n",
      "Logistic Regression (1025/2000): loss=0.6056989834403683\n",
      "Logistic Regression (1026/2000): loss=0.60570509289061\n",
      "Logistic Regression (1027/2000): loss=0.6057111464641972\n",
      "Logistic Regression (1028/2000): loss=0.6056969603523131\n",
      "Logistic Regression (1029/2000): loss=0.6058740887117939\n",
      "Logistic Regression (1030/2000): loss=0.6058786423021902\n",
      "Logistic Regression (1031/2000): loss=0.6059342741162137\n",
      "Logistic Regression (1032/2000): loss=0.6059762750491213\n",
      "Logistic Regression (1033/2000): loss=0.6059819444916891\n",
      "Logistic Regression (1034/2000): loss=0.6060386872546079\n",
      "Logistic Regression (1035/2000): loss=0.6061139626668479\n",
      "Logistic Regression (1036/2000): loss=0.605944111320899\n",
      "Logistic Regression (1037/2000): loss=0.605860344719814\n",
      "Logistic Regression (1038/2000): loss=0.6059534111365493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (1039/2000): loss=0.6059386654748637\n",
      "Logistic Regression (1040/2000): loss=0.6060794276408875\n",
      "Logistic Regression (1041/2000): loss=0.6060708638247256\n",
      "Logistic Regression (1042/2000): loss=0.606128523195775\n",
      "Logistic Regression (1043/2000): loss=0.6060117831040679\n",
      "Logistic Regression (1044/2000): loss=0.6057995075988621\n",
      "Logistic Regression (1045/2000): loss=0.6056432413330262\n",
      "Logistic Regression (1046/2000): loss=0.6057351842693952\n",
      "Logistic Regression (1047/2000): loss=0.605613414274955\n",
      "Logistic Regression (1048/2000): loss=0.6056990422727422\n",
      "Logistic Regression (1049/2000): loss=0.6058450992719858\n",
      "Logistic Regression (1050/2000): loss=0.6057346645099978\n",
      "Logistic Regression (1051/2000): loss=0.6057641869877735\n",
      "Logistic Regression (1052/2000): loss=0.6058750364960527\n",
      "Logistic Regression (1053/2000): loss=0.6059283603397466\n",
      "Logistic Regression (1054/2000): loss=0.605944285647555\n",
      "Logistic Regression (1055/2000): loss=0.6059717146569844\n",
      "Logistic Regression (1056/2000): loss=0.6059942533460919\n",
      "Logistic Regression (1057/2000): loss=0.6060073219448948\n",
      "Logistic Regression (1058/2000): loss=0.6061030153954661\n",
      "Logistic Regression (1059/2000): loss=0.6065225273134839\n",
      "Logistic Regression (1060/2000): loss=0.606301998673594\n",
      "Logistic Regression (1061/2000): loss=0.6064461654898603\n",
      "Logistic Regression (1062/2000): loss=0.6066013444045347\n",
      "Logistic Regression (1063/2000): loss=0.6065263501445004\n",
      "Logistic Regression (1064/2000): loss=0.6063725293069182\n",
      "Logistic Regression (1065/2000): loss=0.6057668888750712\n",
      "Logistic Regression (1066/2000): loss=0.6058672084204744\n",
      "Logistic Regression (1067/2000): loss=0.6057908944418539\n",
      "Logistic Regression (1068/2000): loss=0.605961655066221\n",
      "Logistic Regression (1069/2000): loss=0.6061167710119288\n",
      "Logistic Regression (1070/2000): loss=0.6060263143652558\n",
      "Logistic Regression (1071/2000): loss=0.6060732037624493\n",
      "Logistic Regression (1072/2000): loss=0.6061177256329241\n",
      "Logistic Regression (1073/2000): loss=0.6063298761457363\n",
      "Logistic Regression (1074/2000): loss=0.6060991316703171\n",
      "Logistic Regression (1075/2000): loss=0.60603536513379\n",
      "Logistic Regression (1076/2000): loss=0.6058800812987583\n",
      "Logistic Regression (1077/2000): loss=0.6058734025677943\n",
      "Logistic Regression (1078/2000): loss=0.6059672018118096\n",
      "Logistic Regression (1079/2000): loss=0.6059849457230223\n",
      "Logistic Regression (1080/2000): loss=0.6057343650486698\n",
      "Logistic Regression (1081/2000): loss=0.6057825660008348\n",
      "Logistic Regression (1082/2000): loss=0.6059168841569197\n",
      "Logistic Regression (1083/2000): loss=0.6061358138651703\n",
      "Logistic Regression (1084/2000): loss=0.6057078043285444\n",
      "Logistic Regression (1085/2000): loss=0.605284643236974\n",
      "Logistic Regression (1086/2000): loss=0.6053093344558331\n",
      "Logistic Regression (1087/2000): loss=0.6053200673190026\n",
      "Logistic Regression (1088/2000): loss=0.6052919646930284\n",
      "Logistic Regression (1089/2000): loss=0.605268302411116\n",
      "Logistic Regression (1090/2000): loss=0.6051886603008036\n",
      "Logistic Regression (1091/2000): loss=0.605086616864699\n",
      "Logistic Regression (1092/2000): loss=0.6050555515619356\n",
      "Logistic Regression (1093/2000): loss=0.6050611673575949\n",
      "Logistic Regression (1094/2000): loss=0.605040046558637\n",
      "Logistic Regression (1095/2000): loss=0.6049599739608553\n",
      "Logistic Regression (1096/2000): loss=0.6050491978561016\n",
      "Logistic Regression (1097/2000): loss=0.6051130825889109\n",
      "Logistic Regression (1098/2000): loss=0.605061245612049\n",
      "Logistic Regression (1099/2000): loss=0.6049879718901136\n",
      "Logistic Regression (1100/2000): loss=0.6050627225486068\n",
      "Logistic Regression (1101/2000): loss=0.60500186234914\n",
      "Logistic Regression (1102/2000): loss=0.6049216925704535\n",
      "Logistic Regression (1103/2000): loss=0.6049418985567729\n",
      "Logistic Regression (1104/2000): loss=0.605021243584707\n",
      "Logistic Regression (1105/2000): loss=0.6049588983306401\n",
      "Logistic Regression (1106/2000): loss=0.6048723220652896\n",
      "Logistic Regression (1107/2000): loss=0.6048454776171484\n",
      "Logistic Regression (1108/2000): loss=0.6049210634737778\n",
      "Logistic Regression (1109/2000): loss=0.6048876382405937\n",
      "Logistic Regression (1110/2000): loss=0.6049649871340913\n",
      "Logistic Regression (1111/2000): loss=0.6048275346365432\n",
      "Logistic Regression (1112/2000): loss=0.6048057369535172\n",
      "Logistic Regression (1113/2000): loss=0.6047758012391932\n",
      "Logistic Regression (1114/2000): loss=0.6047239485595015\n",
      "Logistic Regression (1115/2000): loss=0.6047064065099537\n",
      "Logistic Regression (1116/2000): loss=0.604708337636304\n",
      "Logistic Regression (1117/2000): loss=0.6046879417468216\n",
      "Logistic Regression (1118/2000): loss=0.604675792160734\n",
      "Logistic Regression (1119/2000): loss=0.6046521278799311\n",
      "Logistic Regression (1120/2000): loss=0.6046403121464272\n",
      "Logistic Regression (1121/2000): loss=0.6046419132667842\n",
      "Logistic Regression (1122/2000): loss=0.6046425421834113\n",
      "Logistic Regression (1123/2000): loss=0.6046410604589115\n",
      "Logistic Regression (1124/2000): loss=0.6046286059180463\n",
      "Logistic Regression (1125/2000): loss=0.6046237427722169\n",
      "Logistic Regression (1126/2000): loss=0.6045931985858989\n",
      "Logistic Regression (1127/2000): loss=0.6046322906390401\n",
      "Logistic Regression (1128/2000): loss=0.6045737347302368\n",
      "Logistic Regression (1129/2000): loss=0.6045845994469558\n",
      "Logistic Regression (1130/2000): loss=0.6045627079470058\n",
      "Logistic Regression (1131/2000): loss=0.6045770458797864\n",
      "Logistic Regression (1132/2000): loss=0.604556864507862\n",
      "Logistic Regression (1133/2000): loss=0.6045470778300667\n",
      "Logistic Regression (1134/2000): loss=0.6045273355230661\n",
      "Logistic Regression (1135/2000): loss=0.6045415450597517\n",
      "Logistic Regression (1136/2000): loss=0.6045967123380092\n",
      "Logistic Regression (1137/2000): loss=0.6046768618020801\n",
      "Logistic Regression (1138/2000): loss=0.6047632385924578\n",
      "Logistic Regression (1139/2000): loss=0.6046032417130937\n",
      "Logistic Regression (1140/2000): loss=0.6045042091648234\n",
      "Logistic Regression (1141/2000): loss=0.6044511195349667\n",
      "Logistic Regression (1142/2000): loss=0.6044380253904174\n",
      "Logistic Regression (1143/2000): loss=0.6044661673190737\n",
      "Logistic Regression (1144/2000): loss=0.6044670552388361\n",
      "Logistic Regression (1145/2000): loss=0.6045090061176221\n",
      "Logistic Regression (1146/2000): loss=0.6044501780796928\n",
      "Logistic Regression (1147/2000): loss=0.6044142408426071\n",
      "Logistic Regression (1148/2000): loss=0.6043827634972673\n",
      "Logistic Regression (1149/2000): loss=0.6043817585030319\n",
      "Logistic Regression (1150/2000): loss=0.6044417889766669\n",
      "Logistic Regression (1151/2000): loss=0.6043868452030972\n",
      "Logistic Regression (1152/2000): loss=0.6043895132716084\n",
      "Logistic Regression (1153/2000): loss=0.6044102021388857\n",
      "Logistic Regression (1154/2000): loss=0.6044097083289315\n",
      "Logistic Regression (1155/2000): loss=0.6044046185603729\n",
      "Logistic Regression (1156/2000): loss=0.6044306940391703\n",
      "Logistic Regression (1157/2000): loss=0.6044071414702882\n",
      "Logistic Regression (1158/2000): loss=0.604353308597572\n",
      "Logistic Regression (1159/2000): loss=0.6042850475073962\n",
      "Logistic Regression (1160/2000): loss=0.6042993064982327\n",
      "Logistic Regression (1161/2000): loss=0.6043474168416538\n",
      "Logistic Regression (1162/2000): loss=0.6042970093715113\n",
      "Logistic Regression (1163/2000): loss=0.6042501351297052\n",
      "Logistic Regression (1164/2000): loss=0.6042918033606972\n",
      "Logistic Regression (1165/2000): loss=0.6042256714140026\n",
      "Logistic Regression (1166/2000): loss=0.6042291080636817\n",
      "Logistic Regression (1167/2000): loss=0.6042049932476262\n",
      "Logistic Regression (1168/2000): loss=0.6041731116341068\n",
      "Logistic Regression (1169/2000): loss=0.6041615800965714\n",
      "Logistic Regression (1170/2000): loss=0.604185958227517\n",
      "Logistic Regression (1171/2000): loss=0.6042071915811574\n",
      "Logistic Regression (1172/2000): loss=0.6042785858850137\n",
      "Logistic Regression (1173/2000): loss=0.6042371730702718\n",
      "Logistic Regression (1174/2000): loss=0.6042603852006465\n",
      "Logistic Regression (1175/2000): loss=0.6041910346858639\n",
      "Logistic Regression (1176/2000): loss=0.6041468496242806\n",
      "Logistic Regression (1177/2000): loss=0.6041641733242915\n",
      "Logistic Regression (1178/2000): loss=0.6041507598122129\n",
      "Logistic Regression (1179/2000): loss=0.6041431019329135\n",
      "Logistic Regression (1180/2000): loss=0.6040781426089923\n",
      "Logistic Regression (1181/2000): loss=0.6040463992932658\n",
      "Logistic Regression (1182/2000): loss=0.6040424302342762\n",
      "Logistic Regression (1183/2000): loss=0.6040241301855889\n",
      "Logistic Regression (1184/2000): loss=0.6040094682464305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (1185/2000): loss=0.6039943170967982\n",
      "Logistic Regression (1186/2000): loss=0.603996538978508\n",
      "Logistic Regression (1187/2000): loss=0.604007197423438\n",
      "Logistic Regression (1188/2000): loss=0.6039960699449518\n",
      "Logistic Regression (1189/2000): loss=0.603990258767998\n",
      "Logistic Regression (1190/2000): loss=0.6039685823116724\n",
      "Logistic Regression (1191/2000): loss=0.6039459883593234\n",
      "Logistic Regression (1192/2000): loss=0.6039303663043999\n",
      "Logistic Regression (1193/2000): loss=0.6039188896302093\n",
      "Logistic Regression (1194/2000): loss=0.6039190170119724\n",
      "Logistic Regression (1195/2000): loss=0.6038934019214026\n",
      "Logistic Regression (1196/2000): loss=0.6038823801504948\n",
      "Logistic Regression (1197/2000): loss=0.6038703702622513\n",
      "Logistic Regression (1198/2000): loss=0.6038525688697645\n",
      "Logistic Regression (1199/2000): loss=0.6038413234659039\n",
      "Logistic Regression (1200/2000): loss=0.6038497665609198\n",
      "Logistic Regression (1201/2000): loss=0.6038059469377701\n",
      "Logistic Regression (1202/2000): loss=0.6037860250735227\n",
      "Logistic Regression (1203/2000): loss=0.6037765496649516\n",
      "Logistic Regression (1204/2000): loss=0.6038027747139799\n",
      "Logistic Regression (1205/2000): loss=0.6037722371394331\n",
      "Logistic Regression (1206/2000): loss=0.6037476714442949\n",
      "Logistic Regression (1207/2000): loss=0.6037482069148055\n",
      "Logistic Regression (1208/2000): loss=0.6037263696613717\n",
      "Logistic Regression (1209/2000): loss=0.6037150040671788\n",
      "Logistic Regression (1210/2000): loss=0.6037184705752264\n",
      "Logistic Regression (1211/2000): loss=0.6036825445561745\n",
      "Logistic Regression (1212/2000): loss=0.6036820587621885\n",
      "Logistic Regression (1213/2000): loss=0.6036556134389448\n",
      "Logistic Regression (1214/2000): loss=0.6036478375945095\n",
      "Logistic Regression (1215/2000): loss=0.603675037222761\n",
      "Logistic Regression (1216/2000): loss=0.6036451537351453\n",
      "Logistic Regression (1217/2000): loss=0.6036542722335673\n",
      "Logistic Regression (1218/2000): loss=0.6036829590160673\n",
      "Logistic Regression (1219/2000): loss=0.6037597196943968\n",
      "Logistic Regression (1220/2000): loss=0.6036572106708015\n",
      "Logistic Regression (1221/2000): loss=0.6036815115843412\n",
      "Logistic Regression (1222/2000): loss=0.6037245824292249\n",
      "Logistic Regression (1223/2000): loss=0.6038207805136567\n",
      "Logistic Regression (1224/2000): loss=0.6039060590803405\n",
      "Logistic Regression (1225/2000): loss=0.603824627251497\n",
      "Logistic Regression (1226/2000): loss=0.6037096374368714\n",
      "Logistic Regression (1227/2000): loss=0.6036506686886244\n",
      "Logistic Regression (1228/2000): loss=0.6037360635743458\n",
      "Logistic Regression (1229/2000): loss=0.603788076128655\n",
      "Logistic Regression (1230/2000): loss=0.60370559814788\n",
      "Logistic Regression (1231/2000): loss=0.6036725171424252\n",
      "Logistic Regression (1232/2000): loss=0.6036751320153604\n",
      "Logistic Regression (1233/2000): loss=0.6036139223297475\n",
      "Logistic Regression (1234/2000): loss=0.6035827427797078\n",
      "Logistic Regression (1235/2000): loss=0.6035762366833648\n",
      "Logistic Regression (1236/2000): loss=0.6035473903848185\n",
      "Logistic Regression (1237/2000): loss=0.6036720726199741\n",
      "Logistic Regression (1238/2000): loss=0.6035897817728313\n",
      "Logistic Regression (1239/2000): loss=0.6035001041660756\n",
      "Logistic Regression (1240/2000): loss=0.6034967188907385\n",
      "Logistic Regression (1241/2000): loss=0.6034645936624112\n",
      "Logistic Regression (1242/2000): loss=0.6034815262925824\n",
      "Logistic Regression (1243/2000): loss=0.6035563963433752\n",
      "Logistic Regression (1244/2000): loss=0.6035050323021532\n",
      "Logistic Regression (1245/2000): loss=0.6034008576292356\n",
      "Logistic Regression (1246/2000): loss=0.6033765599427873\n",
      "Logistic Regression (1247/2000): loss=0.6033597684171006\n",
      "Logistic Regression (1248/2000): loss=0.6033486038313471\n",
      "Logistic Regression (1249/2000): loss=0.6033503607475045\n",
      "Logistic Regression (1250/2000): loss=0.603332966468965\n",
      "Logistic Regression (1251/2000): loss=0.6033230547399289\n",
      "Logistic Regression (1252/2000): loss=0.6033276615497953\n",
      "Logistic Regression (1253/2000): loss=0.6033100498993658\n",
      "Logistic Regression (1254/2000): loss=0.6032991387193429\n",
      "Logistic Regression (1255/2000): loss=0.6033126969544269\n",
      "Logistic Regression (1256/2000): loss=0.6033002890920297\n",
      "Logistic Regression (1257/2000): loss=0.6033027349522568\n",
      "Logistic Regression (1258/2000): loss=0.6033009534660908\n",
      "Logistic Regression (1259/2000): loss=0.6033129124563809\n",
      "Logistic Regression (1260/2000): loss=0.6033164702158037\n",
      "Logistic Regression (1261/2000): loss=0.6033409613123724\n",
      "Logistic Regression (1262/2000): loss=0.6032996949565145\n",
      "Logistic Regression (1263/2000): loss=0.6033039530817002\n",
      "Logistic Regression (1264/2000): loss=0.6032908956018673\n",
      "Logistic Regression (1265/2000): loss=0.6032590354801692\n",
      "Logistic Regression (1266/2000): loss=0.6032431894793948\n",
      "Logistic Regression (1267/2000): loss=0.6032681162298312\n",
      "Logistic Regression (1268/2000): loss=0.6032360360765773\n",
      "Logistic Regression (1269/2000): loss=0.6032260487777172\n",
      "Logistic Regression (1270/2000): loss=0.6032476396054267\n",
      "Logistic Regression (1271/2000): loss=0.6032844180255018\n",
      "Logistic Regression (1272/2000): loss=0.6033139070435174\n",
      "Logistic Regression (1273/2000): loss=0.6033583191312927\n",
      "Logistic Regression (1274/2000): loss=0.6033126651586259\n",
      "Logistic Regression (1275/2000): loss=0.6033059358957851\n",
      "Logistic Regression (1276/2000): loss=0.6032441393264434\n",
      "Logistic Regression (1277/2000): loss=0.6032214970396788\n",
      "Logistic Regression (1278/2000): loss=0.6031604827079292\n",
      "Logistic Regression (1279/2000): loss=0.6031538507680839\n",
      "Logistic Regression (1280/2000): loss=0.6031826448629557\n",
      "Logistic Regression (1281/2000): loss=0.603171759930236\n",
      "Logistic Regression (1282/2000): loss=0.6031751374740908\n",
      "Logistic Regression (1283/2000): loss=0.6031963273328635\n",
      "Logistic Regression (1284/2000): loss=0.6031679353730056\n",
      "Logistic Regression (1285/2000): loss=0.603222242528848\n",
      "Logistic Regression (1286/2000): loss=0.6031915789942729\n",
      "Logistic Regression (1287/2000): loss=0.6030837827753113\n",
      "Logistic Regression (1288/2000): loss=0.6030492560514069\n",
      "Logistic Regression (1289/2000): loss=0.603040098220653\n",
      "Logistic Regression (1290/2000): loss=0.6030297224674837\n",
      "Logistic Regression (1291/2000): loss=0.6030530188537174\n",
      "Logistic Regression (1292/2000): loss=0.6030844670425479\n",
      "Logistic Regression (1293/2000): loss=0.603063416259366\n",
      "Logistic Regression (1294/2000): loss=0.6030423450917496\n",
      "Logistic Regression (1295/2000): loss=0.6030717930085808\n",
      "Logistic Regression (1296/2000): loss=0.6030543152688903\n",
      "Logistic Regression (1297/2000): loss=0.6030655457997653\n",
      "Logistic Regression (1298/2000): loss=0.6030728824793524\n",
      "Logistic Regression (1299/2000): loss=0.6030323486735654\n",
      "Logistic Regression (1300/2000): loss=0.6030880301631625\n",
      "Logistic Regression (1301/2000): loss=0.6030064816652011\n",
      "Logistic Regression (1302/2000): loss=0.6029475300456963\n",
      "Logistic Regression (1303/2000): loss=0.6029685766710323\n",
      "Logistic Regression (1304/2000): loss=0.6029887902008404\n",
      "Logistic Regression (1305/2000): loss=0.6029618716348143\n",
      "Logistic Regression (1306/2000): loss=0.6029051905318153\n",
      "Logistic Regression (1307/2000): loss=0.6028801721041173\n",
      "Logistic Regression (1308/2000): loss=0.6028697002985928\n",
      "Logistic Regression (1309/2000): loss=0.6028573881488366\n",
      "Logistic Regression (1310/2000): loss=0.6028585185231995\n",
      "Logistic Regression (1311/2000): loss=0.60284987606679\n",
      "Logistic Regression (1312/2000): loss=0.6028424991994139\n",
      "Logistic Regression (1313/2000): loss=0.6028252993031976\n",
      "Logistic Regression (1314/2000): loss=0.6028527247845838\n",
      "Logistic Regression (1315/2000): loss=0.6028341922960921\n",
      "Logistic Regression (1316/2000): loss=0.6028155453295679\n",
      "Logistic Regression (1317/2000): loss=0.6027983018155609\n",
      "Logistic Regression (1318/2000): loss=0.6028116136865732\n",
      "Logistic Regression (1319/2000): loss=0.6028066274694915\n",
      "Logistic Regression (1320/2000): loss=0.6027671991937668\n",
      "Logistic Regression (1321/2000): loss=0.6027758796989362\n",
      "Logistic Regression (1322/2000): loss=0.6027481567980335\n",
      "Logistic Regression (1323/2000): loss=0.6027245240751491\n",
      "Logistic Regression (1324/2000): loss=0.6027184253197323\n",
      "Logistic Regression (1325/2000): loss=0.6027177192371809\n",
      "Logistic Regression (1326/2000): loss=0.6027026685730963\n",
      "Logistic Regression (1327/2000): loss=0.6026960201634606\n",
      "Logistic Regression (1328/2000): loss=0.6026876685329045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (1329/2000): loss=0.6026696515988598\n",
      "Logistic Regression (1330/2000): loss=0.6026693987812416\n",
      "Logistic Regression (1331/2000): loss=0.6026545428450539\n",
      "Logistic Regression (1332/2000): loss=0.6026474183330292\n",
      "Logistic Regression (1333/2000): loss=0.602648009453348\n",
      "Logistic Regression (1334/2000): loss=0.6026345117708909\n",
      "Logistic Regression (1335/2000): loss=0.6026293234211726\n",
      "Logistic Regression (1336/2000): loss=0.6026642217107591\n",
      "Logistic Regression (1337/2000): loss=0.6026734255478415\n",
      "Logistic Regression (1338/2000): loss=0.6026450441315018\n",
      "Logistic Regression (1339/2000): loss=0.6026285434164554\n",
      "Logistic Regression (1340/2000): loss=0.6026404528565024\n",
      "Logistic Regression (1341/2000): loss=0.6026873630921341\n",
      "Logistic Regression (1342/2000): loss=0.6026274961865106\n",
      "Logistic Regression (1343/2000): loss=0.6025910835528164\n",
      "Logistic Regression (1344/2000): loss=0.6025894490639935\n",
      "Logistic Regression (1345/2000): loss=0.6025787019158854\n",
      "Logistic Regression (1346/2000): loss=0.6025559709023655\n",
      "Logistic Regression (1347/2000): loss=0.6025416030801046\n",
      "Logistic Regression (1348/2000): loss=0.6025428279130526\n",
      "Logistic Regression (1349/2000): loss=0.6025934931787383\n",
      "Logistic Regression (1350/2000): loss=0.6026126128681815\n",
      "Logistic Regression (1351/2000): loss=0.6026393391655164\n",
      "Logistic Regression (1352/2000): loss=0.6027445844973013\n",
      "Logistic Regression (1353/2000): loss=0.6029919061051292\n",
      "Logistic Regression (1354/2000): loss=0.6029469817960916\n",
      "Logistic Regression (1355/2000): loss=0.6027315747418716\n",
      "Logistic Regression (1356/2000): loss=0.6027532250251705\n",
      "Logistic Regression (1357/2000): loss=0.6028854397909588\n",
      "Logistic Regression (1358/2000): loss=0.6029922874561598\n",
      "Logistic Regression (1359/2000): loss=0.6027909722554154\n",
      "Logistic Regression (1360/2000): loss=0.6025397970568006\n",
      "Logistic Regression (1361/2000): loss=0.6024553365037623\n",
      "Logistic Regression (1362/2000): loss=0.60239835326905\n",
      "Logistic Regression (1363/2000): loss=0.6023946847961441\n",
      "Logistic Regression (1364/2000): loss=0.6023902428268408\n",
      "Logistic Regression (1365/2000): loss=0.6024019618135253\n",
      "Logistic Regression (1366/2000): loss=0.6023941398294363\n",
      "Logistic Regression (1367/2000): loss=0.6023808407115488\n",
      "Logistic Regression (1368/2000): loss=0.6023659886470281\n",
      "Logistic Regression (1369/2000): loss=0.6023504319632794\n",
      "Logistic Regression (1370/2000): loss=0.6023341346592201\n",
      "Logistic Regression (1371/2000): loss=0.6023496422367587\n",
      "Logistic Regression (1372/2000): loss=0.6023076176071607\n",
      "Logistic Regression (1373/2000): loss=0.6022866387746159\n",
      "Logistic Regression (1374/2000): loss=0.6022805567819902\n",
      "Logistic Regression (1375/2000): loss=0.6023193218743012\n",
      "Logistic Regression (1376/2000): loss=0.6023119794033894\n",
      "Logistic Regression (1377/2000): loss=0.6022690798002466\n",
      "Logistic Regression (1378/2000): loss=0.6022731937321917\n",
      "Logistic Regression (1379/2000): loss=0.6022698078583587\n",
      "Logistic Regression (1380/2000): loss=0.6022737532233606\n",
      "Logistic Regression (1381/2000): loss=0.6022534317040947\n",
      "Logistic Regression (1382/2000): loss=0.6022372160438397\n",
      "Logistic Regression (1383/2000): loss=0.602223830601562\n",
      "Logistic Regression (1384/2000): loss=0.6022108940694805\n",
      "Logistic Regression (1385/2000): loss=0.6021968534565778\n",
      "Logistic Regression (1386/2000): loss=0.6021817074682673\n",
      "Logistic Regression (1387/2000): loss=0.6021674080682416\n",
      "Logistic Regression (1388/2000): loss=0.6021528518311764\n",
      "Logistic Regression (1389/2000): loss=0.6021534705192015\n",
      "Logistic Regression (1390/2000): loss=0.602144476728407\n",
      "Logistic Regression (1391/2000): loss=0.602158068911911\n",
      "Logistic Regression (1392/2000): loss=0.6021460365703645\n",
      "Logistic Regression (1393/2000): loss=0.6020994389294323\n",
      "Logistic Regression (1394/2000): loss=0.6020871361556913\n",
      "Logistic Regression (1395/2000): loss=0.60207435921667\n",
      "Logistic Regression (1396/2000): loss=0.6020692269548497\n",
      "Logistic Regression (1397/2000): loss=0.602070757381645\n",
      "Logistic Regression (1398/2000): loss=0.6020691446430485\n",
      "Logistic Regression (1399/2000): loss=0.6021014136578897\n",
      "Logistic Regression (1400/2000): loss=0.6021314566930732\n",
      "Logistic Regression (1401/2000): loss=0.6021002425282465\n",
      "Logistic Regression (1402/2000): loss=0.602091178128401\n",
      "Logistic Regression (1403/2000): loss=0.6020769539863294\n",
      "Logistic Regression (1404/2000): loss=0.6021143053601458\n",
      "Logistic Regression (1405/2000): loss=0.6021258411156168\n",
      "Logistic Regression (1406/2000): loss=0.6020963018510668\n",
      "Logistic Regression (1407/2000): loss=0.6020590925343396\n",
      "Logistic Regression (1408/2000): loss=0.6020857459429083\n",
      "Logistic Regression (1409/2000): loss=0.6020326365171119\n",
      "Logistic Regression (1410/2000): loss=0.6020452832241873\n",
      "Logistic Regression (1411/2000): loss=0.6020074755638872\n",
      "Logistic Regression (1412/2000): loss=0.6019985268126455\n",
      "Logistic Regression (1413/2000): loss=0.6019933549913296\n",
      "Logistic Regression (1414/2000): loss=0.601968279198514\n",
      "Logistic Regression (1415/2000): loss=0.601952161123666\n",
      "Logistic Regression (1416/2000): loss=0.601949156138948\n",
      "Logistic Regression (1417/2000): loss=0.6019906563684707\n",
      "Logistic Regression (1418/2000): loss=0.6019549922485031\n",
      "Logistic Regression (1419/2000): loss=0.6019858792544104\n",
      "Logistic Regression (1420/2000): loss=0.6020110114129754\n",
      "Logistic Regression (1421/2000): loss=0.6019206038464318\n",
      "Logistic Regression (1422/2000): loss=0.6019338753717265\n",
      "Logistic Regression (1423/2000): loss=0.6019167494335836\n",
      "Logistic Regression (1424/2000): loss=0.6020668748186669\n",
      "Logistic Regression (1425/2000): loss=0.6020061715928583\n",
      "Logistic Regression (1426/2000): loss=0.6019384111431236\n",
      "Logistic Regression (1427/2000): loss=0.6019221527005865\n",
      "Logistic Regression (1428/2000): loss=0.601912460411659\n",
      "Logistic Regression (1429/2000): loss=0.6019981556526641\n",
      "Logistic Regression (1430/2000): loss=0.6020384451154065\n",
      "Logistic Regression (1431/2000): loss=0.6020369416956183\n",
      "Logistic Regression (1432/2000): loss=0.6020783051333775\n",
      "Logistic Regression (1433/2000): loss=0.6019233192010038\n",
      "Logistic Regression (1434/2000): loss=0.6018802635115323\n",
      "Logistic Regression (1435/2000): loss=0.6019402156381056\n",
      "Logistic Regression (1436/2000): loss=0.6018836537341736\n",
      "Logistic Regression (1437/2000): loss=0.6018860391834462\n",
      "Logistic Regression (1438/2000): loss=0.6018192048518444\n",
      "Logistic Regression (1439/2000): loss=0.601909013876614\n",
      "Logistic Regression (1440/2000): loss=0.6018682039519683\n",
      "Logistic Regression (1441/2000): loss=0.6018282766000036\n",
      "Logistic Regression (1442/2000): loss=0.6017807999453559\n",
      "Logistic Regression (1443/2000): loss=0.6017423598822531\n",
      "Logistic Regression (1444/2000): loss=0.6017287370049849\n",
      "Logistic Regression (1445/2000): loss=0.6017288455710756\n",
      "Logistic Regression (1446/2000): loss=0.6017706172845775\n",
      "Logistic Regression (1447/2000): loss=0.6017341341999561\n",
      "Logistic Regression (1448/2000): loss=0.6017351248125429\n",
      "Logistic Regression (1449/2000): loss=0.6016979752103143\n",
      "Logistic Regression (1450/2000): loss=0.6016959201513057\n",
      "Logistic Regression (1451/2000): loss=0.6016825680152188\n",
      "Logistic Regression (1452/2000): loss=0.6016773090996641\n",
      "Logistic Regression (1453/2000): loss=0.6016741388266263\n",
      "Logistic Regression (1454/2000): loss=0.6016742085334645\n",
      "Logistic Regression (1455/2000): loss=0.6016955316998794\n",
      "Logistic Regression (1456/2000): loss=0.601674227053631\n",
      "Logistic Regression (1457/2000): loss=0.6017124471608407\n",
      "Logistic Regression (1458/2000): loss=0.6017017534109181\n",
      "Logistic Regression (1459/2000): loss=0.6017135588893234\n",
      "Logistic Regression (1460/2000): loss=0.6017222682849517\n",
      "Logistic Regression (1461/2000): loss=0.6016699259267902\n",
      "Logistic Regression (1462/2000): loss=0.6016173315297755\n",
      "Logistic Regression (1463/2000): loss=0.6016079351794194\n",
      "Logistic Regression (1464/2000): loss=0.6015917947115386\n",
      "Logistic Regression (1465/2000): loss=0.6015736170164184\n",
      "Logistic Regression (1466/2000): loss=0.6015680484631142\n",
      "Logistic Regression (1467/2000): loss=0.601549565186502\n",
      "Logistic Regression (1468/2000): loss=0.6015471326108425\n",
      "Logistic Regression (1469/2000): loss=0.6015449300329686\n",
      "Logistic Regression (1470/2000): loss=0.601532606273188\n",
      "Logistic Regression (1471/2000): loss=0.6015208357397787\n",
      "Logistic Regression (1472/2000): loss=0.6015061879645169\n",
      "Logistic Regression (1473/2000): loss=0.6014965923719526\n",
      "Logistic Regression (1474/2000): loss=0.6015239409575045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (1475/2000): loss=0.6015453054593869\n",
      "Logistic Regression (1476/2000): loss=0.601542525731316\n",
      "Logistic Regression (1477/2000): loss=0.601465350237978\n",
      "Logistic Regression (1478/2000): loss=0.6014934666699528\n",
      "Logistic Regression (1479/2000): loss=0.6014390181414518\n",
      "Logistic Regression (1480/2000): loss=0.6014372310433724\n",
      "Logistic Regression (1481/2000): loss=0.6014268701707954\n",
      "Logistic Regression (1482/2000): loss=0.6014073538400374\n",
      "Logistic Regression (1483/2000): loss=0.6013945240861349\n",
      "Logistic Regression (1484/2000): loss=0.6013862823502589\n",
      "Logistic Regression (1485/2000): loss=0.6013901398803723\n",
      "Logistic Regression (1486/2000): loss=0.6013972161087949\n",
      "Logistic Regression (1487/2000): loss=0.6014400372633509\n",
      "Logistic Regression (1488/2000): loss=0.6014269267731682\n",
      "Logistic Regression (1489/2000): loss=0.6013584672151221\n",
      "Logistic Regression (1490/2000): loss=0.6013453284217758\n",
      "Logistic Regression (1491/2000): loss=0.6013758328702378\n",
      "Logistic Regression (1492/2000): loss=0.6013767812086921\n",
      "Logistic Regression (1493/2000): loss=0.60144303890227\n",
      "Logistic Regression (1494/2000): loss=0.6013652360135087\n",
      "Logistic Regression (1495/2000): loss=0.601371896238953\n",
      "Logistic Regression (1496/2000): loss=0.6013632943274376\n",
      "Logistic Regression (1497/2000): loss=0.6014044583107887\n",
      "Logistic Regression (1498/2000): loss=0.6013502944229361\n",
      "Logistic Regression (1499/2000): loss=0.6013070095857606\n",
      "Logistic Regression (1500/2000): loss=0.6013196059607577\n",
      "Logistic Regression (1501/2000): loss=0.6013192830941896\n",
      "Logistic Regression (1502/2000): loss=0.6012600108676013\n",
      "Logistic Regression (1503/2000): loss=0.6012366071628054\n",
      "Logistic Regression (1504/2000): loss=0.6012236259433305\n",
      "Logistic Regression (1505/2000): loss=0.6012573512958631\n",
      "Logistic Regression (1506/2000): loss=0.6012203577804051\n",
      "Logistic Regression (1507/2000): loss=0.6012003849218126\n",
      "Logistic Regression (1508/2000): loss=0.6011899383722017\n",
      "Logistic Regression (1509/2000): loss=0.6011926890851156\n",
      "Logistic Regression (1510/2000): loss=0.6011858748930663\n",
      "Logistic Regression (1511/2000): loss=0.6011695350624032\n",
      "Logistic Regression (1512/2000): loss=0.6011643906536958\n",
      "Logistic Regression (1513/2000): loss=0.6011376820805258\n",
      "Logistic Regression (1514/2000): loss=0.601127626823568\n",
      "Logistic Regression (1515/2000): loss=0.6011511163668742\n",
      "Logistic Regression (1516/2000): loss=0.6012166946807163\n",
      "Logistic Regression (1517/2000): loss=0.6011536897387897\n",
      "Logistic Regression (1518/2000): loss=0.6011579008790331\n",
      "Logistic Regression (1519/2000): loss=0.6011512090947343\n",
      "Logistic Regression (1520/2000): loss=0.6011935789625252\n",
      "Logistic Regression (1521/2000): loss=0.6011711345023739\n",
      "Logistic Regression (1522/2000): loss=0.6011840864581977\n",
      "Logistic Regression (1523/2000): loss=0.6011861372430639\n",
      "Logistic Regression (1524/2000): loss=0.6011441597185548\n",
      "Logistic Regression (1525/2000): loss=0.6011390945745531\n",
      "Logistic Regression (1526/2000): loss=0.6011876363377109\n",
      "Logistic Regression (1527/2000): loss=0.6011518406936935\n",
      "Logistic Regression (1528/2000): loss=0.6011466425691138\n",
      "Logistic Regression (1529/2000): loss=0.6010456032673422\n",
      "Logistic Regression (1530/2000): loss=0.6010061469444191\n",
      "Logistic Regression (1531/2000): loss=0.6010260457699295\n",
      "Logistic Regression (1532/2000): loss=0.6011306446839182\n",
      "Logistic Regression (1533/2000): loss=0.6012320665428789\n",
      "Logistic Regression (1534/2000): loss=0.6013765437410918\n",
      "Logistic Regression (1535/2000): loss=0.6013131983429947\n",
      "Logistic Regression (1536/2000): loss=0.6013401405764716\n",
      "Logistic Regression (1537/2000): loss=0.6013236420083979\n",
      "Logistic Regression (1538/2000): loss=0.6012882080003642\n",
      "Logistic Regression (1539/2000): loss=0.6013586570473815\n",
      "Logistic Regression (1540/2000): loss=0.6013600442746155\n",
      "Logistic Regression (1541/2000): loss=0.6013924325934995\n",
      "Logistic Regression (1542/2000): loss=0.6014024731553367\n",
      "Logistic Regression (1543/2000): loss=0.6013244194104499\n",
      "Logistic Regression (1544/2000): loss=0.6011477632413005\n",
      "Logistic Regression (1545/2000): loss=0.6012204561876999\n",
      "Logistic Regression (1546/2000): loss=0.6011619269924383\n",
      "Logistic Regression (1547/2000): loss=0.6011333279459845\n",
      "Logistic Regression (1548/2000): loss=0.6010369684225498\n",
      "Logistic Regression (1549/2000): loss=0.6009890484067916\n",
      "Logistic Regression (1550/2000): loss=0.6010160806583477\n",
      "Logistic Regression (1551/2000): loss=0.6010383363530007\n",
      "Logistic Regression (1552/2000): loss=0.6011025130964176\n",
      "Logistic Regression (1553/2000): loss=0.6010885617404441\n",
      "Logistic Regression (1554/2000): loss=0.6011742977729391\n",
      "Logistic Regression (1555/2000): loss=0.6013363457000396\n",
      "Logistic Regression (1556/2000): loss=0.6014070031805305\n",
      "Logistic Regression (1557/2000): loss=0.6012479733198917\n",
      "Logistic Regression (1558/2000): loss=0.6012864071132642\n",
      "Logistic Regression (1559/2000): loss=0.601264066221147\n",
      "Logistic Regression (1560/2000): loss=0.6013193054670996\n",
      "Logistic Regression (1561/2000): loss=0.601202209711819\n",
      "Logistic Regression (1562/2000): loss=0.6012205979225412\n",
      "Logistic Regression (1563/2000): loss=0.6012595958108836\n",
      "Logistic Regression (1564/2000): loss=0.6015843180415101\n",
      "Logistic Regression (1565/2000): loss=0.6013919284252891\n",
      "Logistic Regression (1566/2000): loss=0.6012073448738446\n",
      "Logistic Regression (1567/2000): loss=0.6012951559008559\n",
      "Logistic Regression (1568/2000): loss=0.6012795885769671\n",
      "Logistic Regression (1569/2000): loss=0.6014461698365987\n",
      "Logistic Regression (1570/2000): loss=0.6016985287394719\n",
      "Logistic Regression (1571/2000): loss=0.6017391445842838\n",
      "Logistic Regression (1572/2000): loss=0.6013975291930365\n",
      "Logistic Regression (1573/2000): loss=0.6012562766002981\n",
      "Logistic Regression (1574/2000): loss=0.6012315348087517\n",
      "Logistic Regression (1575/2000): loss=0.6011420159035437\n",
      "Logistic Regression (1576/2000): loss=0.6010691720732948\n",
      "Logistic Regression (1577/2000): loss=0.6012288326187377\n",
      "Logistic Regression (1578/2000): loss=0.6011603648437416\n",
      "Logistic Regression (1579/2000): loss=0.6012057423107279\n",
      "Logistic Regression (1580/2000): loss=0.6011790931399486\n",
      "Logistic Regression (1581/2000): loss=0.6011295637739096\n",
      "Logistic Regression (1582/2000): loss=0.6010365755164692\n",
      "Logistic Regression (1583/2000): loss=0.6010065604786936\n",
      "Logistic Regression (1584/2000): loss=0.6008790284368368\n",
      "Logistic Regression (1585/2000): loss=0.6008013828356561\n",
      "Logistic Regression (1586/2000): loss=0.6009086357382097\n",
      "Logistic Regression (1587/2000): loss=0.6009115000026352\n",
      "Logistic Regression (1588/2000): loss=0.6010107732237647\n",
      "Logistic Regression (1589/2000): loss=0.6009047968488332\n",
      "Logistic Regression (1590/2000): loss=0.6008501639813493\n",
      "Logistic Regression (1591/2000): loss=0.6007894648721838\n",
      "Logistic Regression (1592/2000): loss=0.6007101337450708\n",
      "Logistic Regression (1593/2000): loss=0.6006888408703975\n",
      "Logistic Regression (1594/2000): loss=0.600654551375979\n",
      "Logistic Regression (1595/2000): loss=0.6007479822585284\n",
      "Logistic Regression (1596/2000): loss=0.6006290553823329\n",
      "Logistic Regression (1597/2000): loss=0.6005607064027154\n",
      "Logistic Regression (1598/2000): loss=0.6005391411844844\n",
      "Logistic Regression (1599/2000): loss=0.6005203604420569\n",
      "Logistic Regression (1600/2000): loss=0.6005230402500692\n",
      "Logistic Regression (1601/2000): loss=0.6005323761229545\n",
      "Logistic Regression (1602/2000): loss=0.6005371472632476\n",
      "Logistic Regression (1603/2000): loss=0.6005155395712799\n",
      "Logistic Regression (1604/2000): loss=0.6005075869540757\n",
      "Logistic Regression (1605/2000): loss=0.6005052180483228\n",
      "Logistic Regression (1606/2000): loss=0.6005323512543994\n",
      "Logistic Regression (1607/2000): loss=0.6005310221641795\n",
      "Logistic Regression (1608/2000): loss=0.6005492707767506\n",
      "Logistic Regression (1609/2000): loss=0.6006152580182093\n",
      "Logistic Regression (1610/2000): loss=0.600636711332898\n",
      "Logistic Regression (1611/2000): loss=0.6005019247038808\n",
      "Logistic Regression (1612/2000): loss=0.6006072227204133\n",
      "Logistic Regression (1613/2000): loss=0.6006006160838194\n",
      "Logistic Regression (1614/2000): loss=0.6006578021937746\n",
      "Logistic Regression (1615/2000): loss=0.6006473052953072\n",
      "Logistic Regression (1616/2000): loss=0.6006505318069569\n",
      "Logistic Regression (1617/2000): loss=0.6005752223522267\n",
      "Logistic Regression (1618/2000): loss=0.6005540169024104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (1619/2000): loss=0.6005145629969844\n",
      "Logistic Regression (1620/2000): loss=0.6004859520438098\n",
      "Logistic Regression (1621/2000): loss=0.6004424136498425\n",
      "Logistic Regression (1622/2000): loss=0.600421340056642\n",
      "Logistic Regression (1623/2000): loss=0.6004322084145555\n",
      "Logistic Regression (1624/2000): loss=0.6004045477465308\n",
      "Logistic Regression (1625/2000): loss=0.6003902242577462\n",
      "Logistic Regression (1626/2000): loss=0.6003985811403632\n",
      "Logistic Regression (1627/2000): loss=0.6003823824726523\n",
      "Logistic Regression (1628/2000): loss=0.6003648322317064\n",
      "Logistic Regression (1629/2000): loss=0.6003854528426104\n",
      "Logistic Regression (1630/2000): loss=0.600487324087209\n",
      "Logistic Regression (1631/2000): loss=0.6005424832054679\n",
      "Logistic Regression (1632/2000): loss=0.600626465481403\n",
      "Logistic Regression (1633/2000): loss=0.6006276738366434\n",
      "Logistic Regression (1634/2000): loss=0.600506700175616\n",
      "Logistic Regression (1635/2000): loss=0.6005576036606771\n",
      "Logistic Regression (1636/2000): loss=0.6005077615008934\n",
      "Logistic Regression (1637/2000): loss=0.6004807455681427\n",
      "Logistic Regression (1638/2000): loss=0.6004107177150115\n",
      "Logistic Regression (1639/2000): loss=0.6004324344841994\n",
      "Logistic Regression (1640/2000): loss=0.6003951499893507\n",
      "Logistic Regression (1641/2000): loss=0.6003583120934496\n",
      "Logistic Regression (1642/2000): loss=0.6003203301706381\n",
      "Logistic Regression (1643/2000): loss=0.6003382688679894\n",
      "Logistic Regression (1644/2000): loss=0.6003263183025226\n",
      "Logistic Regression (1645/2000): loss=0.6003188955073979\n",
      "Logistic Regression (1646/2000): loss=0.6003075046678257\n",
      "Logistic Regression (1647/2000): loss=0.6003543265028141\n",
      "Logistic Regression (1648/2000): loss=0.600389709829277\n",
      "Logistic Regression (1649/2000): loss=0.6003429316204557\n",
      "Logistic Regression (1650/2000): loss=0.6003508760802309\n",
      "Logistic Regression (1651/2000): loss=0.6003962195728388\n",
      "Logistic Regression (1652/2000): loss=0.6004557885434374\n",
      "Logistic Regression (1653/2000): loss=0.600460088309606\n",
      "Logistic Regression (1654/2000): loss=0.6004693144803066\n",
      "Logistic Regression (1655/2000): loss=0.6005092332832999\n",
      "Logistic Regression (1656/2000): loss=0.6004978874359665\n",
      "Logistic Regression (1657/2000): loss=0.6005615371720212\n",
      "Logistic Regression (1658/2000): loss=0.6007332898995649\n",
      "Logistic Regression (1659/2000): loss=0.6007268089382821\n",
      "Logistic Regression (1660/2000): loss=0.6007036604983303\n",
      "Logistic Regression (1661/2000): loss=0.6007209161998088\n",
      "Logistic Regression (1662/2000): loss=0.6007781970396415\n",
      "Logistic Regression (1663/2000): loss=0.6009233412806673\n",
      "Logistic Regression (1664/2000): loss=0.6007951753312832\n",
      "Logistic Regression (1665/2000): loss=0.6007910664467181\n",
      "Logistic Regression (1666/2000): loss=0.6007681814317869\n",
      "Logistic Regression (1667/2000): loss=0.6008945158896506\n",
      "Logistic Regression (1668/2000): loss=0.6008133597265458\n",
      "Logistic Regression (1669/2000): loss=0.6007143811249382\n",
      "Logistic Regression (1670/2000): loss=0.6007932901391291\n",
      "Logistic Regression (1671/2000): loss=0.6004831183255362\n",
      "Logistic Regression (1672/2000): loss=0.6003965330494749\n",
      "Logistic Regression (1673/2000): loss=0.6003623815666124\n",
      "Logistic Regression (1674/2000): loss=0.6003800177322068\n",
      "Logistic Regression (1675/2000): loss=0.6002396715209221\n",
      "Logistic Regression (1676/2000): loss=0.6001973273453964\n",
      "Logistic Regression (1677/2000): loss=0.6001487737232691\n",
      "Logistic Regression (1678/2000): loss=0.6000957570118413\n",
      "Logistic Regression (1679/2000): loss=0.6000500068638943\n",
      "Logistic Regression (1680/2000): loss=0.6000402869286259\n",
      "Logistic Regression (1681/2000): loss=0.6000227993868458\n",
      "Logistic Regression (1682/2000): loss=0.6000125053995705\n",
      "Logistic Regression (1683/2000): loss=0.6000117619771524\n",
      "Logistic Regression (1684/2000): loss=0.6000349288848537\n",
      "Logistic Regression (1685/2000): loss=0.6000250291026515\n",
      "Logistic Regression (1686/2000): loss=0.5999888999601171\n",
      "Logistic Regression (1687/2000): loss=0.5999841476259051\n",
      "Logistic Regression (1688/2000): loss=0.5999725869957181\n",
      "Logistic Regression (1689/2000): loss=0.5999607168376223\n",
      "Logistic Regression (1690/2000): loss=0.5999653386459974\n",
      "Logistic Regression (1691/2000): loss=0.5999447564713821\n",
      "Logistic Regression (1692/2000): loss=0.5999324870071815\n",
      "Logistic Regression (1693/2000): loss=0.5999356533877686\n",
      "Logistic Regression (1694/2000): loss=0.5999331509091451\n",
      "Logistic Regression (1695/2000): loss=0.5999209612708128\n",
      "Logistic Regression (1696/2000): loss=0.5999176629609827\n",
      "Logistic Regression (1697/2000): loss=0.5999098508157982\n",
      "Logistic Regression (1698/2000): loss=0.5999088419232244\n",
      "Logistic Regression (1699/2000): loss=0.5998953469626285\n",
      "Logistic Regression (1700/2000): loss=0.5998896873931298\n",
      "Logistic Regression (1701/2000): loss=0.5998913538741159\n",
      "Logistic Regression (1702/2000): loss=0.599959473376821\n",
      "Logistic Regression (1703/2000): loss=0.59993789034891\n",
      "Logistic Regression (1704/2000): loss=0.599960955610052\n",
      "Logistic Regression (1705/2000): loss=0.5999454733044063\n",
      "Logistic Regression (1706/2000): loss=0.5999695882535993\n",
      "Logistic Regression (1707/2000): loss=0.5999702094459747\n",
      "Logistic Regression (1708/2000): loss=0.599933474024778\n",
      "Logistic Regression (1709/2000): loss=0.5999166495499051\n",
      "Logistic Regression (1710/2000): loss=0.5998770227608708\n",
      "Logistic Regression (1711/2000): loss=0.5998230475677928\n",
      "Logistic Regression (1712/2000): loss=0.5998137938092066\n",
      "Logistic Regression (1713/2000): loss=0.5998279219173702\n",
      "Logistic Regression (1714/2000): loss=0.5998643762944247\n",
      "Logistic Regression (1715/2000): loss=0.5998853377656377\n",
      "Logistic Regression (1716/2000): loss=0.5998208649290766\n",
      "Logistic Regression (1717/2000): loss=0.5997841940947887\n",
      "Logistic Regression (1718/2000): loss=0.5997638328885967\n",
      "Logistic Regression (1719/2000): loss=0.5997600432941307\n",
      "Logistic Regression (1720/2000): loss=0.5997604996007669\n",
      "Logistic Regression (1721/2000): loss=0.5997667422778531\n",
      "Logistic Regression (1722/2000): loss=0.5997917572199191\n",
      "Logistic Regression (1723/2000): loss=0.5998273536172901\n",
      "Logistic Regression (1724/2000): loss=0.599812121770358\n",
      "Logistic Regression (1725/2000): loss=0.5999553561675969\n",
      "Logistic Regression (1726/2000): loss=0.5999380150322359\n",
      "Logistic Regression (1727/2000): loss=0.5998957800325033\n",
      "Logistic Regression (1728/2000): loss=0.5998828370660882\n",
      "Logistic Regression (1729/2000): loss=0.5998808281818978\n",
      "Logistic Regression (1730/2000): loss=0.5998312398487721\n",
      "Logistic Regression (1731/2000): loss=0.5997305422147429\n",
      "Logistic Regression (1732/2000): loss=0.5997347750737834\n",
      "Logistic Regression (1733/2000): loss=0.5997461455686013\n",
      "Logistic Regression (1734/2000): loss=0.5997247437769848\n",
      "Logistic Regression (1735/2000): loss=0.599687248303133\n",
      "Logistic Regression (1736/2000): loss=0.5996362990193191\n",
      "Logistic Regression (1737/2000): loss=0.599649698408391\n",
      "Logistic Regression (1738/2000): loss=0.5996611526335978\n",
      "Logistic Regression (1739/2000): loss=0.5996099846930019\n",
      "Logistic Regression (1740/2000): loss=0.5996354042681443\n",
      "Logistic Regression (1741/2000): loss=0.5996598261083619\n",
      "Logistic Regression (1742/2000): loss=0.5996822831962891\n",
      "Logistic Regression (1743/2000): loss=0.5996443138850113\n",
      "Logistic Regression (1744/2000): loss=0.5996122828935666\n",
      "Logistic Regression (1745/2000): loss=0.5995964550185252\n",
      "Logistic Regression (1746/2000): loss=0.5996426391638224\n",
      "Logistic Regression (1747/2000): loss=0.5996753643405823\n",
      "Logistic Regression (1748/2000): loss=0.5996351154359906\n",
      "Logistic Regression (1749/2000): loss=0.599691105754072\n",
      "Logistic Regression (1750/2000): loss=0.5996161018982442\n",
      "Logistic Regression (1751/2000): loss=0.5996483738711809\n",
      "Logistic Regression (1752/2000): loss=0.5996482273622298\n",
      "Logistic Regression (1753/2000): loss=0.5996272069577158\n",
      "Logistic Regression (1754/2000): loss=0.5998994984734769\n",
      "Logistic Regression (1755/2000): loss=0.599898385666154\n",
      "Logistic Regression (1756/2000): loss=0.5998350633771593\n",
      "Logistic Regression (1757/2000): loss=0.5998611246967459\n",
      "Logistic Regression (1758/2000): loss=0.5996586440050194\n",
      "Logistic Regression (1759/2000): loss=0.599564204804404\n",
      "Logistic Regression (1760/2000): loss=0.5995613078837572\n",
      "Logistic Regression (1761/2000): loss=0.5994753509328979\n",
      "Logistic Regression (1762/2000): loss=0.59944484257141\n",
      "Logistic Regression (1763/2000): loss=0.5994500152684672\n",
      "Logistic Regression (1764/2000): loss=0.5994524625326987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (1765/2000): loss=0.5994501690910672\n",
      "Logistic Regression (1766/2000): loss=0.5994591589041894\n",
      "Logistic Regression (1767/2000): loss=0.5994601815725675\n",
      "Logistic Regression (1768/2000): loss=0.599561411198066\n",
      "Logistic Regression (1769/2000): loss=0.5997307995276343\n",
      "Logistic Regression (1770/2000): loss=0.5998040589215502\n",
      "Logistic Regression (1771/2000): loss=0.5995966503429006\n",
      "Logistic Regression (1772/2000): loss=0.599611940614923\n",
      "Logistic Regression (1773/2000): loss=0.599667964423913\n",
      "Logistic Regression (1774/2000): loss=0.5997645364809672\n",
      "Logistic Regression (1775/2000): loss=0.5996979123297287\n",
      "Logistic Regression (1776/2000): loss=0.5998389008732812\n",
      "Logistic Regression (1777/2000): loss=0.5997649555247491\n",
      "Logistic Regression (1778/2000): loss=0.6001338561744807\n",
      "Logistic Regression (1779/2000): loss=0.6002519795127715\n",
      "Logistic Regression (1780/2000): loss=0.6003013641612786\n",
      "Logistic Regression (1781/2000): loss=0.6003649058637415\n",
      "Logistic Regression (1782/2000): loss=0.6007188579331424\n",
      "Logistic Regression (1783/2000): loss=0.6004922654086673\n",
      "Logistic Regression (1784/2000): loss=0.6003906633982271\n",
      "Logistic Regression (1785/2000): loss=0.6003467323658966\n",
      "Logistic Regression (1786/2000): loss=0.6001784382490835\n",
      "Logistic Regression (1787/2000): loss=0.600006472999567\n",
      "Logistic Regression (1788/2000): loss=0.5999477973766624\n",
      "Logistic Regression (1789/2000): loss=0.6000570738387925\n",
      "Logistic Regression (1790/2000): loss=0.5997633358622277\n",
      "Logistic Regression (1791/2000): loss=0.5999114140803744\n",
      "Logistic Regression (1792/2000): loss=0.6000544064110284\n",
      "Logistic Regression (1793/2000): loss=0.6000249955734844\n",
      "Logistic Regression (1794/2000): loss=0.6000546125117794\n",
      "Logistic Regression (1795/2000): loss=0.6001745559006306\n",
      "Logistic Regression (1796/2000): loss=0.6001812386154022\n",
      "Logistic Regression (1797/2000): loss=0.6000271115378376\n",
      "Logistic Regression (1798/2000): loss=0.5998548119383534\n",
      "Logistic Regression (1799/2000): loss=0.5997401636459084\n",
      "Logistic Regression (1800/2000): loss=0.5997630655716639\n",
      "Logistic Regression (1801/2000): loss=0.5995644578846969\n",
      "Logistic Regression (1802/2000): loss=0.5995166412194326\n",
      "Logistic Regression (1803/2000): loss=0.5993627101520115\n",
      "Logistic Regression (1804/2000): loss=0.5992767507942754\n",
      "Logistic Regression (1805/2000): loss=0.599252312808499\n",
      "Logistic Regression (1806/2000): loss=0.5992573807303219\n",
      "Logistic Regression (1807/2000): loss=0.5992325254473603\n",
      "Logistic Regression (1808/2000): loss=0.5992785735577775\n",
      "Logistic Regression (1809/2000): loss=0.5992217820443743\n",
      "Logistic Regression (1810/2000): loss=0.5993174861747792\n",
      "Logistic Regression (1811/2000): loss=0.5993407277596747\n",
      "Logistic Regression (1812/2000): loss=0.5993091484889413\n",
      "Logistic Regression (1813/2000): loss=0.599301889282335\n",
      "Logistic Regression (1814/2000): loss=0.5993263793371689\n",
      "Logistic Regression (1815/2000): loss=0.5992659685123198\n",
      "Logistic Regression (1816/2000): loss=0.5992453055533001\n",
      "Logistic Regression (1817/2000): loss=0.5992755878449992\n",
      "Logistic Regression (1818/2000): loss=0.599228708246944\n",
      "Logistic Regression (1819/2000): loss=0.5992522836882492\n",
      "Logistic Regression (1820/2000): loss=0.599224650152484\n",
      "Logistic Regression (1821/2000): loss=0.5991491893934908\n",
      "Logistic Regression (1822/2000): loss=0.5991378280636757\n",
      "Logistic Regression (1823/2000): loss=0.5991455712794024\n",
      "Logistic Regression (1824/2000): loss=0.599207712415332\n",
      "Logistic Regression (1825/2000): loss=0.5991675668827043\n",
      "Logistic Regression (1826/2000): loss=0.5991481325653368\n",
      "Logistic Regression (1827/2000): loss=0.5991165784524181\n",
      "Logistic Regression (1828/2000): loss=0.599129989042408\n",
      "Logistic Regression (1829/2000): loss=0.5991048947348881\n",
      "Logistic Regression (1830/2000): loss=0.5991015534645588\n",
      "Logistic Regression (1831/2000): loss=0.5991024409405238\n",
      "Logistic Regression (1832/2000): loss=0.5990909780763615\n",
      "Logistic Regression (1833/2000): loss=0.5990738557149068\n",
      "Logistic Regression (1834/2000): loss=0.599088959615557\n",
      "Logistic Regression (1835/2000): loss=0.5990696797249636\n",
      "Logistic Regression (1836/2000): loss=0.5991598955637757\n",
      "Logistic Regression (1837/2000): loss=0.5991059197949716\n",
      "Logistic Regression (1838/2000): loss=0.5991246614508483\n",
      "Logistic Regression (1839/2000): loss=0.5991209304945596\n",
      "Logistic Regression (1840/2000): loss=0.599090058386781\n",
      "Logistic Regression (1841/2000): loss=0.5990856153773184\n",
      "Logistic Regression (1842/2000): loss=0.5990765081553305\n",
      "Logistic Regression (1843/2000): loss=0.5990599261167276\n",
      "Logistic Regression (1844/2000): loss=0.599021436208645\n",
      "Logistic Regression (1845/2000): loss=0.5989993238907202\n",
      "Logistic Regression (1846/2000): loss=0.5989909739804166\n",
      "Logistic Regression (1847/2000): loss=0.5989662453335487\n",
      "Logistic Regression (1848/2000): loss=0.5989576847972047\n",
      "Logistic Regression (1849/2000): loss=0.5989617966760677\n",
      "Logistic Regression (1850/2000): loss=0.5989643145666534\n",
      "Logistic Regression (1851/2000): loss=0.5989645768369075\n",
      "Logistic Regression (1852/2000): loss=0.598944868932132\n",
      "Logistic Regression (1853/2000): loss=0.5989385622339944\n",
      "Logistic Regression (1854/2000): loss=0.5989421830489245\n",
      "Logistic Regression (1855/2000): loss=0.5989305479681704\n",
      "Logistic Regression (1856/2000): loss=0.598947784785014\n",
      "Logistic Regression (1857/2000): loss=0.5989434941916782\n",
      "Logistic Regression (1858/2000): loss=0.5989457302297875\n",
      "Logistic Regression (1859/2000): loss=0.598911387626114\n",
      "Logistic Regression (1860/2000): loss=0.5989174560088977\n",
      "Logistic Regression (1861/2000): loss=0.5990016717947714\n",
      "Logistic Regression (1862/2000): loss=0.5989904407068958\n",
      "Logistic Regression (1863/2000): loss=0.599003791485614\n",
      "Logistic Regression (1864/2000): loss=0.5989669588603513\n",
      "Logistic Regression (1865/2000): loss=0.5989643135542881\n",
      "Logistic Regression (1866/2000): loss=0.5989355044528649\n",
      "Logistic Regression (1867/2000): loss=0.5989165051580491\n",
      "Logistic Regression (1868/2000): loss=0.5988467895343983\n",
      "Logistic Regression (1869/2000): loss=0.5988396938677363\n",
      "Logistic Regression (1870/2000): loss=0.5988342691779307\n",
      "Logistic Regression (1871/2000): loss=0.5988577946052513\n",
      "Logistic Regression (1872/2000): loss=0.5988369501588545\n",
      "Logistic Regression (1873/2000): loss=0.598818822589457\n",
      "Logistic Regression (1874/2000): loss=0.5988076565155773\n",
      "Logistic Regression (1875/2000): loss=0.5988073110342708\n",
      "Logistic Regression (1876/2000): loss=0.5987947245251978\n",
      "Logistic Regression (1877/2000): loss=0.5987928342829894\n",
      "Logistic Regression (1878/2000): loss=0.5987898996670479\n",
      "Logistic Regression (1879/2000): loss=0.5988045366934331\n",
      "Logistic Regression (1880/2000): loss=0.598841117310965\n",
      "Logistic Regression (1881/2000): loss=0.5988286641155146\n",
      "Logistic Regression (1882/2000): loss=0.5988120080026039\n",
      "Logistic Regression (1883/2000): loss=0.5988431258769638\n",
      "Logistic Regression (1884/2000): loss=0.5987589868249984\n",
      "Logistic Regression (1885/2000): loss=0.5987330195674508\n",
      "Logistic Regression (1886/2000): loss=0.5987212559617487\n",
      "Logistic Regression (1887/2000): loss=0.5987161392177391\n",
      "Logistic Regression (1888/2000): loss=0.5987126893441257\n",
      "Logistic Regression (1889/2000): loss=0.598707586840571\n",
      "Logistic Regression (1890/2000): loss=0.5987056580085687\n",
      "Logistic Regression (1891/2000): loss=0.5986932693970916\n",
      "Logistic Regression (1892/2000): loss=0.5986942457601979\n",
      "Logistic Regression (1893/2000): loss=0.5987183340231739\n",
      "Logistic Regression (1894/2000): loss=0.5987075159912278\n",
      "Logistic Regression (1895/2000): loss=0.5987276342369537\n",
      "Logistic Regression (1896/2000): loss=0.5987348718609706\n",
      "Logistic Regression (1897/2000): loss=0.5986939819122579\n",
      "Logistic Regression (1898/2000): loss=0.5986662422314085\n",
      "Logistic Regression (1899/2000): loss=0.5986738094962695\n",
      "Logistic Regression (1900/2000): loss=0.5986686628403926\n",
      "Logistic Regression (1901/2000): loss=0.5986649609754788\n",
      "Logistic Regression (1902/2000): loss=0.5986610898762892\n",
      "Logistic Regression (1903/2000): loss=0.5986450685015378\n",
      "Logistic Regression (1904/2000): loss=0.5986584862728277\n",
      "Logistic Regression (1905/2000): loss=0.5986530526462733\n",
      "Logistic Regression (1906/2000): loss=0.5986456713945979\n",
      "Logistic Regression (1907/2000): loss=0.5986334570987596\n",
      "Logistic Regression (1908/2000): loss=0.5986688912480461\n",
      "Logistic Regression (1909/2000): loss=0.5986579963609487\n",
      "Logistic Regression (1910/2000): loss=0.5986362360552124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (1911/2000): loss=0.5986345291343477\n",
      "Logistic Regression (1912/2000): loss=0.5986676425965112\n",
      "Logistic Regression (1913/2000): loss=0.5986490145321404\n",
      "Logistic Regression (1914/2000): loss=0.5986660685563698\n",
      "Logistic Regression (1915/2000): loss=0.598671367894182\n",
      "Logistic Regression (1916/2000): loss=0.5986218718151763\n",
      "Logistic Regression (1917/2000): loss=0.5985871185784204\n",
      "Logistic Regression (1918/2000): loss=0.5985720457004111\n",
      "Logistic Regression (1919/2000): loss=0.5985890919269025\n",
      "Logistic Regression (1920/2000): loss=0.5986111431877966\n",
      "Logistic Regression (1921/2000): loss=0.5985902460894569\n",
      "Logistic Regression (1922/2000): loss=0.5985493800914393\n",
      "Logistic Regression (1923/2000): loss=0.5985463804865364\n",
      "Logistic Regression (1924/2000): loss=0.5985458608425706\n",
      "Logistic Regression (1925/2000): loss=0.5985488576528265\n",
      "Logistic Regression (1926/2000): loss=0.5985516978558804\n",
      "Logistic Regression (1927/2000): loss=0.5985378292536241\n",
      "Logistic Regression (1928/2000): loss=0.5985183512349262\n",
      "Logistic Regression (1929/2000): loss=0.5985158110159754\n",
      "Logistic Regression (1930/2000): loss=0.5985462250026228\n",
      "Logistic Regression (1931/2000): loss=0.5985635945421501\n",
      "Logistic Regression (1932/2000): loss=0.5985792969105234\n",
      "Logistic Regression (1933/2000): loss=0.5985711387261502\n",
      "Logistic Regression (1934/2000): loss=0.5985467349010376\n",
      "Logistic Regression (1935/2000): loss=0.5985195119999399\n",
      "Logistic Regression (1936/2000): loss=0.5985044931062516\n",
      "Logistic Regression (1937/2000): loss=0.5984820358422864\n",
      "Logistic Regression (1938/2000): loss=0.5984754516683967\n",
      "Logistic Regression (1939/2000): loss=0.5984758925455524\n",
      "Logistic Regression (1940/2000): loss=0.5985075158924725\n",
      "Logistic Regression (1941/2000): loss=0.5984949079860342\n",
      "Logistic Regression (1942/2000): loss=0.5985120033834445\n",
      "Logistic Regression (1943/2000): loss=0.5984624018203057\n",
      "Logistic Regression (1944/2000): loss=0.5984860627654339\n",
      "Logistic Regression (1945/2000): loss=0.59846435740998\n",
      "Logistic Regression (1946/2000): loss=0.5984722834429667\n",
      "Logistic Regression (1947/2000): loss=0.5984570046358806\n",
      "Logistic Regression (1948/2000): loss=0.5984681409309891\n",
      "Logistic Regression (1949/2000): loss=0.598446216325978\n",
      "Logistic Regression (1950/2000): loss=0.5984413822773325\n",
      "Logistic Regression (1951/2000): loss=0.598506235355855\n",
      "Logistic Regression (1952/2000): loss=0.5984776189707344\n",
      "Logistic Regression (1953/2000): loss=0.5984786319870715\n",
      "Logistic Regression (1954/2000): loss=0.5984880630591174\n",
      "Logistic Regression (1955/2000): loss=0.5985428829104376\n",
      "Logistic Regression (1956/2000): loss=0.5985076601690826\n",
      "Logistic Regression (1957/2000): loss=0.5985390905726862\n",
      "Logistic Regression (1958/2000): loss=0.5986301197352045\n",
      "Logistic Regression (1959/2000): loss=0.5985289109394863\n",
      "Logistic Regression (1960/2000): loss=0.5984707006934893\n",
      "Logistic Regression (1961/2000): loss=0.5984694265313957\n",
      "Logistic Regression (1962/2000): loss=0.5984980520661798\n",
      "Logistic Regression (1963/2000): loss=0.5984281152135592\n",
      "Logistic Regression (1964/2000): loss=0.5983895828549602\n",
      "Logistic Regression (1965/2000): loss=0.5983451241848265\n",
      "Logistic Regression (1966/2000): loss=0.5983401049316364\n",
      "Logistic Regression (1967/2000): loss=0.5983359993433905\n",
      "Logistic Regression (1968/2000): loss=0.5983267633793352\n",
      "Logistic Regression (1969/2000): loss=0.5983218160018329\n",
      "Logistic Regression (1970/2000): loss=0.5982959603061804\n",
      "Logistic Regression (1971/2000): loss=0.5983171882259957\n",
      "Logistic Regression (1972/2000): loss=0.5983266827467445\n",
      "Logistic Regression (1973/2000): loss=0.5983157516372856\n",
      "Logistic Regression (1974/2000): loss=0.5982981292535975\n",
      "Logistic Regression (1975/2000): loss=0.5983360891084502\n",
      "Logistic Regression (1976/2000): loss=0.5983313988205587\n",
      "Logistic Regression (1977/2000): loss=0.598325529346842\n",
      "Logistic Regression (1978/2000): loss=0.5982694108906177\n",
      "Logistic Regression (1979/2000): loss=0.5982482930614483\n",
      "Logistic Regression (1980/2000): loss=0.5982405196534865\n",
      "Logistic Regression (1981/2000): loss=0.5982349070875105\n",
      "Logistic Regression (1982/2000): loss=0.5982445994521779\n",
      "Logistic Regression (1983/2000): loss=0.5982491362317013\n",
      "Logistic Regression (1984/2000): loss=0.5982144154383028\n",
      "Logistic Regression (1985/2000): loss=0.5982173027137059\n",
      "Logistic Regression (1986/2000): loss=0.5982079683336952\n",
      "Logistic Regression (1987/2000): loss=0.5981896266000833\n",
      "Logistic Regression (1988/2000): loss=0.5981881390749437\n",
      "Logistic Regression (1989/2000): loss=0.5981814498687974\n",
      "Logistic Regression (1990/2000): loss=0.5982323723346329\n",
      "Logistic Regression (1991/2000): loss=0.5982128979907846\n",
      "Logistic Regression (1992/2000): loss=0.5982343757794486\n",
      "Logistic Regression (1993/2000): loss=0.5982253105930709\n",
      "Logistic Regression (1994/2000): loss=0.5982937608220015\n",
      "Logistic Regression (1995/2000): loss=0.5982794291602513\n",
      "Logistic Regression (1996/2000): loss=0.5981547761075248\n",
      "Logistic Regression (1997/2000): loss=0.5981607061014731\n",
      "Logistic Regression (1998/2000): loss=0.5981603688021916\n",
      "Logistic Regression (1999/2000): loss=0.5981470602514222\n",
      "Logistic Regression (2000/2000): loss=0.5981698267938818\n",
      "[[ 1.06505291e-03]\n",
      " [-3.01370452e-04]\n",
      " [-1.14486178e-04]\n",
      " [ 2.73903124e-05]\n",
      " [ 2.60886150e-05]\n",
      " [ 4.61937631e-04]\n",
      " [ 1.83902377e-05]\n",
      " [-1.42651619e-06]\n",
      " [-6.70135471e-05]\n",
      " [-2.05155650e-04]\n",
      " [-6.33666161e-06]\n",
      " [ 4.53423547e-06]\n",
      " [ 2.51769232e-05]\n",
      " [ 6.97055241e-05]\n",
      " [ 2.23101689e-08]\n",
      " [-4.12912275e-07]\n",
      " [-9.76521643e-05]\n",
      " [-4.33895254e-08]\n",
      " [-8.50538525e-08]\n",
      " [-5.99449597e-05]\n",
      " [ 4.57662228e-07]\n",
      " [-2.93835715e-04]\n",
      " [-3.12743159e-06]\n",
      " [ 9.94177875e-05]\n",
      " [ 1.33477006e-04]\n",
      " [ 1.33652824e-04]\n",
      " [-4.66010286e-05]\n",
      " [ 2.47441013e-05]\n",
      " [ 2.45933688e-05]\n",
      " [-1.77208986e-04]]\n",
      "0.5981698267938818\n"
     ]
    }
   ],
   "source": [
    "from ML_methods import *\n",
    "\n",
    "initial_w = np.zeros(tX.shape[1])\n",
    "max_iters = 2000\n",
    "gamma = 4.5e-10\n",
    "\n",
    "y_tr_log_reg = y_tr>0\n",
    "\n",
    "\n",
    "w, mse = logistic_regression(y_tr_log_reg, x_tr, initial_w, max_iters, gamma, batch_size=64)\n",
    "\n",
    "print(w)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n3PPK8v5mN8p",
    "outputId": "99746f4a-c950-4942-d9e4-406d5144cb70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.580766304"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Logistic regression\n",
    "# predict on the test data slice\n",
    "y_pred = predict_labels(w, x_te)\n",
    "# Check accuracy\n",
    "np.mean(y_te==y_pred) # 0.580766304"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J34FUFP-mN8q"
   },
   "source": [
    "## Regularized Logistic Regression Regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yscGLE8NmN8r"
   },
   "outputs": [],
   "source": [
    "from ML_methods import *\n",
    "\n",
    "initial_w = np.zeros(tX.shape[1])\n",
    "y_tr_reg_log_reg = y_tr>0\n",
    "\n",
    "max_iters = 2000\n",
    "gamma = 1e-9\n",
    "lambda_ = 1\n",
    "\n",
    "w, mse = reg_logistic_regression(y_tr_reg_log_reg, x_tr, lambda_, initial_w, max_iters, gamma, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1btym3kimN8t",
    "outputId": "4a99a6d7-8a51-4cfb-d368-6e1bf61a2d7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.617529328"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Regularized Logistic regression\n",
    "# predict on the test data slice\n",
    "y_pred = predict_labels(w, x_te)\n",
    "# Check accuracy\n",
    "np.mean(y_te==y_pred) # 0.617529328"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T3RgwnxsmN8w"
   },
   "source": [
    "## 3. Third Model: replace all the nanValues with the column mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "feJkeLQqmN8x"
   },
   "outputs": [],
   "source": [
    "tX_3 = replace_nan_with_mean(tX, -999)\n",
    "x_tr, x_te, y_tr, y_te = split_data(tX_3, y, ratio, seed)\n",
    "\n",
    "x_tr = standardize(x_tr)[0]\n",
    "x_te = standardize(x_te)[0]\n",
    "\n",
    "y_tr, x_tr = build_model_data(x_tr, y_tr)\n",
    "y_te, x_te = build_model_data(x_te, y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i50lIiN_mN8y"
   },
   "source": [
    "### 3.1.1 Third Model: Use least_squares : train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FRxuEt4SmN8z"
   },
   "outputs": [],
   "source": [
    "weights = least_squares(y_tr, x_tr)\n",
    "MSE_tr = compute_loss(y_tr, x_tr, weights)\n",
    "MSE_te = compute_loss(y_te, x_te, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UfFLCwe5mN81"
   },
   "source": [
    "### 3.1.2 Third Model: Use least_squares: prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UoeUF6FNmN81",
    "outputId": "b6c1839e-9f25-4953-a42e-38c47b8d5c33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5331424796300375"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on the test data slice\n",
    "y_pred = predict_labels(weights, x_te)\n",
    "# Check accuracy\n",
    "np.mean(y_te==y_pred) # 0.5331424796300375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U5vAj6-ymN83"
   },
   "source": [
    "### 3.2.1 Third Model: Use GD : Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1kc84Lv4mN83",
    "outputId": "28a5489b-11ce-4f37-d208-831056107268"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/150 loss = 17.3154463737639\n",
      "Step 2/150 loss = 12.85154559694335\n",
      "Step 3/150 loss = 10.15793488186781\n",
      "Step 4/150 loss = 8.393558092837578\n",
      "Step 5/150 loss = 7.146208061898221\n",
      "Step 6/150 loss = 6.207766930275749\n",
      "Step 7/150 loss = 5.468531107274753\n",
      "Step 8/150 loss = 4.867250480498854\n",
      "Step 9/150 loss = 4.3672931475690016\n",
      "Step 10/150 loss = 3.945107348603659\n",
      "Step 11/150 loss = 3.584507879895943\n",
      "Step 12/150 loss = 3.2737494035344157\n",
      "Step 13/150 loss = 3.003955009009131\n",
      "Step 14/150 loss = 2.7682204992546464\n",
      "Step 15/150 loss = 2.5610681557279027\n",
      "Step 16/150 loss = 2.3780904919748433\n",
      "Step 17/150 loss = 2.21570384800992\n",
      "Step 18/150 loss = 2.0709699189386193\n",
      "Step 19/150 loss = 1.9414621183893161\n",
      "Step 20/150 loss = 1.8251632119498742\n",
      "Step 21/150 loss = 1.7203857016225739\n",
      "Step 22/150 loss = 1.6257092655990009\n",
      "Step 23/150 loss = 1.5399312431392211\n",
      "Step 24/150 loss = 1.4620272297442516\n",
      "Step 25/150 loss = 1.391119576400008\n",
      "Step 26/150 loss = 1.3264521046700706\n",
      "Step 27/150 loss = 1.2673697309008831\n",
      "Step 28/150 loss = 1.2133019805895224\n",
      "Step 29/150 loss = 1.1637495944975655\n",
      "Step 30/150 loss = 1.1182735987878\n",
      "Step 31/150 loss = 1.0764863444154489\n",
      "Step 32/150 loss = 1.0380441249870351\n",
      "Step 33/150 loss = 1.0026410638424992\n",
      "Step 34/150 loss = 0.9700040251849756\n",
      "Step 35/150 loss = 0.9398883544980099\n",
      "Step 36/150 loss = 0.9120742932129489\n",
      "Step 37/150 loss = 0.8863639439261444\n",
      "Step 38/150 loss = 0.8625786872179533\n",
      "Step 39/150 loss = 0.8405569707031436\n",
      "Step 40/150 loss = 0.8201524064499298\n",
      "Step 41/150 loss = 0.8012321252086315\n",
      "Step 42/150 loss = 0.7836753456707534\n",
      "Step 43/150 loss = 0.7673721247687452\n",
      "Step 44/150 loss = 0.752222261245137\n",
      "Step 45/150 loss = 0.738134329696625\n",
      "Step 46/150 loss = 0.7250248262931946\n",
      "Step 47/150 loss = 0.7128174105883744\n",
      "Step 48/150 loss = 0.7014422304348106\n",
      "Step 49/150 loss = 0.6908353191260848\n",
      "Step 50/150 loss = 0.6809380556008503\n",
      "Step 51/150 loss = 0.6716966799477725\n",
      "Step 52/150 loss = 0.6630618576017079\n",
      "Step 53/150 loss = 0.6549882865723688\n",
      "Step 54/150 loss = 0.6474343428354737\n",
      "Step 55/150 loss = 0.6403617596741387\n",
      "Step 56/150 loss = 0.6337353373097031\n",
      "Step 57/150 loss = 0.6275226796259898\n",
      "Step 58/150 loss = 0.6216939551849388\n",
      "Step 59/150 loss = 0.6162216800671952\n",
      "Step 60/150 loss = 0.6110805203587445\n",
      "Step 61/150 loss = 0.6062471123522134\n",
      "Step 62/150 loss = 0.6016998987456245\n",
      "Step 63/150 loss = 0.5974189793075648\n",
      "Step 64/150 loss = 0.593385974640277\n",
      "Step 65/150 loss = 0.5895839018147041\n",
      "Step 66/150 loss = 0.5859970607769617\n",
      "Step 67/150 loss = 0.582610930536506\n",
      "Step 68/150 loss = 0.5794120742444594\n",
      "Step 69/150 loss = 0.5763880523578194\n",
      "Step 70/150 loss = 0.5735273431630584\n",
      "Step 71/150 loss = 0.5708192700021012\n",
      "Step 72/150 loss = 0.5682539346058826\n",
      "Step 73/150 loss = 0.5658221559964923\n",
      "Step 74/150 loss = 0.5635154144690775\n",
      "Step 75/150 loss = 0.5613258002098145\n",
      "Step 76/150 loss = 0.5592459661469539\n",
      "Step 77/150 loss = 0.5572690846686743\n",
      "Step 78/150 loss = 0.5553888078746554\n",
      "Step 79/150 loss = 0.5535992310582993\n",
      "Step 80/150 loss = 0.5518948591436936\n",
      "Step 81/150 loss = 0.5502705758260209\n",
      "Step 82/150 loss = 0.5487216151864425\n",
      "Step 83/150 loss = 0.547243535572723\n",
      "Step 84/150 loss = 0.5458321955552446\n",
      "Step 85/150 loss = 0.5444837317847548\n",
      "Step 86/150 loss = 0.5431945385933596\n",
      "Step 87/150 loss = 0.5419612491940675\n",
      "Step 88/150 loss = 0.5407807183467386\n",
      "Step 89/150 loss = 0.5396500063697021\n",
      "Step 90/150 loss = 0.538566364386705\n",
      "Step 91/150 loss = 0.5375272207083142\n",
      "Step 92/150 loss = 0.5365301682555131\n",
      "Step 93/150 loss = 0.535572952941096\n",
      "Step 94/150 loss = 0.5346534629316162\n",
      "Step 95/150 loss = 0.5337697187191817\n",
      "Step 96/150 loss = 0.5329198639383429\n",
      "Step 97/150 loss = 0.532102156868753\n",
      "Step 98/150 loss = 0.5313149625692426\n",
      "Step 99/150 loss = 0.5305567455934778\n",
      "Step 100/150 loss = 0.5298260632415076\n",
      "Step 101/150 loss = 0.5291215593052825\n",
      "Step 102/150 loss = 0.5284419582696784\n",
      "Step 103/150 loss = 0.5277860599337224\n",
      "Step 104/150 loss = 0.5271527344195904\n",
      "Step 105/150 loss = 0.526540917539597\n",
      "Step 106/150 loss = 0.5259496064938061\n",
      "Step 107/150 loss = 0.5253778558731028\n",
      "Step 108/150 loss = 0.5248247739445909\n",
      "Step 109/150 loss = 0.5242895191980314\n",
      "Step 110/150 loss = 0.5237712971337334\n",
      "Step 111/150 loss = 0.5232693572738711\n",
      "Step 112/150 loss = 0.5227829903806157\n",
      "Step 113/150 loss = 0.522311525865787\n",
      "Step 114/150 loss = 0.5218543293779249\n",
      "Step 115/150 loss = 0.5214108005537809\n",
      "Step 116/150 loss = 0.5209803709222421\n",
      "Step 117/150 loss = 0.5205625019496256\n",
      "Step 118/150 loss = 0.5201566832161326\n",
      "Step 119/150 loss = 0.519762430714036\n",
      "Step 120/150 loss = 0.5193792852588957\n",
      "Step 121/150 loss = 0.5190068110057512\n",
      "Step 122/150 loss = 0.5186445940628579\n",
      "Step 123/150 loss = 0.5182922411960877\n",
      "Step 124/150 loss = 0.5179493786176316\n",
      "Step 125/150 loss = 0.5176156508531166\n",
      "Step 126/150 loss = 0.5172907196816855\n",
      "Step 127/150 loss = 0.5169742631439946\n",
      "Step 128/150 loss = 0.5166659746134483\n",
      "Step 129/150 loss = 0.5163655619263404\n",
      "Step 130/150 loss = 0.5160727465668825\n",
      "Step 131/150 loss = 0.5157872629033923\n",
      "Step 132/150 loss = 0.515508857472185\n",
      "Step 133/150 loss = 0.5152372883059594\n",
      "Step 134/150 loss = 0.5149723243036971\n",
      "Step 135/150 loss = 0.5147137446393111\n",
      "Step 136/150 loss = 0.5144613382064693\n",
      "Step 137/150 loss = 0.5142149030972066\n",
      "Step 138/150 loss = 0.5139742461121015\n",
      "Step 139/150 loss = 0.5137391822999532\n",
      "Step 140/150 loss = 0.5135095345250347\n",
      "Step 141/150 loss = 0.5132851330601355\n",
      "Step 142/150 loss = 0.5130658152037264\n",
      "Step 143/150 loss = 0.512851424919696\n",
      "Step 144/150 loss = 0.5126418124982142\n",
      "Step 145/150 loss = 0.5124368342363763\n",
      "Step 146/150 loss = 0.512236352137372\n",
      "Step 147/150 loss = 0.5120402336270091\n",
      "Step 148/150 loss = 0.5118483512865011\n",
      "Step 149/150 loss = 0.511660582600499\n",
      "Step 150/150 loss = 0.5114768097194176\n",
      "Gradient final loss for w* = 0.5114768097194176\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 150\n",
    "gamma = 0.08\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones(x_tr.shape[1])\n",
    "\n",
    "gradient_w, gradient_loss = least_squares_GD(y_tr, x_tr, w_initial, max_iters, gamma) \n",
    "\n",
    "print(\"Gradient final loss for w* = \" + str(gradient_loss)) # 0.5114768097194176\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35mi7d6ZmN85"
   },
   "source": [
    "### 3.2.2 Third Model: Use GD : Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwUbgYFzmN86",
    "outputId": "c2e5cb4b-5ce3-4f08-82dc-1ed556d0e6cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5006239447992366"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on the test data slice\n",
    "y_pred = predict_labels(gradient_w, x_te)\n",
    "# Check accuracy\n",
    "np.mean(y_te==y_pred) # 0.5006239447992366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5N_zjXCpmN88"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yjZtzi6jmN8-"
   },
   "source": [
    "## 4. Fourth Model with Feature Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_no8zn8UmN8-",
    "outputId": "7916ac38-f78f-463c-8459-8b964c079746"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (54491,32) and (54491,) not aligned: 32 (dim 1) != 54491 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-a4f7a1f6e8d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpolynome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolynome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolynome\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-861c55fb9329>\u001b[0m in \u001b[0;36mleast_squares\u001b[0;34m(y, tx)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (54491,32) and (54491,) not aligned: 32 (dim 1) != 54491 (dim 0)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14a0607f0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "degrees = [1, 2, 3, 4]\n",
    "\n",
    "# define the structure of the figure\n",
    "num_row = 2\n",
    "num_col = 2\n",
    "f, axs = plt.subplots(num_row, num_col)\n",
    "    \n",
    "for ind, degree in enumerate(degrees):\n",
    "\n",
    "        polynome = build_poly(x_tr,degree)\n",
    "        w = least_squares(y_tr, polynome.T)\n",
    "        MSE = compute_loss(y_tr, polynome, w)\n",
    "\n",
    "        print(\"Processing {i}th experiment, degree={d}, mse={loss}\".format( \n",
    "            i=ind + 1, d=degree, loss=MSE))\n",
    "        plot_fitted_curve(y_tr, x_tr, weights, degree, axs[ind // num_col][ind % num_col])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2MuXRogAmN9B"
   },
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JOOmucUimN9B"
   },
   "outputs": [],
   "source": [
    "# We observed that model 2 with ridge_regression method and lambda_ = 1e-10 (TODO, we have to check them all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "agdRdr6zmN9D"
   },
   "outputs": [],
   "source": [
    "polynome = build_poly(x_tr,9)\n",
    "weights = least_squares(y_tr, polynome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JpTiNLMfmN9F"
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: unzip the file\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test = build_poly(tX_test,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fzpMrw0EmN9H"
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'out.csv'\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qa5LQkMImN9K"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "include_colab_link": true,
   "name": "project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
